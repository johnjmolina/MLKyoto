{
  
    
        "post0": {
            "title": "Bayesian Parameter Estimation",
            "content": "Motivation / Disclaimer . This is the first in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . We start with the basics of Bayesian Data Analysis, following the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling. Dr. Sivia is a research scientist at the Rutherford Appleton Lab and Prof. Skilling was at the DAMPT (Cambridge) before becoming a founding director of the Maximum Entropy Data Consultants group. The book is written by/for physicist and includes advanced topics (such as Maximum Entropy and Nested Sampling) that we hope to cover in future posts. . References . While mainly following Sivia&#39;s book, we have also found the following references extremely helpful . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | Bayesian Data Analysis, Third Edition. A. Gelman, J. B. Carlin, H. Stern et al., Chapman &amp; Hall/CRC Texts in Statistical Science (2013) | &quot;Frequentism and Bayesianism&quot; Blog-Post series (I, II, III, IV, IV) by Jake VanderPlas | Michael Betancourt&#39;s writings on probability theory. | . Preliminaries . Basic Rules . The probability $P(X lvert I)$ expresses our belief that $X$ is true given prior information $I$. The two basic rules of probability theory are just the sum and product rules, $$ begin{align} P(X lvert I) + P( overline{X} lvert I) &amp;= 1 &amp; textrm{(Sum Rule)} label{e:sum_rule} P(X,Y lvert I) &amp;= P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Product Rule)} notag &amp;=P(Y lvert X, I) times P(X lvert I) label{e:product_rule} end{align}$$ where $ overline{X}$ denotes the negation of $X$. See Sivia or Jaynes for a derivation, originally due to Cox, which shows that these are the only rules that will guarantee consistency. . From the product rule, one can easily derive Bayes&#39; Theorem $$ begin{align*} overbrace{P(X lvert Y, I)}^{ small{ mathrm{posterior}}} &amp;= frac{ overbrace{P(Y lvert X,I)}^{ small{ mathrm{likelihood}}} times overbrace{P(X lvert I)}^{ small{ mathrm{prior}}}}{ underbrace{P(Y lvert I)}_{ small{ mathrm{evidence}}}}&amp; textrm{(Bayes&#39; Theorem)} label{e:bayes} end{align*}$$ which states that the &quot;posterior&quot; is proportional to the &quot;likelihood&quot; times the &quot;prior&quot;, with the &quot;evidence&quot; a normalization constant. Posterior, likelihood and prior are the standard terms for these quantities, but the &quot;evidence&quot; label seems not to be widespread (we are following Sivia&#39;s nomenclature here). Note that there is nothing special about these labels, one&#39;s prior can be another&#39;s posterior, and your posterior today can be your prior tomorrow. Basically, the posterior is what we want to calculate, but can&#39;t (at least not directly), the prior is what we start with, and the likelihood is something we can easily calculate. . The sum rule can be extended to the to a series of mutually exclusive and exhaustive set of propositions $ {Y_k }$, such that $ sum_k P(Y_k lvert I) = 1$, and in the continuum limit we obtain the following marginalization property . $$ begin{align*} P(X lvert I) &amp;= int textrm{d}Y P(X, Y lvert I) = int textrm{d}Y P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Marginalization)} label{e:marginalization} end{align*}$$This is incredibly useful when there are unknown quantities that are required to compute the likelihood, but which are not really of interest to us. These so-called nuisance parameters can then be introduced and integrated out. We are being a bit careless here, and intermixing probabilities with probability densities. Strictly speaking, if $Y$ is a real random variable we should say that the probability for $Y$ to be within the range $y le Y le y+ Delta y$ is . $$ begin{align*} P(y le Y le y + Delta y)= rho(Y=y) Delta t end{align*}$$with $ rho$ the probability density function. However, following Sivia we will use the same symbol for both quantities, as it should be apparent from the context which one we are referring to. . Bayesian Data Analysis . It is easier to recognize how this Bayesian framework fits within a Data Analysis problem if we rewrite Bayes theorem in the form . $$ begin{align*} P( textrm{Hypothesis} lvert textrm{Data}, I) &amp;= frac{P( textrm{Data} lvert textrm{Hypothesis}, I) times P( textrm{Hypothesis} lvert I)}{P( textrm{Data}, I)} label{e:bayes_hypothesis} end{align*}$$or as . $$ begin{align*} P( Theta lvert D, I) &amp;= frac{P(D lvert Theta, I) times P( Theta lvert I)}{P(D lvert I)} label{e:bayes_theta} &amp; propto P(D lvert Theta, I) times P( Theta lvert I) end{align*}$$where $ Theta$ denotes the parameters of our model, $D$ our measured experimental/simulation data. The questions we are looking to answer, which these posteriors allow us to formulate are the following: &quot;What does my data say about my hypothesis&quot; or &quot;What does my data say about the model parameters&quot;? For what follows, we will focus exclusively on the parameter estimation problem. . One shot or sequential analysis? . Assume that our experimental data consists of a series of $N$ measurements, $D = {D_k }_{k=1}^N$. Our prior information $I$ specifies the model we believe explains this data, and we want to infer the parameters of this model. . Thus, what we want is the posterior of $ Theta$ given $D$, which gives $$ begin{align*} P( Theta lvert D,I) &amp; propto P( {D_k }_{k=1}^N lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert D_1, Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=3}^N lvert Theta, I) times P(D_2 lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp; vdots &amp;= left( Pi_{k=1}^N P(D_k lvert Theta, I) right) times P( Theta lvert I) end{align*}$$ . In step $2$, we are using the product rule to peel off one of the $D_i$, then in step $3$, we assume that (given the prior and the model parameters), knowledge of $D_1$ does not give us any extra information regarding the probabilities of observing $ {D_k }_{k ge 2}$, which means that they are independent. This procedure can be repeated until the original (one-shot) likelihood, which quantifies the probability of obtaining the entire set of data, is written as a product of individual likelihoods. From this, it is easy to see that we can analyze the data in one-shot, or sequentially, as the measurements arrive. The two routes are equivalent. . To see this, consider the expression obtained after peeling off m elements from the original likelihood. The posterior is given as $$ begin{align*} P( Theta lvert D,I)&amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) big( Pi_{j=1}^{m} P(D_j lvert Theta, I) big) P( Theta lvert I) &amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) times P( Theta lvert {D_j }_{j=1}^m, I) end{align*}$$ where the second and third terms in the rhs of the first equation are nothing but the likelihood and the prior after obtaining the first $m$ measurements. Thus, we can analyze the first $m$ data points, obtaining a posterior $P( Theta lvert {D_j }_{j=1}^m, I)$, and then use this as a prior when analyzing the remaining $N-m$ points. . What we can never ever do, however, is to use the posterior $P( Theta lvert {D_j }_{j=1}^m, I)$ to re-analyze the $ {D_j }_{j=1}^m$ measurements! . Laplace&#39;s Approximation, Best Estimates and Error Bars . Everything we can say about our parameters, given the Data and prior information, is contained in the posterior $P( Theta lvert D, I)$. However, this quantity can be quite complicated to deal with, particularly when the number of parameters exceeds 3. Therefore, it can be useful to develop approximations to this posterior. In particular, if the distribution is simple enough we can try to approximate it with a Gaussian distribution. Then, all that are needed are the first two moments, i.e., the average and the variance. The average give the peak of the distribution (the most likely point) and the variance an estimate of its spread, and thus of the reliability of our estimate. . This approximation is commonly referred to as Laplace&#39;s approximation, and amounts to a Taylor expansion of the logarithm of the posterior around the maximum (truncated to second order). Consider the general multi-dimensional case with $p$ parameters, $ Theta = ( Theta^{1}, Theta^{2}, ldots, Theta^{p})$, and let $ Theta_0 = text{argmax}{P( Theta lvert D, I)}$ . $$ begin{align*} L &amp;= ln{P( Theta lvert D, I)} = ln P &amp;= L lvert_{ Theta_0} + nabla L lvert_{ Theta_0} cdot( Theta - Theta_0) + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) + mathcal{O} big(( Theta- Theta_0)^3 big) &amp; simeq L lvert_{ Theta_0} + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) end{align*}$$where $ nabla= nabla_{ Theta}$, $A^t$ denotes the transpose of $A$, $( cdot)$ a matrix-matrix multiplication, and $ nabla nabla L$ is the (Hessian) matrix of second derivatives. Notice that the first-order term vanishes, since $ nabla L lvert_{ Theta_0} = 0$ by definition. . We can then re-exponentiate this expression to obtain an approximation to our original posterior distribution $$ begin{align*} P( Theta lvert D,I) propto exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot left(- nabla nabla L lvert_{ Theta_0} right) cdot left( Theta- Theta_0 right) right]} end{align*}$$ . Multi-variate Gaussians . Recall the definition of a multi-variate Gaussian distribution for a random $d$-dimensional vector $A$, expressed as $A sim mathcal{N}( mu, Sigma)$ $$ begin{align*} mathcal{N}(A lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^d det{ Sigma}}} exp{ left[- frac{1}{2} left(A- mu right)^t cdot Sigma^{-1} cdot left(A- mu right) right]} left langle A right rangle &amp;= mu left langle(A^i - mu^i)(A^j - mu^j) right rangle &amp;= Sigma^{ij} int text{d}A mathcal{N}(A lvert mu, Sigma) &amp;=1 end{align*}$$ where $ langle cdot rangle$ denotes an average over $P_{ mathcal{N}}$. . Such distributions have many interesting properties. In particular, if A is partitioned into $B$ and $C$, $A= begin{pmatrix}B C end{pmatrix}$, the marginalization over $B$ ($C$), results in another multi-variate Gaussian distribution for $C$ ($B$) $$ begin{align*} P(B lvert mu, Sigma) &amp;= int textrm{d}C mathcal{N} left( left. begin{pmatrix}B C end{pmatrix} right lvert begin{pmatrix} mu_B mu_C end{pmatrix}, begin{pmatrix} Sigma_{BB} &amp; Sigma_{BC} Sigma_{CB}&amp; Sigma_{CC} end{pmatrix} right) &amp;= frac{1}{ sqrt{(2 pi)^{d_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} left(B - mu_B right)^t Sigma_{BB}^{-1} left(B - mu_B right) right]} &amp; equiv mathcal{N}( mu_{B}, Sigma_{BB}) end{align*}$$ This can be easily proved by completing the squares in the exponent of the joint distribution. . Thus, since each component of $A$ is itself described by a Gaussian distribution, the best estimate is given by $$ begin{align*} A &amp;= mu pm sigma sigma &amp;= sqrt{ text{diag}{ Sigma}} end{align*}$$ . Finally, we see that the Laplace approximation of the posterior distribution for $ Theta$ is nothing but a multi-variate Gaussian, with average and covariance matrix given by $$ begin{align*} mu&amp; rightarrow Theta_0 Sigma&amp; rightarrow (- nabla nabla L lvert_{ Theta_0})^{-1} end{align*}$$ . Bayesian Parameter Estimation . Fitting with (known) noise . Let&#39;s see how this Bayesian formalism can be applied in practice. Our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. . The posterior for $ Theta$, given the Data $D$ and the prior information $I$, is then $$ begin{align*} P( Theta lvert D, I) &amp; propto P(D lvert Theta, I) times P( Theta lvert I) &amp; propto Pi_k P(y_k lvert Theta, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{ big(y_k - f(X_k; Theta) big)^2}{2 sigma_k^2} right]} end{align*}$$ where we have assumed that : . The prior $P( Theta lvert I)$ is a constant | The $y_k$ are independent of each-other (on knowing $ Theta$ and $I$), such that $P(y_i,y_j lvert Theta, I) = P(y_i lvert Theta, I) P(y_j lvert Theta, I)$ for $i ne j$. | The measurement error, and thus the likelihood, is Gaussian with variance $ sigma_k^2$ | . Let&#39;s see what the best-estimate would be in this case. . In practice, it will be more convenient to work with the logarithm of the probabilities, than with the probabilities themselves. Since $ ln$ is a monotonically increasing function, the maximum of $P$ will coincide with the maximum of $ ln{P}$. The log posterior is given by . $$ begin{align*} L &amp;= textrm{constant} - sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma_k^2} &amp;= textrm{constant} - frac{1}{2} chi^2 end{align*}$$where $ chi^2= sum_k(y_k - f_k)^2/ sigma_k^2$ is the sum of the squared residuals, weighted by the inverse of the error, and all terms not depending on $ Theta$ have been absorbed into the normalization constant. . The best estimate $ Theta_0$, which maximizes the posterior, is found by setting the gradient of $L$ to zero, . $$ begin{align*} nabla L = - frac{1}{2} nabla chi^2 = 0 Longrightarrow nabla chi^2 = 0 end{align*}$$which coincides with setting the gradient of $ chi^2$ to zero. Thus, in this case, maximizing the posterior is equivalent to a least-squares fit! . Finally, our reliability estimate is obtained by computing the matrix of second derivatives, evaluated at the optimal value $ Theta_0$ $$ begin{align*} Sigma_{ Theta}^{-1}=- nabla nabla L lvert_{ Theta_0} = frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0} end{align*}$$ . Approximating our posterior as a Gaussian, our best estimate for the model parameters would be $$ begin{align*} Theta = Theta_0 pm sigma_{ Theta} ; qquad left( sigma_{ Theta}= sqrt{ text{diag}( Sigma_{ Theta})} right) end{align*}$$ . Fitting with uknown noise . What happens in the case where don&#39;t have an estimate of the error bars? How would we derive the best-estimate in this case? What quantity should we minimize? . The solution is simply, use marginalization and average over all possible values of $ sigma$ (assumed to be the same for all points) $$ begin{align} P( Theta lvert D, I) &amp;= int textrm{d} sigma , P( Theta, sigma lvert D, I) &amp; propto int textrm{d} sigma , P(D lvert Theta, sigma, I) times P( Theta, sigma lvert I) end{align}$$ where the last step uses Bayes&#39; to write the integrand as the likelihood and the prior. Notice that this is the average of $P(D lvert Theta, sigma, I)$ over $P( Theta, sigma lvert I)$. . Under the same simplifying assumptions as above, i.e., independent measurements, uniform prior for $ Theta$, and Gaussian noise, we can again decompose the likelihood of $D$ into a product of likelihoods for the individual measurements $$ begin{align} P( Theta lvert D, I) &amp; propto int_0^ infty textrm{d} sigma left( Pi_k frac{1}{ sqrt{2 pi} sigma} exp{ left[- frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} right]} right) times frac{1}{ sigma} &amp;= (2 pi)^{-N/2} int_0^ infty frac{ textrm{d} sigma}{ sigma} sigma^{-N} exp{ left[- frac{1}{2 sigma^2} sum_k left(y_k - f(X_k; Theta) right)^2 right]} end{align}$$ . Here we have used Jeffreys&#39; prior for $ sigma$, $P( sigma lvert I) propto 1/ sigma$, as it is a scale-parameter. Depending on your prior information (e.g., do you know the order of magnitude?), there may be more appropriate priors, but we leave that for a future discussion. See Sivia and or Jaynes for a more in depth review of such issues. In practice, if $N$ is large enough this will not make a big difference, choosing Jeffreys prior or a constant prior for $ sigma$ will give essentially the same predictions. . The integral over $ sigma$ can be done analytically through the following variable transformation, with $S = sum_k (y_k - f_k)^2$ $$ begin{align} t &amp;= frac{ sum_k left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} = frac{S}{2 sigma^2} frac{ textrm{d} sigma}{ sigma} &amp;= - frac{ textrm{d}t}{2t} end{align}$$ . The posterior is then $$ begin{align} P( Theta lvert D, I) &amp; propto (2 pi)^{-N/2} int_0^ infty frac{ textrm{d}t}{2t} left( frac{2t}{S} right)^{N/2} e^{-t} &amp;= frac{ pi^{-N/2}}{2} underbrace{ left( int_0^ infty frac{ textrm{d}t}{t} t^{N/2} e^{-t} right)}_{= Gamma(N/2)}S^{-N/2} end{align}$$ . where the quantity in parenthesis is independent of $ Theta$ and equal to the Gamma function of $N/2$. . As before, let&#39;s look at the log-posterior . $$ begin{align} L &amp;= textrm{constant} - frac{N}{2} ln{S} end{align}$$The best-estimate is again that which maximizes $L$ $$ begin{align} nabla L &amp;= - frac{N}{2} frac{ nabla S}{S}= 0 Longrightarrow nabla S = 0 end{align}$$ which in this case corresponds to minimizing $ ln{S}$ or just $S$. So far it seems that the procedure is the same, minimize the sum of the squared residuals (weighted by the magnitude of the error, if known). The differences come when we consider the reliability estimate. . The Hessian is now $$ begin{align} nabla nabla L &amp;= - frac{N}{2} left[ frac{ nabla nabla S}{S} - frac{ left( nabla S right) left( nabla S right)}{S^2} right] end{align}$$ . which reduces to the following when evaluated at the optimum point $ Theta_0$ (since $ nabla S lvert_{ Theta_0} equiv 0$) $$ begin{align} nabla nabla L lvert_{ Theta_0} &amp;= - frac{N}{2} frac{ nabla nabla S lvert_{ Theta_0}}{S_0} = - frac{1}{2} left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} end{align}$$ . Within the Laplace approximation, the covariance matrix for the posterior distribution of $ Theta$ is then given by $$ begin{align} Sigma_ Theta^{-1} &amp;= frac{1}{2} left( left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} right) end{align}$$ . This expression is very similar to the one we obtained above, in the case where the measurement error $ sigma$ was known, $ Sigma_ theta^{-1}=- frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0}$. Here, we have replaced $ sigma_k^2$ with an estimate derived from the data $S_0/N$ . $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{ sigma_k^2} &amp; longrightarrow frac{S}{S_0/N} = frac{N}{S_0} sum_k left(y_k - f(X_k; Theta) right)^2 sigma_k^2 &amp; longrightarrow frac{S_0}{N} = frac{1}{N} sum_{k} left(y_k - f(X_k; Theta_0) right)^2 end{align}$$However, $S_0/N$ is not necessarily the best estimate for the amplitude of the noise, as we will see below. . ... about the noise . What does the data say about the noise, i.e., what is $P( sigma lvert D, I)$? . To be able to compute this quantity, we again make use of marginalization, treating the model parameters as nuisance parameters and integrating over them. $$ begin{align} P( sigma lvert D, I) &amp;= int textrm{d} Theta P( sigma, Theta lvert D, I) &amp; propto int textrm{d} Theta P(D lvert sigma, Theta, I) P( sigma, Theta lvert I) end{align}$$ . Assuming $ sigma$ and $ Theta$ are independent, $P( sigma, Theta lvert I)= P( sigma lvert I) P( Theta lvert I)$, and taking a uniform prior for $ Theta$ and Jeffreys prior for $ sigma$, we have $$ begin{align} P( sigma lvert D, I) propto sigma^{-(N+1)} int textrm{d} Theta exp{ left[- frac{S( Theta)}{2 sigma^2} right]} end{align}$$ where $S$ is still the sum of squared residuals $S = sum_k (y_k - f(X_k; Theta))$. We get one factor of $1/ sigma$ from the normalization constant of the likelihood for each data measurement, and one from the prior. . Without having to assume anything regarding the form of the model function $f(X)$, we can proceed by again using the Laplace approximation, now expanding $S( Theta)$ around the minimum $ Theta_0$ (which corresponds to the maximum of the posterior $P( Theta lvert D, I)$) $$ begin{align} S simeq S_0 + nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) + frac{1}{2} left( Theta - Theta_0 right)^t cdot nabla nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) end{align}$$ . The posterior $P( sigma lvert D, I)$ is then $$ begin{align} P( sigma lvert D, I)&amp; propto sigma^{-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot frac{ nabla nabla S lvert_{ Theta_0}}{2 sigma^2} cdot left( Theta- Theta_0 right) right]} &amp; propto sigma^{p-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} end{align}$$ where the $p$-dimensional integral over $ Theta$ gives us a factor of $ sqrt{(2 pi)^p det{ Sigma}_{ sigma}}$, with $ Sigma_ sigma^{-1} = - frac{1}{2 sigma^2} nabla nabla S lvert_{ Theta_0}$. . To find the best-estimate for $ sigma$ we maximize the log posterior $$ begin{align} L &amp;= ln{P( sigma lvert D, I)} = textrm{const} + left(p - N - 1 right) ln{ sigma} - frac{S_0}{2 sigma^2} frac{ textrm{d}L}{ textrm{d} sigma} &amp;= (p-N-1) frac{1}{ sigma} + frac{S_0}{ sigma^3} frac{ textrm{d}^2L}{ textrm{d}^2 sigma} &amp;= (N+1-p) frac{1}{ sigma^2} - frac{3 S_0}{ sigma^4} end{align}$$ . Setting $ text{d} L / text{d} sigma lvert_{ sigma_0} = 0$, we get $$ begin{align} sigma_0^2 = frac{S_0}{N+1-p} end{align}$$ . with the reliability estimates given by the inverse of the Hessian $$ begin{align} left. frac{ textrm{d}^2L}{ textrm{d}^2 sigma} right lvert_{ sigma_0} &amp;= frac{(N+1-p) sigma_0^2 - 3S_0}{ sigma_0^4} &amp;= frac{-2(N+1-p)}{ sigma_0^2} end{align}$$ . we get $$ begin{align} sigma = sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align}$$ . Example : Fitting a Straigh-Line (known measurement error) . Now, let&#39;s see how all this works out in practice. For convenience, we will use the jax library and take advantage of its automatic differentiation capabilities, which allow us to calculate the gradients and Hessians exactly (to machine precision). If you are on Windows this will not work, in that case remove all the jax commands. . Within the Laplace approximation, we can work out the best estimates for all our quantities by simply maximizing the appropriate posterior. However, to compare with the full solution, we will also make use of the PYMC3 library, which allows us to perform Hamiltonian Monte-Carlo simulations and generate samples which converge to those of the posterior. For the minimiztion, we make use of the scipy library. . Please install the following packages to run the code below . numpy | matplotlib | pymc3 | theano | scipy | jax | . #import numpy as np #### If using windows... import jax.numpy as np import numpy as onp import numpy.random as random import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano import theano.tensor as tt from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, scatter=False, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . We define a linear and quadratic function of a scalar variable $x$, which takes as first argument a list of parameters $ Theta$. We then define a routine to create noisy versions a function. . def linear_func(θ, x): return θ[0] + θ[1]*x # y = mx + b def quadratic_func(θ, x): return θ[0] + θ[1]*x + θ[2]*x**2 # y = a x^2 + bx + c def make_noisy_func(f, σ): &quot;&quot;&quot;Returns function that computes noisy measurements for y ~ N(f, σ^2), such that y will be normally distributed around f, with variance σ^2&quot;&quot;&quot; return lambda θ,x : f(θ,x) + σ*random.randn(len(x)) . Now let&#39;s simulate measuring using a noisy linear function (intercept $b=2$, slope $m=1$). We discretize $x$ over $N_p=10$ points evenly spaced in the range $[0,2]$, and simulate $N=8$ draws. Computing the average and variance over these $N$ sets of measurements gives us our best estimate and error bar for the value of $f$ at $x_i$. . random.seed(12345) nparams,func = 2, linear_func #nparams,func = 3, quadratic_func _θ0,σ0 = np.array([2.0, 1.0, 0.6]), 0.25 # exact parameter values θ0 = _θ0[:nparams] measurement = make_noisy_func(func, σ0) npoints,ndraws= 10,8 x_sample = np.linspace(0, 2.0, num=npoints) y_sample = np.array([measurement(θ0,x_sample) for i in range(ndraws)]) y_avg, y_err = np.average(y_sample, axis=0), np.std(y_sample, axis=0) fig, ax = plt.subplots() for yi in y_sample: ax.plot(x_sample, yi, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, label=&#39;Estimate&#39;, color=color[&#39;dark&#39;]) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . The symbols represent our &quot;measurement&quot;, 8 for each of the 10 $x$ values we are sampling. The average and variance of these measurements would give your experimental estimate (shown here with the error bar plot). . The &quot;Least-Squares&quot; route . Let&#39;s first attempt a least-squares minimization. . Recall the definition of $ chi^2$ $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(x_k; Theta) right)}{ sigma_k^2} end{align}$$ . For simple functions, it is trivial to work out the first and second derivatives of this quantity, but jax does all the work for us! As shown above, the best-estimate for $ Theta$ is given by minimizing $ chi^2$, and the reliability estimates are given by (twice) the inverse of the Hessian $$ begin{align*} Theta &amp;= Theta_0 pm sqrt{ text{diag}( Sigma_{ Theta})} Sigma_{ Theta}&amp;= 2 left( nabla nabla chi^2 lvert_{ Theta_0} right)^{-1} end{align*}$$ . For a the simple linear model we are considering, we can easily work out the analytical solution . $$ begin{align} partial_m chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right) x_k}{ sigma_k^2} qquad &amp; partial_b chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right)}{ sigma_k^2} nabla chi^2 &amp;= begin{pmatrix} partial_m chi^2 partial_b chi^2 end{pmatrix} = begin{pmatrix} sum w_k x_k^2 &amp; sum w_k x_k sum w_k x_k &amp; sum w_k end{pmatrix} begin{pmatrix} m b end{pmatrix} - begin{pmatrix} sum w_k x_k y_k sum w_k y_k end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} begin{pmatrix}m b end{pmatrix} - begin{pmatrix}p q end{pmatrix} end{align}$$ with $ alpha = sum w_k x_k^2$, $ beta = sum_k w_k$, $ gamma= sum_k w_k x_k$, $w_k = 2/ sigma_k^2$, $p= sum_k w_k x_k y_k$ and $q= sum_k w_k y_k$. The optimum solution is then . $$ begin{align*} nabla chi^2 &amp;= 0 begin{pmatrix}m b end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix}^{-1} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta- gamma^2} begin{pmatrix} beta p - gamma q alpha q - gamma p end{pmatrix} end{align*}$$Finally, the covariance matrix $ Sigma_{ Theta} = 2 ( nabla nabla chi^2)^{-1}$ is just . $$ begin{align*} nabla nabla chi^2 &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} Sigma_{ Theta} &amp;= 2 frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} end{align*}$$ @jit def chi2(θ): return np.sum((y_avg-func(θ,x_sample))**2/y_err**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 def exact_sol(xk,yk,dyk): wk = 2/dyk**2 α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_avg, y_err) . opt = minimize(chi2, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_chi2(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 1.118355 Iterations: 5 Function evaluations: 7 Gradient evaluations: 7 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.2f}]&#39;) . |θ - Θ_exact|_inf = 2.442491e-15 |H - H_exact|_inf = 3.469447e-18 |H_min - H_exact|_inf = 1.040834e-17 1.69 &lt; θ^1 (b) &lt; 2.24 [μ = 1.961, σ = 0.14] 0.79 &lt; θ^2 (m) &lt; 1.24 [μ = 1.018, σ = 0.11] . The exact values used to generate the data were $ Theta^1 = b = 2$ and $ Theta^2 = m = 1$, which is very close to the optimal value obtained here. Furthermore, we see that the approximate of the Hessian returned by the minimization routine coincides with the exact result (given by jax). We use $ pm 2 sigma$ interval as a measure of the error bounds. . The full Bayesian route . Now let&#39;s see how to get the &quot;full&quot; solution, not just a point-estimate. For this we use PYMC3 to generate samples from the posterior. To compare with our previous analysis, we will assume a uniform prior for both $m$ and $b$. . with pm.Model() as model: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) # true function y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) # measured data (accounting for noise) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=y_err, observed=y_avg) # generate samples from the prior (before looking at the data) prior = pm.sample_prior_predictive(samples=1000, random_seed = 123456) # generate samples from the posterior trace = pm.sample(5000, tune = 20000, progressbar=True) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [b, m] INFO:pymc3:NUTS: [b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 50000/50000 [00:20&lt;00:00, 2415.40draws/s] The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. . First, let us try to get an idea for how good/bad our priors are. We can do this by looking at the prior predictive distribution. This simply takes parameters values sampled from the prior, and pushes them through the model, generating corresponding samples for $y$. If we plot $y-b = mx$ we can clearly see that this prior is heavily skewed towards lines with high slopes. Is this what we want? Probably not, but this will depend on our prior information. . fig, ax = plt.subplots() for y_i,b_i in zip(prior[&#39;y&#39;], prior[&#39;b&#39;]): ax.plot(x_sample, y_i-b_i, alpha=0.2, color=&#39;k&#39;) ax.set_xlabel(r&#39;x&#39;) ax.set_ylabel(r&#39;y&#39;) ax.set_ylim(0,2) ax.set_xlim(0,2) plt.show() . PYMC3 comes with many useful post-processing functions to visualize our data. Let&#39;s look at a few of them. . pm.traceplot(trace) plt.show(); . Above, we see the distribution of the model parameters, with different different lines corresponding to different chains or independent runs of the MC simulation. At first glance, there doesn&#39;t seem to be any obvious divergence and we get nice Gaussian-like distributions for all model parameters (this includes the values of the function at the measured points). . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace, var_names=[&#39;m&#39;, &#39;b&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.show(); . &lt;Figure size 504x504 with 0 Axes&gt; . Here we have the $94 %$ credible intervals, corresponding to the $ mu pm 2 sigma$ interval in the case of a Gaussian distribution. The vertical lines indicate the real values that were used to generate the data. . pm.plot_posterior(trace, var_names=[&#39;m&#39;, &#39;b&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fff4d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fc8a10&gt;], dtype=object) . Here we have the posterior distribution for our two model parameters. Notice that the $94 %$ HPD (Highest Posterior Density Interval) corresponds exactly to that obtained using the Laplace approximation. This should not be surprising, when we see the posterior. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace[&#39;y&#39;], quantiles_sig) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, marker=&#39;o&#39;, color=color[&#39;superfine&#39;], alpha=0.5, label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], alpha=1, label=&#39;exact&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.legend() plt.show() . The shaded red regions show the $ sigma$, $2 sigma$ and $3 sigma $ regions $y$ obtained from the posterior samples of y. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace[&#39;b&#39;], trace[&#39;m&#39;], scatter=True, colors=[color[&#39;mid&#39;], color[&#39;light_highlight&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) plt.show() . Here we see the trace generated by the MC simulation, where the distribution of points approximates the the posterior distribution. Also drawn are the $ pm sigma$ and $ pm 2 sigma$ contour levels, which should contain $68 %$ and $95 %$ of the points, respectively. . We can redo the analysis using a &quot;better&quot; prior, e.g., assigning a uniform prior on the angles of the lines instead of their slope, but the results will not vary significantly (unless the number of data points is reduced). . Example : Fitting a straight line (with unknown measurement error) . The &quot;Least-Squares&quot; route . Now let&#39;s analyze the case where the measurement error is not known. As shown above, instead of minimizing $ chi^2$, we should maximize $L=- frac{N}{2} ln{S}$, with $S= sum_k (y_k -f_k)^2$, which coincides with minimizing $S$. Thus, the form of the solution is the same as before, we simply set $w_k=2 , ( sigma_k=1)$, as all the points are given equal weight. Recall that the covariance matrix is $ Sigma_ Theta^{-1} = frac{1}{2} nabla nabla frac{S}{S_0/N} lvert_{ Theta_0}$. . @jit def loss(θ): return np.sum((y_sample[0]-func(θ,x_sample))**2) grad_loss = jit(grad(loss)) hess_loss = hessian(loss) def exact_sol(xk,yk): wk = 2.0*np.ones_like(yk) α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_sample[0]) opt = minimize(loss, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) . Optimization terminated successfully. Current function value: 0.323150 Iterations: 4 Function evaluations: 6 Gradient evaluations: 6 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2*loss(opt[&#39;x&#39;])/npoints) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.3f}]&#39;) . |θ - Θ_exact|_inf = 2.220446e-16 |H - H_exact|_inf = 2.775558e-17 |H_min - H_exact|_inf = 4.163336e-17 1.77 &lt; θ^1 (b) &lt; 2.20 [μ = 1.984, σ = 0.106] 0.96 &lt; θ^2 (m) &lt; 1.32 [μ = 1.140, σ = 0.089] . Within the Gaussian approximation, we can also use this optimum solution to give an estimate of the unknown measurement error . $$ begin{align*} sigma_0^2 &amp;= frac{S_0}{N+1-p} sigma &amp;= sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align*}$$ σa = np.sqrt(loss(opt[&#39;x&#39;])/(npoints + 1 - nparams)) dσa= σa / np.sqrt(2*(npoints + 1 - nparams)) print(f&#39;{σa - 2*dσa:.2f} &lt; σ &lt; {σa + 2*dσa:.2f} [μ = {σa : .3f}, σ = {dσa : .3f}]&#39;) . 0.10 &lt; σ &lt; 0.28 [μ = 0.189, σ = 0.045] . Which is in good agreement with the exact value used to generate the noisy data ($ sigma = 0.25$). . fig, ax = plt.subplots() ax.plot(x_sample, y_sample[0], marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.plot(x_sample, func(opt[&#39;x&#39;],x_sample), ls=&#39;-&#39;, color=color[&#39;dark&#39;], alpha=1, label=&#39;Estimate&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . with pm.Model() as model_sig: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) logσ = pm.Uniform(&#39;logσ&#39;, lower=-6, upper=6) σ = pm.Deterministic(&#39;σ&#39;, tt.exp(logσ)) y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=σ, observed=y_sample[0]) prior_sig = pm.sample_prior_predictive(samples=1000, random_seed = 123456) trace_sig = pm.sample(5000, tune = 50000, progressbar=True) #post_sig = pm.sample_posterior_predictive(trace_uni, model=model_sig, random_seed=4938483) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logσ, b, m] INFO:pymc3:NUTS: [logσ, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 110000/110000 [01:08&lt;00:00, 1606.14draws/s] . plt.figure(figsize=(12,12)) pm.traceplot(trace_sig) plt.show(); . &lt;Figure size 864x864 with 0 Axes&gt; . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.axvline(σ0, 0, 1, c=&#39;C1&#39;) plt.show() . &lt;Figure size 504x504 with 0 Axes&gt; . pm.plot_posterior(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); . Notice how the resulting distribution for $ sigma$ is no longer symmetric. In this case we expect the Gaussian approximation will start to break down. However, the error bounds derived above, $0.1&lt; sigma &lt; 0.28$ are still close to the &quot;real&quot; values provided by the MC trace. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace_sig[&#39;y&#39;], quantiles_sig, label=&#39;prediction&#39;) ax.plot(x_sample, y_sample[0], ls=&#39;None&#39;, marker=&#39;o&#39;, mfc=&#39;None&#39;, mew=2, color=color[&#39;dark&#39;], label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], label=&#39;exact&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fe5629e4c90&gt; . Finally, here is the list of packages in my environment. . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "relUrl": "/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://johnjmolina.github.io/MLKyoto/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johnjmolina.github.io/MLKyoto/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://johnjmolina.github.io/MLKyoto/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://johnjmolina.github.io/MLKyoto/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}