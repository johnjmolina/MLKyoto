{
  
    
        "post0": {
            "title": "Nested Sampling",
            "content": "We review the basics of Nested Sampling, as introduced by Sivia and Skilling in their &quot;Data Analysis&quot; book. . References . We again use Sivia&#39;s book as the main source, a partial list of references (including Statistical Physics applications) is here . Data Analysis : A Bayesian Tutorial, second edition. D. S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Bayesian Probability Theory: Applications in the Physical Sciences. Wolfgang von der Linden, Volker Dose, and Udo von Toussaint, Cambridge, Cambridge University Press (2014) | Determining pressure-temperature phase diagrams of materials, Robert J. N. Baldock et al., Physical Review B 93, 174108 (2016) | Nested sampling, statistical physics and the Potts model, Manuel J. Pfeifenberger, Michael Rumetshofer, Wolfgang von der Linden, Journal of Computational Physics 375, 368 (2018) | Dynesty: A Dynamic Nested Sampling Package for Estimating Bayesian Posteriors and Evidences, Joshua S. Speagle, Monthly Notices of the Royal Astronomical Society 493, 3132 (2020) | Dynesty: Documentation for the Python Package | . Preliminaries . #collapse import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.patheffects as PathEffects from mpl_toolkits.axes_grid1 import make_axes_locatable from scipy import integrate, stats from scipy.special import factorial, gamma from tqdm import tqdm, trange from functools import partial,reduce from numpy import random mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def colorbar(axes, mappable, *, loc=&quot;right&quot;, size=&quot;5%&quot;, pad=.1): divider = make_axes_locatable(axes) cax = divider.append_axes(loc, size=size, pad=0.1) cb = plt.colorbar(mappable = mappable, cax = cax) return cb def mappable(*, cmap = mpl.cm.viridis, vmin=0, vmax=1): sm = plt.cm.ScalarMappable(cmap=cmap, norm=mpl.colors.Normalize(vmin=vmin,vmax=vmax)) sm.set_array([]) return sm . . Nested Sampling . Stick-Breaking Statistics . The &quot;trick&quot; behind Nested-Sampling is to discretize the evidence integral over a geometric series of values following a stick-breaking procedure, described as follows. Draw $N$ random points $x_i$, uniformly between $[0,X^ star]$. Let the largest or &quot;worst&quot; of these points be at $X = max{x_i}$. Now, cut the interval at $X$, and define the shrinkage ratio to be $t=X/ X^ star$. The upper value on the constraint is updated, such that $X^ star leftarrow X$, and the procedure is repeated in order to again obtain $N$ uniformly distributed points. Note that all points except the previously identified &quot;worst&quot; point already satisfy this condition. Thus, we just need to reintroduce this point, uniformly within the updated interval. We use this procedure because in most cases it will be impossible to evaluate $x$, so instead we will try to guess the values, using our knowledge of the statistics of this stick-breaking process. . The question we need to answer is the following: At any given step, what is the probability distribution for the largest value to be located at $X = t X^ star$? This is the probability distribution for any one of the $N$ points to be at $X$ (we don&#39;t care about the labeling), with all others at smaller positions . begin{align*} P(X = t X^ star) &amp;= sum_{i=1}^N P(x_i = t X^{ star}, x_{j ne i} &lt; t X^ star) &amp;= N P(x_1 = t X^ star, x_{j &gt; 1} &lt; t X^ star) &amp;= N P(x_1 = t X^ star lvert x_{j&gt;1} &lt; t X^ star) P(x_{j&gt;1} &lt; t X^ star) &amp;= N P(x_1 = t X^ star) prod_{j&gt;1} P(x_{j} &lt; t X^ star) &amp;= N P(x_1 = t X^ star) prod_{j&gt;1} int_0^{t X^ star} text{d}X^ prime P(x_{j} = X^ prime) &amp;= N frac{1}{X^ star} t^{N-1} end{align*}Here we have used the fact that the points are independent and uniformly distributed over $X^ star$, such that $P(x_i = x) = frac{1}{X^ star}$. . The corresponding distribution for the shrinkage ratios is just begin{align} P(t) &amp;= N t^{N-1} label{e:pt} end{align} . For what follows it will be useful to have the expected values (average) and variance for $ log(t)$, which are begin{align*} text{E}( log{t}) &amp;= - frac{1}{N} text{Var}( log{t}) &amp;= frac{1}{N^2} end{align*} . A sampling digression . To sample $x$ from a continuous distribution $ rho(x)$, we draw uniform random numbers $ gamma$ over the unit interval (i.e., the range of the cumulative distribution $ Pi(x)$), and then invert the relationship to find the appropriate $x$ begin{align} Pi(x) &amp;= int_{- infty}^x rho(x) text{d}x gamma &amp; longleftarrow text{ran}(0,1) x &amp;= Pi^{-1}( gamma) Longrightarrow x sim rho(x) end{align} . For the stick breaking procedure, we have $ gamma = Pi(t) = t^N$, such that $t = gamma^{1/N}$ . def stickbreaking(N, size): &quot;&quot;&quot;Return (size) number of random shrinkage factors for a stick breaking procedure with N live points&quot;&quot;&quot; return np.random.uniform(low=0, high=1.0, size=size)**(1/N) . Theory . Given a model and some measured data, we have seen that the posterior distribution for the model parameters $ Theta = ( theta_1, ldots, theta_N)$ is given by Bayes&#39; theorem as begin{align} P( Theta lvert D, I) &amp;= frac{ mathcal{L}( Theta) Pi( Theta)}{Z} label{e:posterior} Z &amp;= int text{d} Theta mathcal{L}( Theta) Pi( Theta) label{e:partition} end{align} with $ mathcal{L}( Theta) = P(D lvert Theta, I)$ the likelihood and $ Pi( Theta)$ the prior, such that begin{align*} int text{d} Theta Pi( Theta) &amp;= 1 int text{d} Theta P( Theta lvert D, I) &amp;= 1 end{align*} . We can define the density of states $g( lambda)$ as . begin{align} g( lambda) &amp;= int text{d} Theta delta( mathcal{L}( Theta) - lambda) Pi( Theta) label{e:dos} end{align}which, by definition, yields the probability of observing a state with likelihood $ lambda$ (among states distributed according to $ Pi( Theta)$). From this, we define the integrated density of states $ chi$ (note that Skilling uses $ xi$) begin{align} chi( lambda)&amp;= int_{ lambda}^ infty text{d} lambda^ prime g( lambda^ prime) = int_{ Theta : mathcal{L}( Theta)&gt; lambda} text{d} Theta , Pi( Theta) label{e:chi} end{align} which gives us the total amount of (prior) probability mass with likelihood greater than $ lambda$, with $ chi( lambda = 0) = 1$ and $ chi( lambda = infty)= 0$. Note that $g$ and $ chi$ are related through begin{align} frac{ text{d} chi}{ text{d} lambda} &amp; equiv -g( lambda) label{e:gchi} end{align} Finally, let us define the inverse of $ chi( lambda)$, as begin{align} mathscr{L}( chi( lambda)) &amp;= lambda label{e:chinv} chi( lambda) &amp;= mathscr{L}^{-1}( lambda) notag end{align} . To avoid confusion, we use a different symbol for the function $ mathcal{L}( Theta)$, which is a function of the parameters $ Theta$, and returns the corresponding likelihood value, to the function $ mathscr{L}( chi)$ which is a (scalar) function of the prior mass $ chi$, and returns the likelihood contour that contains that amount of prior mass. . Going back to Eq. eqref{e:partition}, we can rewrite the partition function as a simple 1D integral! . begin{align} Z &amp;= int text{d} Theta mathcal{L}( Theta) Pi( Theta) notag &amp;= int_0^ infty text{d} lambda g( lambda) notag &amp;= int_0^1 text{d} chi mathscr{L}( chi) notag &amp; simeq sum_k A_k label{e:ns} end{align}Following Skilling, we use the entropy to define the uncertainty in the calculation, with begin{align*} H &amp;= int P(X) log{ left[P(X) right]} text{d}X simeq sum_{k=1}^N frac{A_k}{Z} log{ left[ frac{L_k}{Z} right]} end{align*} . Here we have used a simple trapezoidal integration, discretizing $ chi in[0, 1]$ into $N$ bins, with edges at $( chi_N=0, chi_{N-1}, ldots, chi_0=1)$ begin{align} A_k &amp;= w_k L_k label{e:ak} w_k &amp;= chi_{k-1} - chi_{k} notag end{align} . Finally, not only can we compute the evidence integral, but we also get the posterior $P( Theta lvert D, I)$ for free from the same sampling points, by simply assigning each point its associated weight $w_k$ begin{align} P( Theta_i) = P(X_i) = frac{w_i}{Z} end{align} In this way, we can use the series of samples ($X_k, L_k$) to generate samples for $ Theta$ drawn from the posterior (see 9.4.1 in Sivia&#39;s book for the details.) . Ok ... but how? . If we could evaluate $ mathscr{L}( chi)$ then we would be finished. Unfortunately, this is usually not possible. Instead, the idea is to sample $ chi$ using the stick-breaking procedure, in such a way that we know it&#39;s location (on average). . Assuming we can uniformly sample $ chi$ points within the constraint, and evaluate the corresponding likelihood, then we can use the stick-breaking procedure described above in order to evaluate the partition function. However, in most cases we can only know $ Theta$ ($ mathcal{L}( Theta)$). It turns out that it is enough to be able to sample $ Theta$ from the prior $ Pi( Theta)$! . Assume we are sampling from $ Pi( Theta)$ within a given likelihood constraint $ lambda^ star$, by definition we must have begin{align} P_ Theta( Theta lvert lambda^ star) text{d} Theta &amp;= begin{cases} frac{ Pi( Theta) text{d} Theta}{ int_{ mathcal{L}( Theta) &gt; lambda^ star} text{d} Theta Pi( theta)} = frac{ Pi( Theta) text{d} Theta}{ chi( lambda^ star)} &amp; text{if} , mathcal{L}( Theta) ge lambda^ star 0 &amp; text{otherwise} end{cases} end{align} . The corresponding probability distribution for the likelihood is then begin{align} P_ lambda( lambda lvert lambda^ star) text{d} lambda &amp;= begin{cases} frac{ int text{d} Theta Pi( Theta) delta( mathcal{L}( Theta) - lambda) text{d} lambda}{ chi( lambda^ star)} = frac{g( lambda) text{d} lambda}{ chi( lambda^ star)} &amp; text{if} , lambda &gt; lambda^ star 0 &amp; text{otherwise} end{cases} end{align} but, given the variable transformation $g( lambda) = - text{d} chi/ text{d} lambda$, this implies the following for the distribution of $ chi( lambda) = mathscr{L}^{-1}( lambda)$ . begin{align} P_ chi( chi lvert chi^ star) text{d} chi= P_ lambda( lambda lvert lambda^ star) text{d} lambda &amp;= P_ lambda left( lambda( chi) lvert lambda^ star = mathscr{L}( chi^ star) right) left lvert frac{ text{d} lambda}{ text{d} chi} right rvert text{d} chi &amp;= begin{cases} frac{g( lambda( chi))}{ chi^ star} frac{1}{g( lambda( chi))} text{d} chi = frac{ text{d} chi}{ chi^ star} &amp; text{if} , chi &gt; chi^ star 0 &amp; text{otherwise} end{cases} end{align}Thus, points in configuration space $ Theta$ that are distributed according to the prior $ Pi( Theta)$, within a likelihood constraint $ mathcal{L}( Theta) &gt; lambda^ star$, will be distributed uniformly in $ chi$ space, within the interval $[0, chi^ star]$. Furthermore, by definition, we know that if we order the points by likelihood, they will also be ordered by $ chi$. For the stick-breaking procedure, this means that we don&#39;t need to be able to compute $ chi$ in order to identify the worst point. We can just use the likelihood $ mathcal{L}( Theta)$ as a proxy for $ chi$. . In conclusions, we will distribute the points in configuration space $ Theta$, and use the stick-breaking procedure to iteratively discretize the evidence integral over $ chi$, adding a new integration point at each step. We can&#39;t calculate the values of $ chi$, but we can guess them. Thus, everything depends on being able to distribute points according to the prior $ Pi( Theta)$, within the likelihood constraint $ lambda^ star$. . Implementation Details . At each iteration of the stick-braking procedure, we cut the stick at the worst position (the one with the lowest likelihood, largest $ chi$), and distribute the $N$ points uniformly within the remaining interval $[0, chi^ star]$. This requires that we replace the so-called &#39;dead&#39; point, which we just removed, by introducing a new &#39;live&#39; point uniformly within the constrained region $[0, chi^ star]$. Recall that this is equivalent to placing the points according to the constrained prior $P_{ Theta}( Theta lvert lambda^ star)$. . If the starting length of the interval is $ chi_0 = 1$, at iteration $k$ the worst point is given by ($t_k$ the shrinkage ratio) . begin{align} chi_k &amp;= t_{k} chi_{k-1} notag &amp;= left( prod_{i=1}^k t_i right) chi_0 notag log{ chi}_k &amp;= sum_{i=1}^k log{t_i} + log{ chi_0} end{align}By sampling over $t$, we can use this expression to obtain independent realizations of the $ chi_i$, which will provide better statistics for the partition function. . Alternatively, we can use the expected value for the $t_i$, with $ langle log{t} rangle = - 1 /N$, and assume that the $ chi$ will be located at the corresponding average values begin{align} log{ chi_k} &amp; simeq log{ chi_0} - frac{k}{N} pm frac{ sqrt{k}}{N} end{align} . In terms of the widths needed to compute the areas in the Riemann sum, we have begin{align*} chi_k &amp;= chi_0 e^{-k/N} = chi_0 e^{-(k-1)/N} e^{-1/N} = chi_{k-1} e^{-1/N} notag log{w_k} &amp;= log{ left[ chi_{k-1} - chi_k right]} &amp;= log{ chi_{k-1}} + log{ left(1 - e^{-1/N} right)} &amp;= log{ chi_0} + log{ left(1 - e^{-1/N} right)} - frac{k-1}{N} &amp;= log{ chi_0} + log{ left(1 - e^{-1/N} right)} - frac{k-2}{N} - frac{1}{N} &amp; equiv log{w_{k-1}} - frac{1}{N} end{align*} which provides a convenient expression to update the weights on-the fly. . Likewise, the entropy $ mathcal{H}_N$ can also be updated as follows begin{align*} Z_N &amp;= sum_{k=1}^N A_k H_N &amp;= sum_{k=1}^N frac{A_k}{Z_N} log{ frac{L_k}{Z_N}} &amp;= frac{A_N}{Z_N} log{ frac{L_N}{Z_N}} + sum_{k=1}^{N-1} frac{A_k}{Z_N} log{ frac{L_k}{Z_N}} &amp;= frac{A_N}{Z_N} log{ frac{L_N}{Z_N}} + frac{Z_{N-1}}{Z_N} sum_{k=1}^{N-1} frac{A_k}{Z_{N-1}} log{ left( frac{L_k}{Z_{N-1}} frac{Z_{N-1}}{Z_N} right)} &amp;= e^{ log{A_N} - log{Z_N}} log{ frac{L_N}{Z_N}} + e^{ log{Z_{N-1}}- log{Z_N}} left[ underbrace{ sum_k^{N-1} frac{A_k}{Z_{N-1}} log{ frac{L_k}{Z_{N-1}}}}_{H_{N-1}} + frac{1}{Z_{N-1}} log{ frac{Z_{N-1}}{Z_N}} underbrace{ sum_k^{N-1}A_k}_{Z_{N-1}} right] &amp;= e^{ log{A_N} - log{Z_N}} log{ frac{L_N}{Z_N}} + e^{ log{Z_{N-1}}- log{Z_N}} left[H_{N-1} + log{ frac{Z_{N-1}}{Z_N}} right] &amp;= e^{ log{A_N}- log{Z_N}} log{L_N} + e^{ log{Z_{N-1}}- log{Z_N}} left[H_{N-1} + log{Z_{N-1}} right] - log{Z_N} left[ underbrace{ frac{A_N + Z_{N-1}}{Z_N}}_{ frac{A_N + sum_k^{N-1}A_k}{A_N} = frac{Z_N}{Z_N} = 1} right] &amp;= e^{ log{A_N}- log{Z_N}} log{L_N} + e^{ log{Z_{N-1}}- log{Z_N}} left[H_{N-1} + log{Z_{N-1}} right] - log{Z_N} end{align*} . Sidenote . We have chosen to work with the logarithm of the variables, instead of the variables themselves, to avoid any overflow/underflow issues, which can arise when computing probabilities. This is particuarly important when considering partition functions, since they are exponential in the system size. . For example, to compute the the sum $ sum_i x_i$, we would instead compute begin{align*} log{ sum_i x_i} &amp;= log{ left[x_{ text{max}} left(1 + sum_i^ prime x_i/x_{ text{max}} right) right]} &amp;= log{x_{ text{max}}} + log{ left[1 + sum_i^ prime x_i/x_{ text{max}} right]} &amp;= log{x_{ text{max}}} + log{ left[1 + sum_i^ prime exp{ left( log{x_i} - log{x_{ text{max}}} right)} right]} &amp;= log{x_{ text{max}}} + log{ left[ sum_i exp{ left( log{x_i} - log{x_{ text{max}}} right)} right]} end{align*} where the prime on the summation indicates that the term corresponding to $x_{ text{max}}$ is excluded. Note that we all quantities are given in terms of their logarithms. . def logAdd(logx,logy): &quot;&quot;&quot;Function to add values in logarithm log(x + y) = logx + log(1 + exp(log(y) - log(x))), for x &gt; y &quot;&quot;&quot; if logx &gt; logy: return logx + np.log(1.0 + np.exp(logy-logx)) else: return logy + np.log(1.0 + np.exp(logx-logy)) def logSum(logx): &quot;&quot;&quot;Function to perform logarithmus summation of array of values (also given in log)&quot;&quot;&quot; logxmax = np.max(logx) return logxmax + np.log(np.sum(np.exp(logx - logxmax))) def logWeights(logx): &quot;&quot;&quot; Return log weights given nested sampling positions logχ_i log w_k = log(χ_{k-1} - χ_k) = log[χ_{k-1} (1 - χ_k/χ_{k-1})] = logχ_{k-1} + log[1 - χ_k / χ_{k-1}] &quot;&quot;&quot; logx_prev = np.roll(logx, 1, axis=-1) # χ_{k-1} logx_prev[0] = 0.0 # log(χ_0) = log(1) = 0 return logx_prev + np.log(1 - np.exp(logx - logx_prev)) def logZtrap(logx, logl): &quot;&quot;&quot; Return logZ given nested sampling positions and likelihoods log Z = log(Σ A_i) &quot;&quot;&quot; logA = logWeights(logx) + logl logA = np.append(logA, logx[-1] + logl[-1]) # approximate cleanup term for remaining area, assuming constant likelihood for (0, x_N) return logSum(logA) def posterior(ndraws, sample, logA, logZ): &quot;&quot;&quot;Draw representative samples from the posterior using the staircase sampling method (see Sivia &amp; Skilling, 9.4.1)&quot;&quot;&quot; weights = np.exp(logA - logZ) ν = np.minimum(np.int(np.floor(1.0 / np.max(weights))), ndraws) staircase = ν*np.cumsum(weights) + np.random.rand(len(sample)) ids = np.searchsorted(np.floor(staircase), np.arange(1,ν+1,step=1)) print(f&#39;Effective draws : {ν}&#39;) return sample[ids] . def fmap(func, *iterables): &quot;&quot;&quot; Map func over the list of iterables map(f, (a1, a2, ...), (b1, b2, ...), (c1,c2, ...), ...) = ( f(a1,b1,c2, ...), f(a2,b2,c2, ...), ..., f(ai,bi,ci), ...) &quot;&quot;&quot; return np.array(list(map(func, *iterables))) def nestedSampler(*, n, maxIter, newObj, prior, logLikelihood, sampler, params): &quot;&quot;&quot; Nested Sampling procedure :Adapted from Sivia and Skilling, Data Analysis : A Bayesian Tutorial, pg. 188 Args: n : number of live points maxIter : maximum number of iterations (i.e., number of integration points) newObj : function to create objects/state points Θ prior : prior probability function Π(θ) logLikelihood : log-likelihood function log(L(θ)) sampler : routine to sample within constrained likelihood, given some valid initial point params : parameters for sampling routine &quot;&quot;&quot; obj = newObj(n) # initialize memory for the n live points θ logL = np.zeros(n) # log(L) for live points sample = newObj(maxIter) # initialize memory for dead points θ* logLsample = np.zeros(maxIter) # log(L) for dead points obj[...] = prior(n) # sample objects from prior Π(θ) logL[...] = fmap(logLikelihood, obj) # compute likelihood of all points for nest in trange(maxIter): worst = np.argmin(logL) # find worst object in collection sample[nest] = obj[worst] # add worst point to list of dead points (posterior samples) logLsample[nest] = logL[worst] copy = np.random.randint(low=0, high=n) # kill worst object -&gt; replace with copy of different survivor (if there is more than one point) while copy == worst and n &gt; 1: copy = np.random.randint(low=0, high=n) logLstar = logL[worst] # new likelihood constraint obj[worst] = sampler(x0=obj[copy], logLikelihood=logLikelihood, logLstar=logLstar, params=params) logL[worst] = logLikelihood(obj[worst]) return obj, sample, logLsample def nestedAvgLogZ(*, n, sample, logL): &quot;&quot;&quot;Process the results of a nested sampling run to evaluate the evidence integral, assuming average statistics for the χ_i n : number of live points used in the sampling sample : list of configuration points θ resulting from the NS, i.e., the dead points logL : list of log-likelihoods for the dead points &quot;&quot;&quot; K = len(sample) # number of dead points used for integration logX = -np.arange(1,K+1,1)/n # &lt;log x_i&gt; = -i/n logW = np.log(1.0 - np.exp(-1.0/n)) - np.arange(0, K, 1)/n # log(w_i) = log(x_{i-1} - x_i), with x_0 = 1 logA = np.append(logW + logL, logX[-1] + logL[-1]) # cleanup term, assuming constant likelihood within (x=0,x_K) logZ = logSum(logA) H = np.sum(np.exp(logA - logZ)*np.append(logL, logL[-1])) - logZ print(f&#39;&lt;logZ&gt; = {logZ:12.6g} +/- {np.sqrt(H/n):8.6g}&#39;) print(f&#39;&lt; Z &gt; = {np.exp(logZ):12.6g}&#39;) print(f&#39; H = {H:12.6g} nats = {H/np.log(2.0):12.6g} bits&#39;) return logX,logA[:-1],logZ,H def nestedSampleLogZ(*, n, sample, logL, ndraws=1): &quot;&quot;&quot;Process the results of a nested sampling run to evaluate the evidence integral, randomly samply the χ_i n : number of live poitns used in the sampling sample : list of configuration points θ resulting form the NS, i.e., the dead points logL : list of log-likelihoods for the dead points &quot;&quot;&quot; nsamples = len(logL) logt = stickbreaking(n, (ndraws, nsamples)) # generate &#39;ndraws&#39; individual trajectories, sampling all x_i for each trajectory logX = np.cumsum(np.log(logt), axis=1)[::-1] logZ = fmap(logZtrap, logX, np.broadcast_to(logL, logX.shape)) print(f&#39;Ensemble logZ = {np.average(logZ):12.6g} +/- {np.std(logZ):8.6g}&#39;) print(f&#39;Ensemble Z = {np.average(np.exp(logZ)):12.6g} +/- {np.std(np.exp(logZ)):8.6g}&#39;) return logX, logZ . Example : $d$-dimensional Gaussian within spherically uniform prior . Assume we have a $d$-dimensional configuration space, $ Theta=( theta_1, ldots, theta_d)$, with a uniform flat prior with the unit $d$-sphere, zero outside . begin{align} Pi( Theta) &amp;= begin{cases} frac{1}{V_d(R=1)} &amp; sum_i theta_i^2 =r^2 le 1 0 &amp; text{otherwise} end{cases} end{align}where $V_d$ is the volume of the $d$-sphere begin{align} V_d(R) &amp;= frac{ pi^{d/2}}{ Gamma_{ frac{n}{2} + 1}} R^d = frac{ pi^{d/2}}{ left( frac{n}{2} right)!} R^d end{align} . Let the likelihood be Gaussian begin{align} mathcal{L}( Theta) &amp;= exp{ left[- frac{1}{2} Theta^t Sigma^{-1} Theta right]} end{align} . Assuming that the prior is constant in the region where the likelihood is non-zero, i.e., the contribution from the tails outside the unit sphere is negligible, the evidence integral can be approximated as begin{align*} Z &amp;= int mathcal{L}( Theta) Pi( Theta) text{d} Theta &amp; simeq frac{1}{V_d(1)} int mathcal{L}( Theta) text{d} Theta &amp;= frac{ Gamma_{ frac{n}{2} + 1}}{ pi^{d/2}} sqrt{ det{2 pi Sigma}} &amp;= Gamma_{ frac{n}{2} + 1} frac{(2 pi)^{d/2}}{ pi^{d/2}} sqrt{ det{ Sigma}} &amp;= Gamma_{ frac{n}{2} + 1} 2^{d/2} sqrt{ det{ Sigma}} log{Z} &amp;= frac{1}{2} log{ det{ Sigma}} + frac{d}{2} log{2} + log{ Gamma_{ frac{n}{2} + 1}} end{align*} . For the case of a spherically-symmetric Gaussian, $ Sigma = sigma^2 mathsf{I}$, and begin{align} mathcal{L}( Theta) &amp;= exp{ left[- frac{r^2}{2 sigma^2} right]} log{Z} &amp;= frac{d}{2} log{ left(2 sigma^2 right)} + log{ Gamma_{ frac{n}{2} + 1}} end{align} . In this case we can easily compute all the intermediate quantities analytically. Consider the integrated prior mass $ chi( lambda)$, we can divide the domain into spherical shells of decreasing radius (increasing likelihood). Let $ lambda$ be the likelihood value associated to a given shell (at radius $r$) begin{align} mathcal{L}( Theta)= lambda &amp;= exp{ left[- frac{r^2}{2 sigma^2} right]} r^2 &amp;= -2 sigma^2 log{ lambda} end{align} . The prior mass contained within this shell is then begin{align} chi( lambda) &amp;= int_{ Theta : mathcal{L}( Theta) &gt; lambda} Pi( Theta) text{d} Theta &amp;= frac{1}{V_d(R=1)} underbrace{ int_{r^2 le -2 sigma^2 log{ lambda}} text{d} Theta}_{ textrm{Volume of d-sphere with radius} R^2=-2 sigma^2 log{ lambda}} &amp;= frac{V_d(R= sqrt{-2 sigma^2 log{ lambda}})}{V_d(R=1)} &amp;= left(-2 sigma^2 log{ lambda} right)^{d/2} end{align} . Allowing us to explicitly invert this relationship begin{align} mathscr{L}(X( lambda)) &amp;= lambda mathscr{L}(X) &amp;= exp{ left[- frac{X^{2/d}}{2 sigma^2} right]} end{align} . Let&#39;s define the necessary auxiliary functions for this problem, and then use them to integrate a $10$-dimensional gaussian with $ sigma=0.01$. . def unitSphereGaussian(*, dim, σ): def _prior(num): &quot;&quot;&quot;Return num points uniformly sampled in unit d-ball&quot;&quot;&quot; def _marsaglia(): &quot;&quot;&quot;Return num points sampled over surface of unit d-ball&quot;&quot;&quot; r = np.random.randn(num, dim) return r / np.linalg.norm(r, axis=1)[..., None] x = _marsaglia() u = np.random.rand(num)**(1/dim) x[...] = u[:,None]*x[...] return x def _loglike(θ): &quot;&quot;&quot;Gaussian log-likelihood Π(θ)&quot;&quot;&quot; return -np.sum(θ**2) / (2*σ**2) def _loglikeX(X): &quot;&quot;&quot;Return loglikelihood as a function of prior mass , log(L(χ)) = log(λ)&quot;&quot;&quot; return -X**(2/dim)/(2*σ**2) def _logX(logλ): &quot;&quot;&quot;Return log of prior mass as a function of likelihood, log(χ(λ))&quot;&quot;&quot; return dim/2*(np.log(2*σ**2) + np.log(-logλ)) def _sampler(*, x0, logLikelihood, logLstar, params): &quot;&quot;&quot;Constrained sampler, returns points sampled uniformly in χ within the likelihood contour&quot;&quot;&quot; return np.sqrt(-2*σ**2*logLstar)*_prior(1)[0] return _prior, _loglike, _sampler, _loglikeX, _logX dim,σ = 10, 0.01 print(f&#39;log(Z_theory) = {dim/2*np.log(2*σ**2) + np.log(factorial(dim//2)):12.6g}&#39;) print(f&#39;Z_theory = {factorial(dim//2) * (2*σ**2)**(dim//2):12.6g}&#39;) . log(Z_theory) = -37.7985 Z_theory = 3.84e-17 . npoints = 100 nsamples = npoints*100 prior, loglike, sampler,loglikeX, logX = unitSphereGaussian(dim=dim, σ=σ) live, sample, logLsample = nestedSampler(n = npoints, maxIter = nsamples, newObj = lambda n : np.zeros((n,dim)), prior = prior, logLikelihood = loglike, sampler = sampler, params = None) logXsample = logX(logLsample) logXavg,logAavg,logZavg,Havg = nestedAvgLogZ(n=npoints, sample=sample, logL = logLsample) . 100%|██████████| 10000/10000 [00:00&lt;00:00, 28282.07it/s] . &lt;logZ&gt; = -38.3371 +/- 0.577448 &lt; Z &gt; = 2.24091e-17 H = 33.3446 nats = 48.1061 bits . . The nested sampling, using average locations for the &#39;dead&#39; points, gives excellent agreement with the exact solution. To see how good this approximation is, let&#39;s consider actually sampling the locations (using the stick breaking distribution). For this, we will generate 50 different trajectories, each consisting of $K$ number of ($ log{x_k}$, $ log{L_k}$) points, where the values of $ log{L_k}$ are the same for all trajectories, the only difference will be the $x_k$ values (which are now randomly sampled, instead of assuming $ log(x_k) = -k/N$). . logXguess,logZguess = nestedSampleLogZ(n=npoints, sample=sample, logL = logLsample, ndraws=50) # generate 50 individual trajectories, sampling all x_i for each trajectory . Ensemble logZ = -38.3386 +/- 0.549597 Ensemble Z = 2.60594e-17 +/- 1.5177e-17 . fig, [ax,bx] = plt.subplots(figsize=(24,9), ncols=2, sharex=True) ax.plot(logXsample, logXavg, color=color[&#39;dark&#39;]) ax.set_xlabel(&#39;$ log{ chi_{ mathrm{real}}}$&#39;) ax.set_ylabel(&#39;$ log{ chi_{ mathrm{avg}}}$&#39;) for lxi in logXguess: bx.plot(lxi, np.exp(logLsample), color=color[&#39;superfine&#39;], alpha=0.1) bx.plot(logXsample, np.exp(logLsample), color=color[&#39;mid&#39;], label=&#39;NS (average)&#39;) logχ = np.linspace(-100, 0, num=25) bx.plot(logχ, np.exp(loglikeX(np.exp(logχ))), color=color[&#39;dark&#39;], marker=&#39;o&#39;, mfc=&#39;None&#39;, ls=&#39;None&#39;, mew=&#39;2&#39;, label=&#39;Exact&#39;) bx.legend() bx.set_xlabel(&#39;$ log{χ}$&#39;) bx.set_ylabel(&#39;$L( chi)$&#39;) plt.show() . The figure on the right shows the average $ chi$ values versus the real values. The figure on the left shows the likelihood curve $ log{ mathcal{L}( chi)}$ curve as a function of $ log{ chi}$. The open symbols show the exact solution, the solid pink line shows the curve using the average values of $ chi$, whereas the light dark curves show the individually sampled curves. . fig, ax = plt.subplots(figsize=(12,9)) draws = posterior(500, sample, logAavg, logZavg) ax.hist(draws.flatten(), density=True, bins=30, color=color[&#39;superfine&#39;]) # use all d-dimensions ax.hist(draws[:,0], density=True, bins=30, alpha=0.3, color=color[&#39;mid&#39;]) # use only first dimension θrange = np.linspace(-3*σ, 3*σ) ax.plot(θrange, np.exp(fmap(loglike, θrange))/np.sqrt(2*np.pi*σ**2), color=color[&#39;dark_highlight&#39;]) ax.set_xlim(-3*σ, 3*σ) ax.set_xlabel(r&#39;$ Theta$&#39;) ax.set_ylabel(r&#39;$p( Theta| D, I)$&#39;) plt.show() . Effective draws : 500 . As mentioned above, we can use the sampled points (obtained from the NS procedure) to generate typical samples from the posterior. . Example : $d$-dimensional Gaussian within rectangular domain . Let&#39;s consider the same problem as above, but now using an MCMC sampler to generate the new points. This is the more general solution, as we will usually not be able to sample directly from $ chi$. For this, let us slightly change the problem specification, and now assume a uniform prior over a hyper-cube, with sides $(-1,1)$, instead of a hyper-sphere. . begin{align} Pi( Theta) &amp;= begin{cases} frac{1}{2^d} &amp; -1 le theta_i le 1 0 &amp; text{otherwise} end{cases} end{align} begin{align*} Z &amp;= int text{d} Theta mathcal{L}( Theta) Pi( Theta) &amp;= Pi( Theta) int text{d} Theta exp{ left[- frac{1}{2} Theta^t Sigma^{-1} Theta right]} , qquad Sigma = sigma^2 mathsf{I} &amp;= Pi( Theta) det{ left(2 pi Sigma right)}^{1/2} &amp;= 2^{-d} (2 pi)^{d/2} det{ Sigma}^{1/2} &amp;= left( frac{ pi}{2} right)^{d/2} sigma^{d} log{Z} &amp;= frac{d}{2} log{ frac{ pi}{2}} + d log{ sigma} end{align*} In this case, while we can still work out the evidence integral analytically, computing the integrated mass $ chi( lambda)$ is not as easy, since the spherical symmetry of the likelihood is not matched by the cubic symmetry of the prior (i.e., we need to compute the intersection between the hyper-sphere and the hyper-cube). For this example, it will be easier to setup a MC sampler to generate new points within the likelihood constraint. . def unitCubeGaussian(*, dim, σ): def _prior(num): &quot;&quot;&quot;Return num points uniformly sampled in d-cube [-1,1]&quot;&quot;&quot; return 2*np.random.rand(num, dim) - 1 def _loglike(θ): &quot;&quot;&quot;Gaussian log-likelihood Π(θ)&quot;&quot;&quot; return -np.sum(θ**2) / (2*σ**2) return _prior, _loglike def MCMCsampler(*, x0, logLikelihood, logLstar, params): &quot;&quot;&quot;MCMC sampler which returns points according to Π(θ), i.e., uniformly in d-cube [-1,1], within the likelihood contour L*&quot;&quot;&quot; dim = len(x0) lo,hi,δ,steps = params[&#39;a&#39;], params[&#39;b&#39;], params[&#39;δ&#39;], params[&#39;steps&#39;] def tounitcube(z): &quot;&quot;&quot;Map from (lo,hi) to (0,1) for simplicity&quot;&quot;&quot; return (z - lo) / (hi - lo) def fromunitcube(z): &quot;&quot;&quot;Map from (0,1) to (lo,hi)&quot;&quot;&quot; return (hi-lo)*z + lo def random_step(): &quot;&quot;&quot;Uniform random number between -1 and 1&quot;&quot;&quot; return 2*np.random.rand(dim) - 1 # x (u) : start points # y (v) : trial points x = x0.copy() u = tounitcube(x) v = np.zeros_like(x) y = np.zeros_like(x) accept, reject = 0,0 for i in range(steps): v[:] = u[:] + δ*random_step() # new trial point v[:]-= np.floor(v) # assume periodic boundary conditions to make sure all elements are within [-1,1] y[:] = fromunitcube(v) # remap to original domain logL = logLikelihood(y) if(logL &gt; logLstar): u[:] = v[:] x[:] = y[:] accept += 1 else: reject += 1 if accept &gt; reject: # modify step size to get convergence rate of ~ 50% δ *= np.exp(1.0 / accept) if accept &lt; reject: δ /= np.exp(1.0 / reject) δ = np.min([δ, (hi-lo)/2]) params[&#39;δ&#39;] = δ return x dim, σ = 10, 0.01 print(f&#39;log(Z_theory) = {dim/2*np.log(np.pi/2) + dim*np.log(σ):12.6g}&#39;) print(f&#39;Z_theory = {(np.pi/2)**(dim/2) * σ**dim:12.6g}&#39;) . log(Z_theory) = -43.7938 Z_theory = 9.56312e-20 . npoints = 100 nsamples = npoints*100 params = {&#39;δ&#39;:0.2, &#39;steps&#39;:100, &#39;a&#39;:-1, &#39;b&#39;:1} prior, loglike = unitCubeGaussian(dim=dim, σ=σ) live, sample, logLsample = nestedSampler(n = npoints, maxIter = nsamples, newObj = lambda n : np.zeros((n,dim)), prior = prior, logLikelihood = loglike, sampler = MCMCsampler, params = params) logXavg,logAavg,logZavg,Havg = nestedAvgLogZ(n=npoints, sample=sample, logL = logLsample) . 100%|██████████| 10000/10000 [00:32&lt;00:00, 310.09it/s] . &lt;logZ&gt; = -44.3365 +/- 0.628301 &lt; Z &gt; = 5.55805e-20 H = 39.4762 nats = 56.9521 bits . . logXguess,logZguess = nestedSampleLogZ(n=npoints, sample=sample, logL=logLsample, ndraws=50) . Ensemble logZ = -44.3946 +/- 0.672253 Ensemble Z = 6.41601e-20 +/- 3.96863e-20 . fig, [ax,bx] = plt.subplots(figsize=(24,9), ncols=2, sharex=True) for lxi in logXguess: ax.plot(logXavg, lxi, color=color[&#39;superfine&#39;], alpha=0.5) ax.plot(logXavg, logXavg, color=color[&#39;mid&#39;]) ax.set_xlabel(&#39;$ log{ chi_{ mathrm{avg}}}$&#39;) ax.set_ylabel(&#39;$ log{ chi_{ mathrm{guess}}}$&#39;) for lxi in logXguess: bx.plot(lxi, np.exp(logLsample), color=color[&#39;superfine&#39;], alpha=0.1) bx.plot(logXavg, np.exp(logLsample), color=color[&#39;mid&#39;], label=&#39;NS (average)&#39;) bx.legend() bx.set_xlabel(&#39;$ log{χ}$&#39;) bx.set_ylabel(&#39;$L( chi)$&#39;) plt.show() . Now let&#39;s see how to use the general purpose dynesty package to perform the same calculations, as it allows for greater flexibility and can handle multi-modal likelihoods! . import dynesty as dyn from dynesty import plotting as dyplot . def priorTransform(u): &quot;&quot;&quot;Transforms samples `u` drawn from the default unit cube [0,1] to samples from our desired distribution, in this case uniform [-1., 1)&quot;&quot;&quot; return (2.0 * u - 1) # Vanilla nested sampler (same as we coded above) sampler = dyn.NestedSampler(loglike, priorTransform, dim, bound=&#39;single&#39;) sampler.run_nested(dlogz=0.01) res = sampler.results . 24019it [00:29, 826.84it/s, +500 | bound: 298 | nc: 1 | ncall: 574652 | eff(%): 4.267 | loglstar: -inf &lt; -0.285 &lt; inf | logz: -43.668 +/- 0.394 | dlogz: 0.000 &gt; 0.010] . fig, axes = dyplot.runplot(res, color=&#39;blue&#39;) fig.tight_layout() . # Dynamic nested sampler (fancier!!!) dsampler = dyn.DynamicNestedSampler(loglike, priorTransform, dim, bound=&#39;single&#39;) dsampler.run_nested(nlive_init=500, nlive_batch=500, maxiter=res.niter+res.nlive) # place all the weight on the evidence calculation (zero weight for posterior calculation) dres = dsampler.results . 24522it [00:29, 832.11it/s, batch: 0 | bound: 298 | nc: 1 | ncall: 574753 | eff(%): 4.267 | loglstar: -inf &lt; -0.334 &lt; inf | logz: -43.723 +/- 0.395 | dlogz: 0.000 &gt; 0.010] . fig, axes = dyplot.runplot(dres, color=&#39;blue&#39;) fig.tight_layout() . Example: Egg-Basket . Define the Egg-basket likelihood $ mathcal{L}( Theta)$, $ Theta=(x,y)$, as begin{align*} mathcal{L}(x,y) = exp{ left[ left(2 + cos{x} cos{y} right)^5 right]} end{align*} with a uniform prior $U[0,6 pi]$ for both x and y. Note that the system is periodic in both $x$ and $y$ in this case. . The Evidence integral is begin{align*} Z &amp;= int text{d} Theta mathcal{L}( Theta) Pi( Theta) = frac{1}{(6 pi)^2} int text{d} Theta mathcal{L}( Theta) log{Z}&amp;= log{ left[ int mathcal{L} text{d} Theta right]} - 2 log{6 pi} simeq 235.856 end{align*} . def eggbasket(): def _prior(num): &quot;&quot;&quot;Return num points uniformly sampled in 2-cube [0,6π]&quot;&quot;&quot; return 6*np.pi*np.random.rand(num, 2) def _loglike(θ): &quot;&quot;&quot;Eggbasket log-likelihood Π(θ)&quot;&quot;&quot; x,y = θ return (2 + np.cos(x)*np.cos(y))**5 return _prior, _loglike prior, loglike = eggbasket() a,b = 0.0, 6*np.pi X,Y = np.meshgrid(np.linspace(a,b,1000), np.linspace(a,b,1000), indexing=&#39;ij&#39;) Z = integrate.dblquad(lambda y, x : np.exp(loglike(np.array([x,y]))), a, b, lambda x:a, lambda x:b) print(f&#39;Z = {Z[0]:12.6g}&#39;) print(f&#39;log(Z) = {np.log(Z[0]) - 2*np.log(b-a):12.6g}&#39;) . Z = 9.58375e+104 log(Z) = 235.856 . fig, ax = plt.subplots(figsize=(12,12)) im = ax.pcolormesh(X, Y, loglike(np.stack([X,Y])), shading=&#39;gouraud&#39;) ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) colorbar(ax,im) plt.show() . Lets first try to solve this using our naive nested-sampling implementation, with the MCMC random walk sampler for introducing new live points. . dim = 2 npoints = 1000 nsamples= npoints*100 params = {&#39;δ&#39;:0.1, &#39;steps&#39;:100, &#39;a&#39;:a, &#39;b&#39;:b} live, sample, logLsample = nestedSampler(n = npoints, maxIter = nsamples, newObj = lambda n : np.zeros((n,dim)), prior = prior, logLikelihood = loglike, sampler = MCMCsampler, params = params) logXavg,logAavg,logZavg,Havg = nestedAvgLogZ(n=npoints, sample=sample, logL = logLsample) . 100%|██████████| 100000/100000 [04:55&lt;00:00, 338.08it/s] . &lt;logZ&gt; = 235.865 +/- 0.0781797 &lt; Z &gt; = 2.72199e+102 H = 6.11206 nats = 8.81784 bits . . logXguess,logZguess = nestedSampleLogZ(n=npoints, sample=sample, logL=logLsample, ndraws=50) . Ensemble logZ = 235.867 +/- 0.0793413 Ensemble Z = 2.73541e+102 +/- 2.17504e+101 . fig, ax = plt.subplots(figsize=(12,9)) for lxi in logXguess: ax.plot(-lxi, np.exp(logLsample), color=color[&#39;superfine&#39;], alpha=0.5) ax.plot(-logXavg, np.exp(logLsample), color=color[&#39;dark&#39;]) ax.set_xlabel(&#39;$- log{χ}$&#39;) ax.set_ylabel(&#39;$L( chi)$&#39;) plt.show() . draws = posterior(5000, sample, logAavg, logZavg) fig, ax = plt.subplots(figsize=(12,12)) im = ax.pcolormesh(X, Y, loglike(np.stack([X,Y])), shading=&#39;gouraud&#39;) ax.plot(draws[:,0], draws[:,1], ls=&#39;None&#39;, marker=&#39;x&#39;, mew=0.2, color=color[&#39;mid&#39;]) ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) colorbar(ax,im) plt.show() . Effective draws : 2733 . Here we see that although our estimate for the evidence is excellent, the samples we obtain from the estimated posterior provide only a rough picture. Since the likelihood is peaked so strongly, we have almost no points in the blue regions (we would need to rerun the calculation with a larger number of iterations). Fortunately, dynesty uses a more sophisticated/adaptive procedure, allowing one to focus on the evidence or the posterior calculations. Let&#39;s see how this would work. . def priorTransform(u): &quot;&quot;&quot;Transforms samples `u` drawn from the default unit cube [0,1] to samples from our desired distribution, in this case uniform [0, 6π)&quot;&quot;&quot; return (6*np.pi * u) dsampler = dyn.DynamicNestedSampler(loglike, priorTransform, ndim=dim, bound=&#39;multi&#39;, sample=&#39;unif&#39;) . # First lets focus on the evidence dsampler.run_nested(dlogz_init=0.01, nlive_init=500, nlive_batch=500, wt_kwargs={&#39;pfrac&#39;: 0.0}, stop_kwargs={&#39;pfrac&#39;: 0.0}) dres_z = dsampler.results . 14048it [00:45, 309.90it/s, batch: 2 | bound: 83 | nc: 1 | ncall: 114102 | eff(%): 12.312 | loglstar: -inf &lt; 242.997 &lt; 241.693 | logz: 235.978 +/- 0.110 | stop: 0.808] . # Now lets add samples to obtain reliable estimates for the posterior, note the pfrac = 1.0 argument dsampler.reset() dsampler.run_nested(dlogz_init=0.01, nlive_init=500, nlive_batch=500,wt_kwargs={&#39;pfrac&#39;: 1.0}, stop_kwargs={&#39;pfrac&#39;: 1.0}) dres_p = dsampler.results . 22604it [02:53, 130.53it/s, batch: 12 | bound: 173 | nc: 1 | ncall: 172303 | eff(%): 13.119 | loglstar: 237.870 &lt; 242.999 &lt; 242.627 | logz: 235.811 +/- 0.157 | stop: 0.905] . # Let&#39;s plot the evidence fig, axes = dyplot.runplot(dres_z, color=&#39;blue&#39;) fig.tight_layout() . # Here we plot the marginalized posteriors fig, axes = dyplot.cornerplot(dres_p, quantiles=None, color=color[&#39;dark&#39;],span=[[0, 6*np.pi], [0, 6*np.pi]],fig=plt.subplots(2, 2, figsize=(10, 10))) fig.tight_layout() . # Here we have the estimate of the posterior obtained from the evidence calculations fig, ax = plt.subplots(figsize=(12,12)) im = ax.pcolormesh(X,Y,loglike(np.stack([X,Y])), shading=&#39;gouraud&#39;) colorbar(ax, im) ax.plot(dres_z.samples[:,0], dres_z.samples[:,1], ls=&#39;None&#39;, marker=&#39;x&#39;, mew=0.2, color=color[&#39;mid&#39;]) ax.set_xlabel(r&#39;$x$&#39;, fontsize=22) ax.set_ylabel(r&#39;$y$&#39;, fontsize=22) plt.show() . These results look much better, however, notice that we have more points in the unlikely dark blue regions than is warranted. This is because we sample points according to the (constrained) prior, which is uniform over all of space. Sampling these regions is necessary to provide an accurate estimate of the evidence, but it overestimates their importance for the posterior calculations. A more accurate picture is given by the results of the &#39;posterior&#39; run, shown below. . # Here we have the estimate of the posterior obtained from the posterior calculations fig, ax = plt.subplots(figsize=(12,12)) im = ax.pcolormesh(X,Y,loglike(np.stack([X,Y])), shading=&#39;gouraud&#39;) colorbar(ax, im) ax.plot(dres_p.samples[:,0], dres_p.samples[:,1], ls=&#39;None&#39;, marker=&#39;x&#39;, mew=0.2, color=color[&#39;mid&#39;]) ax.set_xlabel(r&#39;$x$&#39;, fontsize=22) ax.set_ylabel(r&#39;$y$&#39;, fontsize=22) plt.show() . They examples page on the dynesty website contains many more illustrative examples, which showcase the power of nested sampling. The take home message here should be clear, don&#39;t try to implement your own nested sampler if you can avoid it! Use available well tested packages (dynesty, polychord, etc.). . #collapse from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . . absl-py @ file:///Users/runner/miniforge3/conda-bld/absl-py_1602289414926/work appnope @ file:///Users/runner/miniforge3/conda-bld/appnope_1602242458663/work argon2-cffi @ file:///Users/runner/miniforge3/conda-bld/argon2-cffi_1602546574518/work arviz @ file:///home/conda/feedstock_root/build_artifacts/arviz_1600909298598/work async-generator==1.10 attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1599308529326/work backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work backports.functools-lru-cache==1.6.1 bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1600454382015/work brotlipy==0.7.0 certifi==2020.6.20 cffi @ file:///Users/runner/miniforge3/conda-bld/cffi_1602537245790/work cftime @ file:///Users/runner/miniforge3/conda-bld/cftime_1602504459860/work chardet @ file:///Users/runner/miniforge3/conda-bld/chardet_1602255309528/work cryptography @ file:///Users/runner/miniforge3/conda-bld/cryptography_1604179103599/work cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 dynesty==1.0.1 entrypoints @ file:///Users/runner/miniforge3/conda-bld/entrypoints_1602701742773/work/dist/entrypoints-0.3-py2.py3-none-any.whl fastprogress @ file:///home/conda/feedstock_root/build_artifacts/fastprogress_1597932925331/work h5py @ file:///Users/runner/miniforge3/conda-bld/h5py_1602551952801/work idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1593328102638/work importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1602263269022/work ipykernel @ file:///Users/runner/miniforge3/conda-bld/ipykernel_1602682896422/work/dist/ipykernel-5.3.4-py3-none-any.whl ipython @ file:///Users/runner/miniforge3/conda-bld/ipython_1604159586038/work ipython-genutils==0.2.0 ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1599554010055/work jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1603763676610/work jaxlib @ file:///Users/runner/miniforge3/conda-bld/jaxlib_1602648340228/work/jaxlib-0.1.56-cp38-none-macosx_10_9_x86_64.whl jedi @ file:///Users/runner/miniforge3/conda-bld/jedi_1602395236037/work Jinja2==2.11.2 json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1600692310011/work jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1602551949684/work jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1598486169312/work jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1598728807792/work jupyter-contrib-core==0.3.3 jupyter-contrib-nbextensions @ file:///Users/runner/miniforge3/conda-bld/jupyter_contrib_nbextensions_1602805472999/work jupyter-core @ file:///Users/runner/miniforge3/conda-bld/jupyter_core_1602537297490/work jupyter-highlight-selected-word @ file:///Users/runner/miniforge3/conda-bld/jupyter_highlight_selected_word_1603234251450/work jupyter-latex-envs @ file:///Users/runner/miniforge3/conda-bld/jupyter_latex_envs_1602788808989/work jupyter-nbextensions-configurator @ file:///Users/runner/miniforge3/conda-bld/jupyter_nbextensions_configurator_1602769531809/work jupyterlab==2.2.9 jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1601375948261/work jupyterlab-server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server_1593951277307/work kiwisolver @ file:///Users/runner/miniforge3/conda-bld/kiwisolver_1604323069108/work lxml @ file:///Users/runner/miniforge3/conda-bld/lxml_1603104859015/work Mako @ file:///home/conda/feedstock_root/build_artifacts/mako_1595925083607/work MarkupSafe @ file:///Users/runner/miniforge3/conda-bld/markupsafe_1602267327822/work matplotlib @ file:///Users/runner/miniforge3/conda-bld/matplotlib-suite_1602600871595/work mistune @ file:///Users/runner/miniforge3/conda-bld/mistune_1602381756020/work nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1602859080374/work nbconvert==5.6.1 nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1602732862338/work nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1604290829246/work netCDF4 @ file:///Users/runner/miniforge3/conda-bld/netcdf4_1602508558073/work notebook @ file:///Users/runner/miniforge3/conda-bld/notebook_1602720147128/work numpy @ file:///Users/runner/miniforge3/conda-bld/numpy_1603047582194/work olefile @ file:///home/conda/feedstock_root/build_artifacts/olefile_1602866521163/work opt-einsum==0+untagged.61.gd905544.dirty packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1589925210001/work pandas==1.1.4 pandocfilters==1.4.2 parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1595548966091/work patsy==0.5.1 pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work Pillow @ file:///Users/runner/miniforge3/conda-bld/pillow_1603406624790/work pip==20.2.4 prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1590412252446/work prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1602524994744/work protobuf==3.13.0 ptyprocess==0.6.0 pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1593275161868/work Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1603558917696/work pygpu==0.7.6 pymc3 @ file:///home/conda/feedstock_root/build_artifacts/pymc3_1602091536109/work pyOpenSSL==19.1.0 pyparsing==2.4.7 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtChart==5.12 PyQtWebEngine==5.12.1 pyrsistent @ file:///Users/runner/miniforge3/conda-bld/pyrsistent_1602259983752/work PySocks @ file:///Users/runner/miniforge3/conda-bld/pysocks_1602326916036/work python-dateutil==2.8.1 pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1604321279890/work PyYAML==5.3.1 pyzmq==19.0.2 qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole_1599147533948/work QtPy==1.9.0 requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1592425495151/work scipy @ file:///Users/runner/miniforge3/conda-bld/scipy_1604305021986/work seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-base_1604238753550/work Send2Trash==1.5.0 setuptools==49.6.0.post20201009 six @ file:///home/conda/feedstock_root/build_artifacts/six_1590081179328/work statsmodels @ file:///Users/runner/miniforge3/conda-bld/statsmodels_1604227112528/work terminado @ file:///Users/runner/miniforge3/conda-bld/terminado_1602679604944/work testpath==0.4.4 Theano @ file:///Users/runner/miniforge3/conda-bld/theano_1604228561644/work tornado @ file:///Users/runner/miniforge3/conda-bld/tornado_1604105041409/work tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1603658198734/work traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1602771532708/work typing-extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1602702424206/work urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1603125704209/work wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work webencodings==0.5.1 wheel==0.35.1 widgetsnbextension @ file:///Users/runner/miniforge3/conda-bld/widgetsnbextension_1603129445321/work xarray @ file:///home/conda/feedstock_root/build_artifacts/xarray_1600638299066/work zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1603668650351/work .",
            "url": "https://johnjmolina.github.io/MLKyoto/nested%20sampling/2020/11/06/Nested-Sampling.html",
            "relUrl": "/nested%20sampling/2020/11/06/Nested-Sampling.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Gaussian Processes 2 - Application",
            "content": "We reproduce a recent paper by Raissi and Karniadakis (Raissi &amp; Karniadakis, 2018), in which Gaussian Processes are used to learn partial differential equations from &quot;small&quot; data. In particular, it is assumed the the general form of the underlying PDE is known, while the model parameters are unspecified. The proposed method is then used to infer the parameters of the one-dimensional Burgers&#39;, Korteweg-de Vries, Kuramoto-Sivishinky, and non-linear Schrödinger equations. The datasets are taken from earlier work of Rudy et al. (Rudy et al., 2017), who have proposed a sparse-regression technique to discover the symbolic form of the governing pde. The full dataset is available on the PDE-FIND github page (in scipy.io format). . Hidden physics models: Machine learning of nonlinear partial differential equations M. Raissi and G. E. Karniadakis, Journal of Computational Physics 357, 125 (2018) . | Data-driven discovery of partial differential equations S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz, Science Advances 3, e1602614 (2017) . | . Preliminaries . #collapse import jax import jax.numpy as np import pandas as pd import numpy as onp import h5py as h5 import scipy.io as sio import importlib import utils.pdesolver.phi as phi import utils.pdesolver.jax_etdrk as etdrk import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.patheffects as PathEffects from scipy import optimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.numpy.lax_numpy import _wraps from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from functools import partial,reduce from numpy import random from tqdm import tqdm mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) . . The following function will load the solutions obtained by Rudy et al., as well as extract the training data (and save it to a separate file for easy recovery). . #collapse-show # Parameters for all the 1D problems considered in the paper equations = {&#39;burgers&#39;:{&#39;label&#39;:&#39;Burgers&#39;, &#39;ts&#39;:[81,82], &#39;num&#39;:[71,69]}, &#39;kdv&#39; :{&#39;label&#39;:&#39;Korteweg-de Vries&#39;, &#39;ts&#39;:[162,163], &#39;num&#39;:[111,109]}, &#39;kuramoto_sivashinsky&#39;:{&#39;label&#39;:&#39;Kuramoto-Sivashinsky&#39;, &#39;ts&#39;:[203,204], &#39;num&#39;:[301,299]}, &#39;nls&#39; :{&#39;label&#39;:&#39;non-linear Schrödinger&#39;, &#39;ts&#39;:[407,408], &#39;num&#39;:[49,51]}, &#39;harmonic_osc&#39;:{&#39;label&#39;:&#39;Harmonic Oscillator&#39;, &#39;ts&#39;:[325, 326], &#39;num&#39;:[55, 53]}} names = list(equations) def h5update(gp, *, name, data): gp.require_dataset(name = name, shape = data.shape, dtype=data.dtype) gp[name][...] = data def parseData(): &quot;&quot;&quot;Function to convert Rudy et al.&#39;s dataset into hdf5 file (save format was not the same for all files...) Parse data for equations (kdv, burgers, nls, harmonic_osc, kuramoto_sivishinky) &quot;&quot;&quot; with h5.File(&#39;../_simulation_data/datasets.h5&#39;, &#39;a&#39;) as fp: for name in equations.keys(): data = sio.loadmat(f&#39;../_simulation_data/PDE-FIND/Datasets/{name}.mat&#39;, mat_dtype=False) gp = fp.require_group(name) if name in [&#39;kdv&#39;, &#39;burgers&#39;]: u = np.real(data[&#39;usol&#39;]).transpose() x = data[&#39;x&#39;][0] t = data[&#39;t&#39;][:,0] if name in [&#39;kuramoto_sivashinsky&#39;]: u = data[&#39;uu&#39;].transpose() x = data[&#39;x&#39;][:,0] t = data[&#39;tt&#39;][0] if name in [&#39;nls&#39;, &#39;harmonic_osc&#39;]: u = data[&#39;usol&#39;] x = data[&#39;x&#39;][0] t = data[&#39;t&#39;][:,0] h5update(gp, name = &#39;u&#39;, data = u) h5update(gp, name = &#39;x&#39;, data = x) h5update(gp, name = &#39;t&#39;, data = t) parseData() . . /opt/anaconda3/envs/ML/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . Load data from hdf5 file . #collapse-show dataset_file = &#39;../_simulation_data/datasets.h5&#39; def loadData(fname, name, frame=-1): with h5.File(fname, &#39;r&#39;) as fp: if frame &lt; 0: u = np.array(fp[name][&#39;u&#39;][...]) # default is to load everything else: u = np.array(fp[name][&#39;u&#39;][frame,...]) x = np.array(fp[name][&#39;x&#39;][...]) t = np.array(fp[name][&#39;t&#39;][...]) return {&#39;t&#39;:t, &#39;x&#39;:x, &#39;u&#39;:u} def updateParams(params, sol): t,x = sol[&#39;t&#39;], sol[&#39;x&#39;] params[&#39;dt&#39;] = t[1]-t[0] params[&#39;lbox&#39;] = (x[1]-x[0])*len(x) . . Hidden Physics Models . Consider the following partial differential equation . begin{align} partial_{t}u + mathcal{G}(u)= 0, label{e:pde} end{align}with $ mathcal{G}$ an arbitrary non-Linear operator. Without loss of generality, we can write $ mathcal{G}$ as begin{align} mathcal{G}(u) &amp;= mathcal{Q}^{(u)} u, notag end{align} where $ mathcal{Q}^{(u)}$, for fixed $u$, is a linear operator, i.e., begin{align} mathcal{Q}^{(u)} ( alpha v + beta w) &amp;= alpha mathcal{Q}^{(u)} v + beta mathcal{Q}^{(u)} w. notag end{align} Here, the $u$ appearing in $ mathcal{Q}^u$ should be thought of as an additional parameter or frozen variable. . Following Raissi and Karniadakis, let&#39;s discretize the time-derivative using a back-ward Euler step, such that begin{align} u^{n} + Delta t , mathcal{G}(u^n) = u^{n-1} label{e:backwards_euler} mathcal{N}(u^n) = u^{n-1} notag end{align} with $ mathcal{N} = mathcal{I} + Delta t , mathcal{G}$ a (non-linear) propagator taking the solution at $n$ to that at $n-1$, . begin{align} mathcal{N}(u^n) &amp;= u^n + Delta t mathcal{G}(u^n) label{e:propagator} &amp;= u^n + Delta t mathcal{Q}^{(u^n)} u^n end{align}Now, we approximate $ mathcal{Q}^{u^n} simeq mathcal{Q}^{u^{n-1}}$, in order to linearize $ mathcal{N}$ begin{align} mathcal{N}(u^n) &amp; simeq mathcal{L}^{(u^{n-1})} u^n = left( mathcal{I} + Delta t mathcal{Q}^{(u^{n-1})} right) u^n label{e:linear_propagator} end{align} . The linearized propagator equation is now begin{align} mathcal{L}^{(u^{n-1})} u^{n}&amp; simeq u^{n-1} label{e:linearized_pde} end{align} . The benefit of this linearization comes in when we place a GP prior on $u^{n}$ begin{align} u^{n}&amp; sim mathcal{GP}(0, K^{n,n}) label{e:gp} end{align} since this implies a GP prior for the joint distribution begin{align} begin{pmatrix}u^n u^{n-1} end{pmatrix} &amp; sim mathcal{GP} left(0, begin{bmatrix}K^{n,n}(X_n,X_n) &amp; K^{n,n-1}(X_n,X_{n-1}) K^{n-1,n}(X_{n-1},X_{n}) &amp; K^{n-1,n-1}(X_{n-1},X_{n-1}) end{bmatrix} right) label{gp_joint} end{align} . The physics of the problem, codified in the underlying differential equations, will determine the correlations between the solutions at time $n$ and $n-1$. . Let the solution $u^n$ ($u^{n-1}$) be specified at points $x_n$ ($x_{n-1}$). . The correlation matrices are defined as begin{align} K^{n,n}(x_n, x_n^ prime) &amp;= left langle u^n(x_n) , u^n(x_n^ prime) right rangle equiv K(x_n, x_n^ prime) label{e:knn} K^{n,n-1}(x_n, x_{n-1}^ prime)&amp;= left langle u^{n}(x_n) , u^{n-1}(x_{n-1}^ prime) right rangle label{e:knm} &amp;= left langle u^{n}(x_n) , left( mathcal{L}^{(u^{n-1})} u^n right) (x_{n-1}^ prime) right rangle notag &amp;= mathcal{L}_{2}^{(u^{n-1})} left langle u^n(x_n) , u^n(x_{n-1}^ prime) right rangle notag &amp;= mathcal{L}_{2}^{(u^{n-1})} K(x_{n}, x_{n-1}^ prime) = left( mathcal{L}_{2}^{(u^{n-1})} K right)(x_{n}, x_{n-1}^ prime) notag K^{n-1,n}(x_{n-1}, x_n^ prime) &amp;= mathcal{L}_{1}^{(u^{n-1})} K(x_{n-1}, x_{n}^ prime) = left( mathcal{L}_{1}^{(u^{n-1})} K right)(x_{n-1}, x_{n}^ prime) label{e:kmn} K^{n-1,n-1}(x_{n-1}, x_{n-1}^ prime) &amp;= mathcal{L}_{1}^{(u^{n-1})} mathcal{L}_{2}^{(u^{n-1})} K(x_{n-1}, x_{n-1}^ prime) = left( mathcal{L}_{1}^{(u^{n-1})} mathcal{L}_{2}^{(u^{n-1})} K right)(x_{n-1}, x_{n-1}^ prime) label{e:kmm} end{align} where the $1$ and $2$ sub-indices in the $ mathcal{L}$ operator refer to the argument of the $K$ function on which it is acting on. . Putting everything together, the joint distribution can be written solely in terms of the $K^{nn}$ correlation matrix, as . begin{align} begin{pmatrix}u^n u^{n-1} end{pmatrix} &amp; sim mathcal{GP} left(0, begin{bmatrix} K(X_n, X_n) &amp; mathcal{L}^{u^{n-1}}_{2} K (X_n, X_{n-1}) mathcal{L}_1^{u^{n-1}} K (X_{n-1}, X_{n}) &amp; mathcal{L}_1^{u^{n-1}} mathcal{L}_2^{u^{n-1}} K(X_{n-1}, X_{n-1}) end{bmatrix} right) label{e:gp_joint_final} end{align} Note that all we need to do is to specify the appropriate linearized operator $ mathcal{L}$ for the problem at hand. In what follows, let $ Delta t = h$. Furthermore, recall that $K(x_1,x_2)$, a function of two variables, is parametrized by hyper-parameters $ Theta = ( theta_1, cdots, theta_p)$. When we operate with $ mathcal{L}_{i}^{ tilde{u}}$ on $K$, we get another function of two variables, with additional parameters $i$ and $ tilde{u}$. We will use jax for all the computations and avoid messy calculations by hand (symbolic expressions for all necessary terms can be found here), but this requires that all functions ($K$, $ mathcal{L} K$, $ mathcal{L} mathcal{L}K$) have the same signature (number and types of arguments). Therefore, we extend all functions as follows . begin{align*} K(x_1, x_2, Theta, u_1, u_2, lambda) &amp; equiv K(x_1, x_2; Theta) left( mathcal{L}_i K right)(x_1, x_2, Theta, u_1, u_2, lambda) &amp; equiv mathcal{L}_i^{u_i, lambda} K(x_1, x_2; Theta) left( mathcal{L}_i mathcal{L}_j K right)(x_1, x_2, Theta, u_1, u_2, lambda) &amp; equiv mathcal{L}_i^{u_i, lambda} mathcal{L}_j^{u_j, lambda} K(x_1,x_2; Theta) end{align*} Bayes&#39; theorem allows us to compute the posterior distribution for the parameters ($ Theta, lambda$), given the measurements of $u^n$ and $u^{n-1}$, as . begin{align} P( lambda, Theta rvert u^n, u^{n-1}, I) &amp; propto P(u^n, u^{n-1} rvert Gamma, I) P( Gamma rvert I), label{e:gamma_posterior} P( lambda rvert u^n, u^{n-1}, I)&amp;= int text{d} Theta P( lambda, Theta rvert u^{n}, u^{n-1}, I) end{align}where the likelihood $P(u^n,u^{n-1} rvert Gamma, I)$ is just given by Eq. eqref{e:gp_joint_final}. In what follows, we use a simple maximum aposteriori estimate, maximizing the (log)-likelihood (assuming uniform priors for $ Theta$ and $ lambda$), in order to obtain the optimal point-wise solution. . @jit def distance(r1, r2, lbox): &quot;&quot;&quot;Compute distance vector between two positions assuming PBC&quot;&quot;&quot; r12 = r2 - r1 return r12 - np.around(r12 / lbox) * lbox @jit def K_NN(x0,x1,θ,lbox): σ0,σ = np.exp(θ[0]),np.exp(θ[1]) return 2.0/np.pi*np.arcsin(2.0*(σ0 + σ*x0*x1)/np.sqrt((1.0 + 2.0*(σ0 + σ*x0**2.0)) * (1.0 + 2.0*(σ0 + σ*x1**2)))) @jit def K_SquareExp(x0,x1,θ,lbox): logη2,logl2 = θ[0], θ[1] return np.exp(logη2 - 0.5*np.exp(-logl2)*distance(x0,x1,lbox)**2) . def repeatRow(x,nrow): ncol = len(x) return np.broadcast_to(x,(nrow,ncol)).flatten() def repeatCol(x,ncol): return np.repeat(x, ncol) @jit def logGP(Σ, y, ϵ): noise = np.ones_like(y)*ϵ L = np.linalg.cholesky(Σ + np.diag(noise)) v = np.linalg.solve(L, y) n = len(y) return (0.5*np.dot(v, v) + np.sum(np.log(np.diag(L))) + 0.5*n*np.log(2.0*np.pi)) def hiddenPhysics1D(L,Kernel,n0,n1): def K(x0,x1,θ,u0,u1,λ): return Kernel(x0,x1,θ) n = n0+n1 LK = jit(L(K,0)) LLK = jit(L(L(K,1), 0)) map_K = jit(vmap(K, in_axes=(0,None,None,None,None,None))) map_LK = jit(vmap(LK, in_axes=(0,0,None,0,None,None))) map_LLK = jit(vmap(LLK,in_axes=(0,0,None,0,0,None))) def bigK(p,x0,x1,u0,u1): θ,λ = p[:2], p[2:] Σ = np.zeros((n,n)) Σ = jax.ops.index_add(Σ, jax.ops.index[:n1,:n1], map_K(x1,x1,θ,None,None,None)) Σ = jax.ops.index_add(Σ, jax.ops.index[n1:,:n1], map_LK(repeatCol(x0,n1), repeatRow(x1,n0), θ, repeatCol(u0,n1), None, λ).reshape(n0,n1)) Σ = jax.ops.index_add(Σ, jax.ops.index[n1:,n1:], map_LLK(repeatCol(x0,n0), repeatRow(x0,n0), θ, repeatCol(u0,n0), repeatRow(u0,n0), λ).reshape(n0,n0)) Σ = jax.ops.index_add(Σ, jax.ops.index[:n1,n1:], np.transpose(Σ[n1:,:n1])) return Σ return jit(bigK) def minimizingFunction1D(params, Kernel, ϵ=1e-8): lbox,h,ns = params[&#39;lbox&#39;], params[&#39;dt&#39;], params[&#39;num&#39;] L = partial(params[&#39;L&#39;], h=h) K = partial(Kernel, lbox=lbox) bigK = hiddenPhysics1D(L,K, ns[0], ns[1]) n = ns[0] + ns[1] @jit def func0(p, x0, x1, u0, u1, scale): y = np.concatenate([u1,u0]) Σ = bigK(p, x0, x1, u0, u1) return logGP(Σ, y, ϵ)/scale func = jit(partial(func0, scale=n)) # scaled logp dfunc = jit(grad(func)) # scaled D(logp) hess = hessian(partial(func0, scale=1.0)) # unscaled hessian(logp) return bigK, func, dfunc, hess . def printer(o): print(f&#39; t loss = {o[&quot;fun&quot;]:12.6e}, niter = {o[&quot;nit&quot;]:5d}, ntotal = {o[&quot;ntotal&quot;]:5d}, Converged = {o[&quot;success&quot;]:6b} : {o[&quot;message&quot;]}&#39;) print(f&#39; t x = {o[&quot;x&quot;]}&#39;) def minimizer(f, df, x0, *args): options = {&#39;maxiter&#39;:500, &#39;disp&#39;:0} if df == None: opta = optimize.minimize(f, x0, args=args, method=&#39;Nelder-Mead&#39;, options=options) opt = optimize.minimize(f, opta[&#39;x&#39;], args=args, method=&#39;L-BFGS-B&#39;, options=options) opt[&#39;ntotal&#39;] = opta[&#39;nit&#39;] + opt[&#39;nit&#39;] else: opta = optimize.minimize(f, x0, args=args, method=&#39;Nelder-Mead&#39;, options=options) optb = optimize.minimize(f, opta[&#39;x&#39;], jac=df, args=args, method=&#39;BFGS&#39;, options=options) opt = optimize.minimize(f, optb[&#39;x&#39;], jac=df, args=args, method=&#39;TNC&#39;, options=options) opt[&#39;ntotal&#39;] = opta[&#39;nit&#39;] + optb[&#39;nit&#39;] + opt[&#39;nit&#39;] return opt def paramUncertainty(num, opt, hessian, *args): x = opt[&#39;x&#39;] σ = np.sqrt(np.diag(np.linalg.inv(hessian(x, *args)))) m = len(σ) - num for i in range(num): print(f&#39;λ_{i:1d} = {x[m+i]:8.3f} +/- {σ[m+i]:.0e}&#39;) . def D(f,arg): return grad(f, arg) def D2(f,arg): return grad(grad(f,arg), arg) def D3(f,arg): return grad(grad(grad(f,arg), arg), arg) def D4(f,arg): return grad(grad(grad(grad(f,arg), arg), arg), arg) . #collapse def plotSolution(solution, ts, lbl): def _plot_line_points(ax, x, u, i, **kargs): ax.plot(x, u, **kargs) ax.plot(x[i], u[i], color=color[&#39;dark&#39;], marker=&#39;x&#39;, ls=&#39;None&#39;, mew=2) fig = plt.figure(figsize=(24,10), constrained_layout=True) gs = fig.add_gridspec(2, 2) ax = fig.add_subplot(gs[0,:]) bx = fig.add_subplot(gs[1,0]) cx = fig.add_subplot(gs[1,1]) #plot full solution + training data slices t,x,u = solution[&#39;t&#39;], solution[&#39;x&#39;], solution[&#39;u&#39;] T,X = np.meshgrid(t,x,indexing=&#39;ij&#39;) uu = u if u.dtype == onp.complex: uu = np.abs(uu) ax.pcolormesh(T,X,uu, shading=&#39;gouraud&#39;, cmap=mpl.cm.inferno) ax.vlines(t[ts], np.min(x), np.max(x), colors=&#39;w&#39;) ax.set_title(r&#39;$u(t,x)$&#39;, fontsize=22) ax.set_xlabel(r&#39;$t$&#39;, fontsize=22) ax.set_ylabel(r&#39;$x$&#39;, fontsize=22) addtxt(ax, 0.025, 0.9, lbl, fs=22, clr=&#39;w&#39;, bclr=&#39;gray&#39;) #plot slice for i,axis in enumerate([bx, cx]): ti = ts[i] addtxt(axis, 0.05, 0.8, f&#39;$t={t[ti]:.3f}$&#39;, fs=22) if u.dtype == onp.float: axis.plot(x, u[ti], color=color[&#39;mid&#39;]) elif u.dtype == onp.complex: axis.plot(x, np.real(u[ti]), color=color[&#39;mid&#39;]) axis.plot(x, np.imag(u[ti]), color=color[&#39;mid&#39;], ls=&#39;--&#39;) axis.set_xlabel(r&#39;$x$&#39;, fontsize=22) axis.set_ylabel(r&#39;$u(t,x)$&#39;, fontsize=22) return fig, [ax,bx,cx] def addTrainingPoints(fig, axes, x0, x1, u0, u1): for ax,x,u in zip(axes, [x0, x1], [u0, u1]): addtxt(ax, 0.05, 0.7, f&#39;${len(u)}$ training points&#39;, fs=22) if u.dtype == onp.float: ax.plot(x, u, color=color[&#39;dark&#39;], marker=&#39;x&#39;, ls=&#39;None&#39;, mew=2) elif u.dtype == onp.complex: ax.plot(x, np.real(u), color=color[&#39;dark&#39;], marker=&#39;x&#39;, ls=&#39;None&#39;, mew=2) ax.plot(x, np.imag(u), color=color[&#39;dark&#39;], marker=&#39;x&#39;, ls=&#39;None&#39;, mew=2) return fig, axes . . def randomSelection1D(n, ks): return [np.sort(random.choice(n, size=k, replace=False)) for k in ks] def sampleSolution1D(sol, t0, i0): return sol[&#39;x&#39;][i0], sol[&#39;u&#39;][t0][i0] def sampleSolutionPairs1D(sol, ts, shape, nsamples): samples = randomSelection1D(shape[0], nsamples) x0,u0 = sampleSolution1D(sol, ts[0], samples[0]) x1,u1 = sampleSolution1D(sol, ts[1], samples[1]) return [x0,x1,u0,u1] def trajectoryParams(func, dfunc, solution, sampler, nsamples, init, seed): random.seed(seed) nt,shape = len(solution[&#39;t&#39;]), solution[&#39;x&#39;].shape params = [init] success = [] for t0 in tqdm(range(nt-1)): args = sampler(solution, [t0, t0+1], shape, nsamples) opt = minimizer(func, dfunc, params[-1], *args) params.append(opt[&#39;x&#39;]) success.append(opt[&#39;success&#39;]) return np.array(params)[1:],np.array(success, dtype=onp.int) . Burgers . begin{align*} 0 &amp;= partial_t u - lambda_2 partial_{xx} u + lambda_1 u partial_x u mathcal{L}^{ tilde{u}} &amp;= 1 + h big[ - lambda_2 partial_{xx} + lambda_1 tilde{u} partial_x big] end{align*} def L_Burgers(K, arg, h): if arg == 0: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(-λ[1]*(D2(K,0)(x0,x1,θ,u0,u1,λ)) + λ[0]*u0*(D(K,0)(x0,x1,θ,u0,u1,λ))) else: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(-λ[1]*(D2(K,1)(x0,x1,θ,u0,u1,λ)) + λ[0]*u1*(D(K,1)(x0,x1,θ,u0,u1,λ))) equations[&#39;burgers&#39;][&#39;L&#39;] = L_Burgers . solution = loadData(dataset_file, &#39;burgers&#39;) params = equations[&#39;burgers&#39;] updateParams(params, solution) random.seed(1256) args = sampleSolutionPairs1D(solution, params[&#39;ts&#39;], solution[&#39;x&#39;].shape, params[&#39;num&#39;]) fig, axes = plotSolution(solution, params[&#39;ts&#39;], params[&#39;label&#39;]) fig, axes = addTrainingPoints(fig, axes[1:], *args) plt.show() . K, func, dfunc, hess = minimizingFunction1D(params, K_SquareExp) . opt = minimizer(func,None, np.zeros(4),*args); printer(opt) . loss = -6.542355e+00, niter = 0, ntotal = 355, Converged = 0 : b&#39;ABNORMAL_TERMINATION_IN_LNSRCH&#39; x = [-4.32697844 -0.50148533 1.02212026 0.09694921] . opt = minimizer(func,dfunc,np.zeros(4),*args); printer(opt) . loss = -6.542355e+00, niter = 1, ntotal = 356, Converged = 1 : Converged (|f_n-f_(n-1)| ~= 0) x = [-4.32697913 -0.50148515 1.02212285 0.09694933] . paramUncertainty(2, opt, hess, *args) . λ_0 = 1.022 +/- 1e-02 λ_1 = 0.097 +/- 1e-03 . burgers_params, success = trajectoryParams(func, dfunc, solution, sampleSolutionPairs1D, params[&#39;num&#39;], np.ones(4), 1256) print(np.average(success)) . 100%|██████████| 100/100 [03:12&lt;00:00, 1.93s/it] . 0.91 . fig, [ax,bx] = plt.subplots(figsize=(18,6), ncols=2) ax.hist(burgers_params[:,2], color=color[&#39;dark&#39;]) bx.hist(burgers_params[:,3], color=color[&#39;dark&#39;]) plt.show() . Korteweg-de Vries . begin{align} 0 &amp;= partial_t u + lambda_2 partial_{xxx} u + lambda_1 u partial_x u mathcal{L}^{ tilde{u}} &amp;= 1 + h big[ lambda_2 partial_{xxx} + lambda_1 tilde{u} partial_x big] end{align} def L_KdV(K, arg, h): if arg == 0: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(λ[1]*(D3(K,0)(x0,x1,θ,u0,u1,λ)) + λ[0]*u0*(D(K,0)(x0,x1,θ,u0,u1,λ))) else: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(λ[1]*(D3(K,1)(x0,x1,θ,u0,u1,λ)) + λ[0]*u1*(D(K,1)(x0,x1,θ,u0,u1,λ))) equations[&#39;kdv&#39;][&#39;L&#39;] = L_KdV . solution = loadData(dataset_file, &#39;kdv&#39;) params = equations[&#39;kdv&#39;] updateParams(params, solution) random.seed(1256) args = sampleSolutionPairs1D(solution, params[&#39;ts&#39;], solution[&#39;x&#39;].shape, params[&#39;num&#39;]) fig, axes = plotSolution(solution, params[&#39;ts&#39;], params[&#39;label&#39;]) fig, axes = addTrainingPoints(fig, axes[1:], *args) plt.show() . K, func, dfunc, hess = minimizingFunction1D(params, K_SquareExp) . opt = minimizer(func,dfunc,np.zeros(4),*args); printer(opt) . loss = -4.072229e+00, niter = 1, ntotal = 279, Converged = 1 : Converged (|f_n-f_(n-1)| ~= 0) x = [-5.34265784 -0.46785635 5.96778687 1.06643539] . kdv_params, success = trajectoryParams(func, dfunc, solution, sampleSolutionPairs1D, params[&#39;num&#39;], np.ones(4), 1256) print(np.average(success)) . 100%|██████████| 200/200 [12:49&lt;00:00, 3.85s/it] . 0.925 . fig, [ax,bx] = plt.subplots(figsize=(18,6), ncols=2) ax.hist(kdv_params[:,2], color=color[&#39;dark&#39;]) bx.hist(kdv_params[:,3], color=color[&#39;dark&#39;]) plt.show() . Kuramoto-Sivashinsky . begin{align} 0 &amp;= partial_t u + lambda_3 partial_{xxxx} u + lambda_2 partial_{xx} u + lambda_1 u partial_x u mathcal{L}^{ tilde{u}} &amp;= 1 + h big[ lambda_3 partial_{xxxx} + lambda_2 partial_{xx} + lambda_1 tilde{u} partial_x big] end{align} def L_KS(K, arg, h): if arg == 0: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(λ[2]*(D4(K,0)(x0,x1,θ,u0,u1,λ)) + λ[1]*(D2(K,0)(x0,x1,θ,u0,u1,λ)) + λ[0]*u0*(D(K,0)(x0,x1,θ,u0,u1,λ))) else: return lambda x0,x1,θ,u0,u1,λ : K(x0,x1,θ,u0,u1,λ) + h*(λ[2]*(D4(K,1)(x0,x1,θ,u0,u1,λ)) + λ[1]*(D2(K,1)(x0,x1,θ,u0,u1,λ)) + λ[0]*u1*(D(K,1)(x0,x1,θ,u0,u1,λ))) equations[&#39;kuramoto_sivashinsky&#39;][&#39;L&#39;] = L_KS . solution = loadData(dataset_file, &#39;kuramoto_sivashinsky&#39;) params = equations[&#39;kuramoto_sivashinsky&#39;] updateParams(params, solution) random.seed(1256) args = sampleSolutionPairs1D(solution, params[&#39;ts&#39;], solution[&#39;x&#39;].shape, params[&#39;num&#39;]) fig, axes = plotSolution(solution, params[&#39;ts&#39;], params[&#39;label&#39;]) fig, axes = addTrainingPoints(fig, axes[1:], *args) plt.show() . K, func, dfunc, hess = minimizingFunction1D(params, K_SquareExp) . opt = minimizer(func,None,np.array([0.0, -1.0, 0.5, 0.5, 0.5]),*args); printer(opt) . loss = 1.025009e+00, niter = 6, ntotal = 263, Converged = 1 : b&#39;CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH&#39; x = [-0.01277412 -1.66073755 0.9817695 1.02015253 0.98327998] . I gave up waiting for the derivative function to compile on my laptop... . Non-linear Schr&#246;dinger . begin{align} 0&amp;= partial_t u - i lambda_1 partial_{xx} u - i lambda_2 lvert u rvert^2 u mathcal{L}^{ tilde{u}} &amp;= 1 + frac{h}{i} big[ lambda_1 partial_{xx} + lambda_2 lvert tilde{u} rvert^2 big] end{align} Quantum Harmonic Oscillator . begin{align} 0 &amp;= partial_t u - i lambda_1 partial_{xx}u + i frac{ lambda_2}{2} x^2 u mathcal{L} &amp;= 1 + frac{h}{i} left[ lambda_1 partial_{xx} - frac{ lambda_2}{2} x^2 right] end{align} Reaction-Diffusion . begin{align} 0 &amp;= partial_t begin{pmatrix}u v end{pmatrix} - lambda_1 begin{pmatrix} nabla^2 &amp; 0 0 &amp; nabla^2 end{pmatrix} begin{pmatrix}u v end{pmatrix} - begin{pmatrix} lambda_2 (1 - A^2) &amp; lambda_3 A^2 - lambda_3 A^2 &amp; lambda_2(1 - A^2) end{pmatrix} begin{pmatrix}u v end{pmatrix}, qquad qquad (A^2 = u^2 + v^2) mathcal{L}^{ tilde{u}, tilde{v}} &amp;= mathcal{I} + h left[- lambda_1 begin{pmatrix} nabla^2 &amp; 0 0 &amp; nabla^2 end{pmatrix} - begin{pmatrix} lambda_2(1- tilde{A}^2) &amp; lambda_3 tilde{A}^2 - lambda_3 tilde{A}^2 &amp; lambda_2(1- tilde{A}^2) end{pmatrix} right], qquad qquad qquad qquad ( tilde{A}^2 = tilde{u}^2 + tilde{v}^2) end{align} Appendix . Solving nonlinear PDEs: Exponential time-difference methods . The pde we have considered are of the form . begin{align} partial_{t}u &amp;= mathcal{L}u + mathcal{G}(u,t) label{e:uLN} end{align}where $u$ is the solution vector (not necessarily a scalar), $ mathcal{L}$ represents a linear operator (with constant coefficients), which is easily solved for and does not depend on time, and $ mathcal{G}$ is a non-linear operator. Note that this includes all equations considered in the paper. Also, . With the following change of variables . begin{align*} v &amp;= e^{- mathcal{L} t} u partial_{t} v &amp;= e^{- mathcal{L} t} partial_{t} u - e^{- mathcal{L}t} mathcal{L}u end{align*}Eq. eqref{e:uLN} becomes begin{align*} partial_{t}v &amp;= e^{- mathcal{L} t} mathcal{G}(t, u) end{align*} . which can be solved analytically to yield . begin{align} v(t_n + h) &amp;= v(t_n) + int_0^h textrm{d}{ tau}e^{- mathcal{L} tau} mathcal{G} left({t_n+ tau, e^{ mathcal{L} tau}v(t_n + tau)} right) notag u(t_n + h) &amp;= e^{h mathcal{L}} Big[{u(t_n) + int_0^h textrm{d}{ tau} e^{- mathcal{L} tau} mathcal{G} big({t_n+ tau, u(t_n + tau)} big)} Big] label{e:un_int} end{align}The problem, then, is how to treat the integral appearing on the right-hand side of the equation. . The $ varphi$ functions . For what follows, it will be useful to introduce the $ varphi_n$ functions, which can be defined in integral, series, or recursion form as (Schmelzer &amp; Trefethen, 2007) . begin{align} varphi_n(z) &amp;= begin{cases} e^{z} &amp; n = 0 frac{1}{ left({n-1} right)!} int_0^1 textrm{d}{s} e^{z(1-s)} s^{n-1}&amp; n &gt; 0 end{cases} label{e:phin_int} varphi_n(z) &amp;= sum_{k=0}^{ infty} frac{z^k}{ left({k + n} right)!} label{e:phin_sum} varphi_n(z) &amp;= frac{ varphi_{n-1}(z) - frac{1}{ left({n-1} right)!}}{z}, qquad varphi_0(z) = e^{z} label{e:phin_rec} end{align}for $z$ a complex scalar. The first four $ varphi$ functions are given by . begin{align*} varphi_0(z) &amp;= e^{z} &amp; varphi_2(z) &amp;= frac{e^z - 1 - z}{z^2} varphi_1(z) &amp;= frac{e^z - 1}{z} &amp; varphi_3(z) &amp;= frac{e^z - 1 -z - frac{1}{2}z^2}{z^3} end{align*}Note that in the case of a (non-diagonal) matrix operator $ Lambda$, the $ varphi$ functions would become . begin{align*} varphi_0( Lambda) &amp;= e^{ Lambda} &amp; varphi_2( Lambda) &amp;= Lambda^{-2} left({e^{ Lambda} - I - Lambda} right) varphi_1( Lambda) &amp;= Lambda^{-1} left({e^{ Lambda} - 1} right) &amp; varphi_3( Lambda) &amp;= Lambda^{-3} bigg[{e^{ Lambda} - I - Lambda - frac{1}{2} Lambda^{2}} bigg] end{align*} Unfortunately, the $ varphi_n(z)$ suffer from a catastrophic error cancellation for $ lvert z rvert lesssim 1$ (small eigenvalues), which gets progressively worse for higher-order functions (i.e., as $n$ increases). . Below we evaluate $ varphi_n(z)$ ($n=1,2,3$) for $10^{-13} &lt; z &lt; 10^{0}$, using the (naive) direct calculation, a Taylor expansion for small argument values, and a high-precision calculation through the mpmath library, which allows for arbitrary precision. The Taylor expansion functions are adapted from the Phi.h header library written by John Bowman. Note that this requires going up to $6$, $8$, $18$-order for $ varphi_1$, $ varphi_2$, $ varphi_3$, respectively. . importlib.reload(phi) def phin_direct(n,z): ans,nonzero = np.zeros_like(z), z != 0 if n == 1: ans = jax.ops.index_update(ans, jax.ops.index[nonzero], np.expm1(z[nonzero])/z[nonzero]) ans = jax.ops.index_update(ans, jax.ops.index[np.logical_not(nonzero)], 1) elif n == 2: ans = jax.ops.index_update(ans, jax.ops.index[nonzero], (np.expm1(z[nonzero]) - z[nonzero])/(z[nonzero]**2)) ans = jax.ops.index_update(ans, jax.ops.index[np.logical_not(nonzero)], 1/2) elif n == 3: ans = jax.ops.index_update(ans, jax.ops.index[nonzero], (np.expm1(z[nonzero]) - z[nonzero] - 0.5*z[nonzero]**2)/(z[nonzero]**3)) ans = jax.ops.index_update(ans, jax.ops.index[np.logical_not(nonzero)], 1/6) else: return return ans def phin_taylor(n,z): Coeff = np.array([1.0,1.0/2.0,1.0/6.0,1.0/24.0,1.0/120.0, 1.0/720.0,1.0/5040.0,1.0/40320.0, 1.0/362880.0,1.0/3628800.0,1.0/39916800.0,1.0/479001600.0,1.0/6227020800.0, 1.0/87178291200.0,1.0/1307674368000.0,1.0/20922789888000.0,1.0/355687428096000.0, 1.0/6402373705728000.0,1.0/121645100408832000.0,1.0/2432902008176640000.0,1.0/51090942171709440000.0, 1.0/1124000727777607680000.0]) ans = np.zeros_like(z) if n == 1: z2 = z*z z3 = z2*z large = np.abs(z) &gt; 4e-2 small = np.logical_not(large) ans = jax.ops.index_update(ans, jax.ops.index[large], np.expm1(z[large])/z[large]) ans = jax.ops.index_update(ans, jax.ops.index[small], Coeff[0] + Coeff[1]*z[small] + Coeff[2]*z2[small] + Coeff[3]*z3[small] + Coeff[4]*z2[small]**2 + Coeff[5]*z2[small]*z3[small] + Coeff[6]*z3[small]**2) elif n == 2: z2 = z*z z3 = z2*z z5 = z2*z3 z7 = z5*z2 z8 = z7*z small = np.abs(z) &lt; 0.1 large = np.abs(z) &gt; 1.0 medium= np.logical_not(np.logical_or(small, large)) ans = jax.ops.index_update(ans, jax.ops.index[large], (np.expm1(z[large])-z[large])/z2[large]) ans = jax.ops.index_update(ans, jax.ops.index[medium], Coeff[1] + z[medium]*Coeff[2] + z2[medium]*Coeff[3] + z3[medium]*Coeff[4] + z2[medium]*z2[medium]*Coeff[5] + z5[medium]*Coeff[6] + z3[medium]*z3[medium]*Coeff[7] + z7[medium]*Coeff[8] + z8[medium]*Coeff[9] + z8[medium]*z[medium]*Coeff[10] + z5[medium]*z5[medium]*Coeff[11] + z8[medium]*z3[medium]*Coeff[12] + z7[medium]*z5[medium]*Coeff[13] + z8[medium]*z5[medium]*Coeff[14] + z7[medium]*z7[medium]*Coeff[15] + z8[medium]*z7[medium]*Coeff[16] + z8[medium]*z8[medium]*Coeff[17]) ans = jax.ops.index_update(ans, jax.ops.index[small], Coeff[1] + z[small]*Coeff[2] + z2[small]*Coeff[3] + z3[small]*Coeff[4] + z2[small]*z2[small]*Coeff[5] + z5[small]*Coeff[6] + z3[small]*z3[small]*Coeff[7] + z7[small]*Coeff[8] + z8[small]*Coeff[9]) elif n == 3: z2 = z*z z3 = z2*z z5 = z2*z3 z7 = z5*z2 z8 = z7*z z16= z8*z8 small = np.abs(z) &lt; 0.1 large = np.abs(z) &gt; 1.6 medium= np.logical_not(np.logical_or(small, large)) ans = jax.ops.index_update(ans, jax.ops.index[large], (np.expm1(z[large])-0.5*z2[large]-z[large])/z3[large]) ans = jax.ops.index_update(ans, jax.ops.index[medium], Coeff[2] + z[medium]*Coeff[3] + z2[medium]*Coeff[4] + z3[medium]*Coeff[5] + z2[medium]*z2[medium]*Coeff[6] + z5[medium]*Coeff[7] + z3[medium]*z3[medium]*Coeff[8] + z7[medium]*Coeff[9] + z8[medium]*Coeff[10] + z8[medium]*z[medium]*Coeff[11] + z5[medium]*z5[medium]*Coeff[12] + z8[medium]*z3[medium]*Coeff[13] + z7[medium]*z5[medium]*Coeff[14] + z8[medium]*z5[medium]*Coeff[15] + z7[medium]*z7[medium]*Coeff[16] + z8[medium]*z7[medium]*Coeff[17] + z16[medium]*Coeff[18] + z16[medium]*z[medium]*Coeff[19] + z16[medium]*z2[medium]*Coeff[20]) ans = jax.ops.index_update(ans, jax.ops.index[small], Coeff[2] + z[small]*Coeff[3] + z2[small]*Coeff[4] + z3[small]*Coeff[5] + z2[small]*z2[small]*Coeff[6] + z5[small]*Coeff[7] + z3[small]*z3[small]*Coeff[8] + z7[small]*Coeff[9] + z8[small]*Coeff[10]) else: return np.array([]) return ans zvals = np.logspace(-15, 0, 16)[::-1] def get_phivals(n=1): phivals = pd.DataFrame({&#39;z&#39;:zvals}) for f,key in zip([phin_direct, phin_taylor, phi.phin], [&#39;direct&#39;, &#39;taylor&#39;, &#39;exact&#39;]): phivals[key] = f(n,zvals) return phivals phivals = {str(n):get_phivals(n) for n in [1, 2, 3]} for key in phivals.keys(): phis = phivals[key] print(key, np.allclose(phis[&#39;taylor&#39;], phis[&#39;exact&#39;])) phivals[&#39;1&#39;].style.format(&quot;{:20.16e}&quot;) . 1 True 2 True 3 True . z direct taylor exact . 0 1.0000000000000000e+00 | 1.7182818284590451e+00 | 1.7182818284590451e+00 | 1.7182818284590453e+00 | . 1 1.0000000000000005e-01 | 1.0517091807564767e+00 | 1.0517091807564767e+00 | 1.0517091807564762e+00 | . 2 1.0000000000000011e-02 | 1.0050167084168160e+00 | 1.0050167084168058e+00 | 1.0050167084168058e+00 | . 3 1.0000000000000020e-03 | 1.0005000000000002e+00 | 1.0005001667083417e+00 | 1.0005001667083417e+00 | . 4 9.9999999999999802e-05 | 1.0000499999999999e+00 | 1.0000500016667084e+00 | 1.0000500016667084e+00 | . 5 9.9999999999999788e-06 | 1.0000050000000000e+00 | 1.0000050000166667e+00 | 1.0000050000166667e+00 | . 6 9.9999999999999995e-07 | 1.0000004999999998e+00 | 1.0000005000001668e+00 | 1.0000005000001666e+00 | . 7 9.9999999999999995e-08 | 1.0000000500000001e+00 | 1.0000000500000017e+00 | 1.0000000500000017e+00 | . 8 1.0000000000000000e-08 | 1.0000000050000000e+00 | 1.0000000050000000e+00 | 1.0000000050000000e+00 | . 9 1.0000000000000001e-09 | 1.0000000005000000e+00 | 1.0000000005000000e+00 | 1.0000000005000000e+00 | . 10 9.9999999999999590e-11 | 1.0000000000500000e+00 | 1.0000000000500000e+00 | 1.0000000000500000e+00 | . 11 9.9999999999999994e-12 | 1.0000000000050000e+00 | 1.0000000000050000e+00 | 1.0000000000050000e+00 | . 12 9.9999999999999998e-13 | 1.0000000000005000e+00 | 1.0000000000005000e+00 | 1.0000000000005000e+00 | . 13 1.0000000000000000e-13 | 1.0000000000000500e+00 | 1.0000000000000500e+00 | 1.0000000000000500e+00 | . 14 1.0000000000000000e-14 | 1.0000000000000051e+00 | 1.0000000000000051e+00 | 1.0000000000000051e+00 | . 15 1.0000000000000001e-15 | 1.0000000000000007e+00 | 1.0000000000000004e+00 | 1.0000000000000004e+00 | . For $ varphi_1$ using the &#39;expm1&#39; function instead of directly evaluating $( exp{(z)} - 1)$ will give reasonably good results. However, there are no easy fixes for the higher order functions. . (lambda df : onp.abs(df[&#39;direct&#39;] - df[&#39;exact&#39;]))(phivals[&#39;1&#39;]) . 0 2.220446e-16 1 4.440892e-16 2 1.021405e-14 3 1.667083e-07 4 1.666709e-09 5 1.666667e-11 6 1.667555e-13 7 1.554312e-15 8 0.000000e+00 9 0.000000e+00 10 0.000000e+00 11 0.000000e+00 12 0.000000e+00 13 0.000000e+00 14 0.000000e+00 15 2.220446e-16 dtype: float64 . Notice that the Taylor expansion produces the same results (to machine precision) as those obtained using the mpmath library (using ~100 digit precision). . (lambda df : onp.abs(df[&#39;taylor&#39;] - df[&#39;exact&#39;]))(phivals[&#39;3&#39;]) . 0 2.775558e-17 1 2.775558e-17 2 0.000000e+00 3 2.775558e-17 4 0.000000e+00 5 0.000000e+00 6 0.000000e+00 7 0.000000e+00 8 0.000000e+00 9 0.000000e+00 10 0.000000e+00 11 0.000000e+00 12 2.775558e-17 13 0.000000e+00 14 0.000000e+00 15 0.000000e+00 dtype: float64 . ETD-RK . Exponential time-difference Runge-Kutta methods up to order four have been developed to solve Eq. eqref{e:un_int}. They start from the following variation of constants formula begin{align} u(t_n + h) &amp;= e^{h mathcal{L}} u_n + int_{0}^h textrm{d}{ tau} e^{- mathcal{L}( tau - h)} g left({t_n + tau, u(t_n + tau)} right) notag end{align} and introduce the following general class of one-step methods (Hochbruck &amp; Ostermann, 2010): begin{align} u_{n+1} &amp;= chi(h mathcal{L})u_n + h sum_{i=1}^s b_i(h mathcal{L}) G_{ni} notag G_{nj} &amp;= mathcal{G}(t_n + c_j h, U_{nj}) label{e:etdrkn} U_{ni} &amp;= chi_{i}(h mathcal{L})u_n + h sum_{j=1}^{s} a_{ij}(h mathcal{L}) G_{nj} notag end{align} where the method coefficients $ chi$, $ chi_i$, $a_{ij}$ and $b_i$ are constructed from exponential functions (or their rational approximation) evaluated at the operator $h mathcal{L}$. In the formal limit when $b_i = b_i(0)$ and $a_{ij} = a_{ij}(0)$, we recover the standard Runge-Kutta method. It is assumed that the coefficients satisfy the following properties begin{align*} sum_{j=1}^s b_j(0) &amp;= 1, &amp; sum_{j=1}^{s} a_{ij}(0) &amp;= c_i qquad (i = 1, ldots,s) chi(0) = chi_i(0) &amp;= 1 end{align*} Hochbruck and Ostermann list the conditions that the constants should satisfy to guarantee the convergence of the methods (missing reference). Following them, we focus on &quot;explicit&quot; methods with begin{align} sum_{j=1}^s b_j(z) &amp;= varphi_1(z), &amp; sum_{j=1}^s a_{ij}(z)&amp;= c_i varphi_1 (c_i z) label{e:etdrkn_ab} chi(z) &amp;= e^{z} = varphi_0(z), qquad &amp; chi_i(c) &amp;= e^{c_i z} = varphi_0(c_i z) qquad 1 le i le s label{e:etdrkn_chi} end{align} The coefficients are most easily represented in tableau form as begin{align} begin{array}{c|cccc|c} c_1 &amp; &amp; &amp; &amp; &amp; chi_1(h mathcal{L}) c_2 &amp; a_{21}(h mathcal{L}) &amp; &amp; &amp; &amp; chi_2(h mathcal{L}) vdots &amp; vdots &amp; ddots &amp; &amp; &amp; vdots c_s &amp; a_{s1}(h mathcal{L}) &amp; ldots &amp; a_{s,s-1}(h mathcal{L}) &amp; &amp; chi_s(h mathcal{L}) hline &amp; b_{1}(h mathcal{L}) &amp; ldots &amp; b_{s-1}(h mathcal{L}) &amp; b_s(h mathcal{L}) &amp; chi(h mathcal{L}) end{array} label{e:rktable} end{align} As with all Runge-Kutta methods, the internal stages are of order one only, which makes the construction of higher-order methods quite involved. For what follows, we adopt the following simplifying notation begin{align} varphi_j &amp;= varphi_j left({h mathcal{L}} right) label{e:phijk} varphi_{j,k} &amp;= varphi_j left({c_k h mathcal{L}} right) notag end{align} A detailed discussion of explicit ETDRK methods up to order four be found in Refs. (Hochbruck &amp; Ostermann, 2005; Cox &amp; Matthews, 2002). . ETDRK1 . For $s=1$, we have the exponential version of Euler&#39;s method . begin{align} begin{array}{c|c} 0 &amp; hline &amp; varphi_1 end{array} label{e:etdrk1} end{align}or . begin{align*} u_{n+1} &amp;= varphi_0 u_n + h varphi_1(h mathcal{L}) mathcal{G}(t_n, u_n) end{align*}Expanding the terms inside the parenthesis to first order in $h$, gives the following approximate integrator (equivalent to Euler&#39;s method) . begin{align*} u_{n+1} &amp;= e^{h mathcal{L}} big({u_n + mathcal{L}^{-1} left({1 - e^{-h mathcal{L}}} right) mathcal{G}(t_n, u_n)} big) &amp; approx e^{h mathcal{L}} big({u_n + h mathcal{G}(t_n, u_n)} big) end{align*} ETDRK2 . For a second-order method at least two-internal stages are needed(missing reference), . begin{align} begin{array}{c|cc} 0 &amp; &amp; c_2 &amp; c_2 varphi_{1,2} &amp; hline &amp; varphi_{1} - frac{1}{c_2} varphi_{2} &amp; frac{1}{c_2} varphi_2 end{array} qquad begin{array}{c|cc} 0 &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,2} &amp; hline &amp; varphi_{1} - 2 varphi_{2} &amp; 2 varphi_2 end{array} label{e:etdrk2} end{align}or . begin{align*} U_{n1} &amp;= u_n &amp;G_{n1} &amp;= mathcal{G}(t_n, U_{n1}) U_{n2} &amp;= varphi_{0} left({h/2 mathcal{L}} right) u_n + h bigg[{ frac{1}{2} varphi_1 left({h/2 mathcal{L}} right) G_{n1}} bigg] &amp; G_{n2} &amp;= mathcal{G}(t_n + h/2, U_{n2}) u_{n+1} &amp;= varphi_{0} u_n + h big[{ left({ varphi_1 - 2 varphi_2} right)G_{n1} + 2 varphi_2G_{n2}} big] end{align*}Easing some of the restrictions, one can obtain an alternative one-parameter method, . begin{align} begin{array}{c|cc} 0 &amp; &amp; c_2 &amp; c_2 varphi_{1,2} &amp; hline &amp; big({1 - frac{1}{2 c_2}} big) varphi_1 &amp; frac{1}{2 c_2} varphi_1 end{array} qquad begin{array}{c|cc} 0 &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,2} &amp; hline &amp; 0 &amp; varphi_1 end{array} label{e:etdrk2b} end{align}such that . begin{align*} u_{n+1} &amp;= varphi_0 u_n + h varphi_1G_{n2} end{align*}Both methods ( ref{e:etdrk2} and ref{e:etdrk2b}) are B-consistent of order one. . ETDRK3 . A third order method, with three internal sub-stages, is given by . begin{align} begin{array}{c|ccc} 0 &amp; &amp; &amp; c_2 &amp; c_2 varphi_{1,2} &amp; &amp; frac{2}{3} &amp; frac{2}{3} varphi_{1,3} - frac{4}{9c_2} varphi_{2,3} &amp; frac{4}{9 c_2} varphi_{2,3} &amp; hline &amp; varphi_1 - frac{3}{2} varphi_2 &amp; 0 &amp; frac{3}{2} varphi_2 end{array} qquad begin{array}{c|ccc} 0 &amp; &amp; &amp; frac{1}{3} &amp; frac{1}{3} varphi_{1,2} &amp; &amp; frac{2}{3} &amp; frac{2}{3} varphi_{1,3} - frac{4}{3} varphi_{2,3} &amp; frac{4}{3} varphi_{2 ,3} &amp; hline &amp; varphi_1 - frac{3}{2} varphi_2 &amp; 0 &amp; frac{3}{2} varphi_2 end{array} label{e:etdrk3} end{align}such that . begin{align*} U_{n1} &amp;= u_n &amp; G_{n1} &amp;= mathcal{G}(t_n, U_{n1}) U_{n2} &amp;= varphi_{0} left({h/3 mathcal{L}} right)u_n + h bigg[{ frac{1}{3} varphi_1 left({h/3 mathcal{L}} right)} bigg] &amp; G_{n2} &amp;= mathcal{G}(t_n + h/3, U_{n2}) U_{n3} &amp;= varphi_{0} left({2h/3 mathcal{L}} right) u_n + h bigg[{ bigg({ frac{2}{3} varphi_{1} left({2h/3 mathcal{L}} right) - frac{4}{3} varphi_{2} left({2h/3 mathcal{L}} right)}G_{n1} bigg)} bigg] &amp; G_{n3} &amp;= mathcal{G} left({t_n + 2h/3, U_{n3}} right) &amp; qquad qquad qquad quad +h bigg[{ frac{4}{3} varphi_{2} left({2h/3 mathcal{L}} right) G_{n2}} bigg] &amp;&amp; u_{n+1} &amp;= varphi_0 u_n + h bigg[{ bigg({ varphi_1 - frac{3}{2} varphi_2} bigg) G_{n1} + frac{3}{2} varphi_2 G_{n3}} bigg] end{align*}The standard choice is to take $c_2 = 1/3$ and this method is order three (worst case $2.75$). It is referred to as the Heun method. . ETDRK4 . A full fourth-order method requires $5$ internal stages, as was shown by Hochbruck and Ostermann (Hochbruck &amp; Ostermann, 2005; Rothauge et al., 2016), . begin{align} &amp; begin{array}{c|ccccc} 0 &amp; &amp; &amp; &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,2} &amp; &amp; &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,3} - varphi_{2,3} &amp; varphi_{2,3} &amp; &amp; &amp; 1 &amp; varphi_{1,4} - 2 varphi_{2,4} &amp; varphi_{2,4} &amp; varphi_{2,4} &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,5} - frac{1}{4} varphi_{2,5} - a_{5,2} &amp; a_{5,2} &amp; a_{5,2} &amp; frac{1}{4} varphi_{2,5} - a_{5,2} &amp; hline &amp; varphi_1 - 3 varphi_2 + 4 varphi_3 &amp; 0 &amp; 0 &amp; - varphi_2 + 4 varphi_3 &amp; 4 varphi_2- 8 varphi_3 end{array} label{e:etdrk4} a_{5,2} &amp;= frac{1}{2} varphi_{2,5} - varphi_{3,4} + frac{1}{4} varphi_{2,4} - frac{1}{2} varphi_{3,5} notag &amp;= frac{1}{2} varphi_2(h/2 mathcal{L}) - varphi_3(h mathcal{L}) + frac{1}{4} varphi_2(h mathcal{L}) - frac{1}{2} varphi_3(h/2 mathcal{L}) notag end{align}which leads to the following update scheme . begin{align*} U_{n1} &amp;= u_n &amp; G_{n1} &amp;= mathcal{G}(t_n, u_n) U_{n2} &amp;= varphi_0(h/2 mathcal{L})u_n + h bigg[{ frac{1}{2} varphi_{1}(h/2 mathcal{L}) G_{n1}} bigg] &amp; G_{n2}&amp;= mathcal{G}(t_n + h/2, U_{n2}) U_{n3} &amp;= varphi_0(h/2 mathcal{L})u_n + h bigg[{ bigg({ frac{1}{2} varphi_1(h/2 mathcal{L}) - varphi_2(h/2 mathcal{L})} bigg) G_{n1} + varphi_2(h/2 mathcal{L})G_{n2}} bigg] &amp; G_{n3} &amp;= mathcal{G}(t_n + h/2, U_{n3}) U_{n4} &amp;= varphi_0 u_n + h bigg[{ big({ varphi_1 - 2 varphi_2} big)G_{n1} + varphi_2G_{n2} + varphi_2G_{n3}} bigg] &amp; G_{n4} &amp;= mathcal{G}(t_n + h, U_{n4}) U_{n5} &amp;= varphi_0(h/2 mathcal{L})u_n + h bigg[{ bigg({ frac{1}{2} varphi_1(h/2 mathcal{L}) - frac{1}{4} varphi_2(h/2 mathcal{L}) - a_{5,2}} bigg)G_{n1}} bigg] &amp; G_{n5} &amp;= mathcal{G}(t_n + h/2, U_{n5}) &amp; qquad qquad qquad , , ,+h bigg[{a_{5,2}G_{n2} + a_{5,2}G_{n,3} + bigg({ frac{1}{4} varphi_2(h/2 mathcal{L}) - a_{5,2}} bigg)G_{n4}} bigg] &amp; &amp; u_{n+1} &amp;= varphi_0 u_n + h bigg[{ left({ varphi_1 - 3 varphi_2 + 4 varphi_3} right)G_{n1} + left({- varphi_2 + 4 varphi_3} right)G_{n4} + left({4 varphi_2 - 8 varphi_3} right)G_{n5}} bigg] end{align*}Krogstad (Krogstad, 2005) proposed an alternative (with only four internal stages), which converges weakly to order four, but in the worst case reduces to third order . begin{align} begin{array}{c|cccc} 0 &amp; &amp; &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,2} &amp; &amp; &amp; frac{1}{2} &amp; frac{1}{2} varphi_{1,3} - varphi_{2,3} &amp; varphi_{2,3} &amp; &amp; 1 &amp; varphi_{1,4} - 2 varphi_{2,4} &amp; 0 &amp; 2 varphi_{2,4} &amp; hline &amp; varphi_1 - 3 varphi_2 + 4 varphi_3 &amp; 2 varphi_2 - 4 varphi_3 &amp; 2 varphi_2 - 4 varphi_3 &amp; - varphi_2 + 4 varphi_3 end{array} label{e:etdrk4b} end{align}This leads to the following update scheme . begin{align*} U_{n1} &amp;= u_n &amp; G_{n1} &amp;= mathcal{G}(t_n, U_{n1}) U_{n2} &amp;= varphi_0(h/2 mathcal{L}) u_n + h bigg[{ frac{1}{2} varphi_1(h/2 mathcal{L}) G_{n1}} bigg] &amp; G_{n2} &amp;= mathcal{G}(t_n + h/2, U_{n2}) U_{n3} &amp;= varphi_0(h/2 mathcal{L}) u_n + h bigg[{ bigg({ frac{1}{2} varphi_1(h/2 mathcal{L}) - varphi_2(h/2 mathcal{L})} bigg)G_{n1} + varphi_2(h/2 mathcal{L})G_{n2}} bigg] &amp; G_{n3} &amp;= mathcal{G}(t_n + h/2, U_{n3}) U_{n4} &amp;= varphi_0 u_n + h bigg[{ big({ varphi_1 - 2 varphi_2} big)G_{n1} + 2 varphi_2G_{3}} bigg] &amp; G_{n4} &amp;= mathcal{G}(t_n + h, U_{n4}) u_{n+1} &amp;= varphi_0 u_n + h bigg[{ left({ varphi_1 - 3 varphi_2 + 4 varphi_3} right)G_{n1} + left({2 varphi_2 - 4 varphi_3} right) left({G_{n2} + G_{n3}} right) + left({- varphi_2 + 4 varphi_3} right)G_{n4}} bigg] end{align*}They call this method ETDRK4-B (which is guaranteed to be of third order at least), in contrast to the original ETDRK4 method proposed by Cox and Mathews (which is second order in the worst-case scenario). . Numerical Examples . For completeness, let&#39;s verify the solutions provide by Rudy et al. We will also use a pseudo-spectral method, assuming periodic boundary conditions, but with the ETDRK4 method of Hochbruck and Ostermann (instead of just a vanilla Runge-Kutta), as this allows us to handle the stiff differential operators exactly. . def setup_time(L, t, α): &quot;&quot;&quot;Estimate characteristic time step τ given stiff linear operator Args: L: the linear operator (array) t: the linear time grid for the solution (assuming equal spacing and t_0 = 0) α: scale factor Returns: Δt = α τ with τ ~ 1 / max(h L) such that t_i = i * gts * Δt &quot;&quot;&quot; tdump = t[1]-t[0] dt = tdump τ = 1.0 / np.max(np.abs(L)) dt0 = α * τ while dt &gt; dt0: # half dt until it is less than the &#39;optimum&#39; dt0 dt /= 2 gts = int(tdump/dt) dt = tdump / gts params = {&#39;dt&#39;:dt, &#39;gts&#39;:gts, &#39;frames&#39;:len(t)} print(f&quot;τ = {τ:.2e}, Δt = {dt:.2e}, gts = {gts:6d}, steps = {gts*len(t):10d}&quot;) return params . Pseudo-Spectral Methods | . We will evaluate all derivatives using a spectral method, whereby $ partial_x longrightarrow i k_x$ ($ boldsymbol{ nabla} longrightarrow i boldsymbol{k}$), and only transform to real-space to compute the non-linear terms. . For example, the 1D Burger&#39;s, Korteweg-de Vries and Kuramoto-Sivishinky equations all contain an advection term of the form begin{align} &amp;u partial_x u &amp; longrightarrow&amp; i mathcal{F} left[u mathcal{F}^{-1} left(k_x hat{u} right) right] = &amp; frac{1}{2} partial_x u^2 &amp; longrightarrow&amp; frac{1}{2} (i k) mathcal{F} left[ left( mathcal{F}^{-1}( hat{u}) right)^2 right] end{align} with $ mathcal{F}$ denoting a Fourier transform, and $ hat{u}(k) = mathcal{F}(u(x))$. For what follows, we will denote this 1D advection term as begin{align} mathcal{A}( hat{u}) &amp;= frac{1}{2} (i k) mathcal{F} left[ left( mathcal{F}^{-1}( hat{u}) right)^2 right] end{align} . rfft = jit(np.fft.rfft) irfft = jit(np.fft.irfft) fft = jit(np.fft.fft) ifft = jit(np.fft.ifft) rfftn = jit(np.fft.rfftn) irfftn= jit(np.fft.irfftn) @jit def advection1d(uk,k,λ): return -0.5j*λ*k*rfft(irfft(uk)**2) . Burgers begin{align} partial_t u &amp;= lambda_2 partial_{xx} u - lambda_1 u partial_x u partial_t hat{u} &amp;= - lambda_2 k_x^2 hat{u} - lambda_1 mathcal{A}( hat{u}) end{align} | . begin{align} mathcal{L} &amp; longrightarrow - lambda_2 k^2 mathcal{G}( hat{u}) &amp; longrightarrow - lambda_1 mathcal{A}( hat{u}) end{align} def setup_burgers(x, t, α=1.0): k = np.fft.rfftfreq(len(x), d=x[1]-x[0])*2*onp.pi λ1,λ2 = 1.0, 0.1 L = -λ2*k**2 G = jit(partial(advection1d, k=k, λ=λ1)) params = setup_time(L, t, α) return params, L, G equations[&#39;burgers&#39;][&#39;setup&#39;] = setup_burgers . Korteweg-de Vries | . begin{align} partial_t u &amp;= - lambda_2 partial_{xxx} u - lambda_1 u partial_x u partial_t hat{u} &amp;= i lambda_2 k_x^3 hat{u} - lambda_1 mathcal{A}( hat{u}) end{align} begin{align} mathcal{L} &amp; longrightarrow i lambda_2 k_x^3 mathcal{G}( hat{u}) &amp; longrightarrow - lambda_1 mathcal{A}( hat{u}) end{align} def setup_kdv(x, t, α=1.0): k = np.fft.rfftfreq(len(x), d=x[1]-x[0])*2*onp.pi λ1,λ2 = 6.0, 1.0 L = 1.0j*λ2*k**3 G = jit(partial(advection1d,k=k, λ=λ1)) params = setup_time(L, t, α) return params, L, G equations[&#39;kdv&#39;][&#39;setup&#39;] = setup_kdv . Kuramoto-Sivishinky | . begin{align} partial_t u &amp;= - lambda_3 partial_{xxxx} u - lambda_2 partial_{xx} u - lambda_1 u partial_x u partial_t hat{u} &amp;= - lambda_3 k_x^4 hat{u} + lambda_2 k_x^2 hat{u} - lambda_1 mathcal{A}( hat{u}) &amp;= k_x^2 left( lambda_2 - lambda_3 k_x^2 right) hat{u} - lambda_1 mathcal{A}( hat{u}) end{align} begin{align} mathcal{L} &amp; longrightarrow k_x^2 left( lambda_2 - lambda_3 k_x^2 right) mathcal{G}( hat{u})&amp; longrightarrow - lambda_1 mathcal{A}( hat{u}) end{align} def setup_ks(x, t, α=1.0): k = np.fft.rfftfreq(len(x), d=x[1]-x[0])*2*onp.pi λ1,λ2,λ3 = 1.0, 1.0, 1.0 L = k**2*(λ2 - λ3*k**2) G = jit(partial(advection1d,k=k,λ=λ1)) params = setup_time(L, t, α) return params, L, G equations[&#39;kuramoto_sivashinsky&#39;][&#39;setup&#39;] = setup_ks . Non-Linear Schrödinger begin{align} partial_t u &amp;= i lambda_1 partial_{xx} u + i lambda_2 lvert u rvert^2 u partial_t hat{u} &amp;= - i lambda_1 k_x^2 hat{u} + i lambda_2 mathcal{F} left[ lvert u rvert^2 u right] end{align} | . begin{align} mathcal{L} &amp; longrightarrow -i lambda_1 k_x^2 mathcal{G}( hat{u}) &amp; longrightarrow i lambda_2 mathcal{F} left[ lvert u rvert^2 u right] qquad (u = mathcal{F}^{-1}( hat{u}), lvert u rvert^2 = u^* u) end{align} def setup_nls(x, t, α=1.0): k = np.fft.fftfreq(len(x), d=x[1]-x[0])*2*onp.pi λ1,λ2 = 0.5, 1.0 L = -1.0j*λ1*k**2 G = jit(lambda u:1.0j*λ2*fft((lambda v : np.abs(v)**2*v)(ifft(u)))) params = setup_time(L, t, α) return params, L, G equations[&#39;nls&#39;][&#39;setup&#39;] = setup_nls . Quantum Harmonic Oscillator | . begin{align} partial_t u &amp;= i lambda_1 partial_{xx}u - i frac{ lambda_2}{2} x^2 u partial_t hat{u} &amp;= -i lambda_1 k_x^2 hat{u} - i frac{ lambda_2}{2} mathcal{F} left[x^2 mathcal{F}^{-1}( hat{u}) right] end{align} begin{align} mathcal{L} &amp;= -i lambda_1 k_x^2 mathcal{G}( hat{u}) &amp;= - frac{ lambda_2}{2} mathcal{F} left[x^2 u right]; qquad (u = mathcal{F}^{-1}( hat{u})) end{align} def setup_harmonic_osc(x, t, α=1.0): k = np.fft.fftfreq(len(x), d=x[1]-x[0])*2*onp.pi λ1, λ2 = 0.5, 1.0 L = -1.0j*λ1*k**2 G = jit(lambda u:-0.5j*λ2*fft((lambda v : v*x**2)(ifft(u)))) params = setup_time(L, t, α) return params, L, G equations[&#39;harmonic_osc&#39;][&#39;setup&#39;] = setup_harmonic_osc . Reaction-Diffusion begin{align} partial_t begin{pmatrix}u v end{pmatrix}&amp;= lambda_1 begin{pmatrix} nabla^2 &amp; 0 0 &amp; nabla^2 end{pmatrix} begin{pmatrix}u v end{pmatrix} + begin{pmatrix} lambda_2 (1 - A^2) &amp; lambda_3 A^2 - lambda_3 A^2 &amp; lambda_2(1 - A^2) end{pmatrix} begin{pmatrix}u v end{pmatrix}, qquad qquad (A^2 = u^2 + v^2) partial_t begin{pmatrix} hat{u} hat{v} end{pmatrix} &amp;= - lambda_1 k^2 begin{pmatrix}1 &amp; 0 0 &amp; 1 end{pmatrix} begin{pmatrix} hat{u} hat{v} end{pmatrix} + mathcal{F} left[ begin{pmatrix} lambda_2 (1 - A^2) &amp; lambda_3 A^2 - lambda_3 A^2 &amp; lambda_2(1 - A^2) end{pmatrix} begin{pmatrix}u v end{pmatrix} right], qquad (k^2 = k_x^2 + k_y^2) end{align} | . begin{align} mathcal{L} &amp; longrightarrow - lambda_1 k^2 begin{pmatrix}1 &amp; 0 0 &amp; 1 end{pmatrix} mathcal{G} left( begin{pmatrix} hat{u} hat{v} end{pmatrix} right) &amp; longrightarrow mathcal{F} left[ begin{pmatrix} lambda_2 (1 - A^2) &amp; lambda_3 A^2 - lambda_3 A^2 &amp; lambda_2(1 - A^2) end{pmatrix} begin{pmatrix}u v end{pmatrix} right] end{align} rfftn = jit(np.fft.rfftn) irfftn = jit(np.fft.irfftn) rfftu = jit(lambda uu : np.array([rfftn(u) for u in uu])) irfftu = jit(lambda uu : np.array([irfftn(u) for u in uu])) def setup_reaction_diffusion(xy, t, α=1.0): @jit def Op(w, λ2, λ3): A2 = w[0]**2 + w[1]**2 # w = [u, v] dia = λ2*(1 - A2) off = λ3*A2 return np.array([ dia * w[0] + off * w[1], - off * w[0] + dia*w[1] ]) x,y = xy kx = np.fft.fftfreq(len(x), d=x[1]-x[0])*2*onp.pi ky = np.fft.rfftfreq(len(y), d=y[1]-y[0])*2*onp.pi KX,KY = np.meshgrid(kx,ky, indexing=&#39;ij&#39;) K2 = KX**2 + KY**2 λ1,λ2,λ3 = 0.1, 1.0, 1.0 L = -λ1*K2 Gr = jit(partial(Op, λ2=λ2, λ3=λ3)) G = jit(lambda w_k : rfftu(Gr(irfftu(w_k)))) params = setup_time(L, t, α) return params, L, G def load_reaction_diffusion(fname): x = np.linspace(-10, 10, num=512, endpoint=False) y = x X,Y = np.meshgrid(x,y, indexing=&#39;ij&#39;) m = 1 # number of spirals R2 = X**2 + Y**2 U = np.tanh(np.sqrt(R2)) * np.cos(m * np.angle(X + 1j*Y) - np.sqrt(R2)) V = np.tanh(np.sqrt(R2)) * np.sin(m * np.angle(X + 1j*Y) - np.sqrt(R2)) t = np.arange(0.0, 10.05, step=0.05) return {&#39;x&#39;:np.array([x,y]), &#39;u&#39;:np.array([U,V]), &#39;t&#39;:t} equations[&#39;rd&#39;] = {&#39;label&#39;: &#39;Reaction Diffusion&#39;, &#39;load&#39;:load_reaction_diffusion, &#39;integrator&#39;:etdrk.etdrk45, &#39;setup&#39;:setup_reaction_diffusion, &#39;init&#39;:rfftu, &#39;save&#39;:irfftu, &#39;α&#39;:1.0} all_names = names + [&#39;rd&#39;] . for name,α in zip(names, [0.1, 0.5, 256, 0.1, 0.1]): equations[name][&#39;α&#39;] = α equations[name][&#39;load&#39;] = partial(loadData, name=name, frame=0) equations[name][&#39;integrator&#39;] = etdrk.etdrk45 for name in [&#39;burgers&#39;, &#39;kdv&#39;, &#39;kuramoto_sivashinsky&#39;]: equations[name][&#39;init&#39;] = rfft equations[name][&#39;save&#39;] = irfft for name in [&#39;harmonic_osc&#39;, &#39;nls&#39;]: equations[name][&#39;init&#39;] = fft equations[name][&#39;save&#39;] = ifft def my_solution(*,name, integrate_function, setup_function, save_function, fname, u0, x, t, α=1.0): def arraytype(u): dmy = save_function(u) return dmy.shape, dmy.dtype def simulate(*, integrate, frames, gts): with h5.File(fname, &#39;a&#39;) as fp: u = u0 shape, dtype = arraytype(u) gp = fp.require_group(name) gp.require_dataset(name = &#39;x&#39;, shape=x.shape, dtype=x.dtype, data=x) gp.require_dataset(name = &#39;t&#39;, shape=t.shape, dtype=t.dtype, data=t) ut = gp.require_dataset(name = &#39;unew&#39;, shape=(frames,)+shape, dtype=dtype) ut[0,...] = save_function(u) for i in tqdm(range(1, frames)): for _ in range(gts): u = integrate.step(u) ut[i,...] = save_function(u) params, L, G = setup_function(x, t, α) solver = integrate_function(L=L, G=G, dt=params[&#39;dt&#39;]) solution = simulate(integrate=solver, frames=params[&#39;frames&#39;], gts=params[&#39;gts&#39;]) return solution def run(names, fname): for name in names: print(name) eq = equations[name] init_state = eq[&#39;load&#39;](fname) my_solution(name=name, integrate_function = eq[&#39;integrator&#39;], setup_function = eq[&#39;setup&#39;], save_function = eq[&#39;save&#39;], fname=fname, u0=eq[&#39;init&#39;](init_state[&#39;u&#39;]), x=init_state[&#39;x&#39;], t=init_state[&#39;t&#39;], α = eq[&#39;α&#39;]) print() . run(names, dataset_file) . burgers τ = 3.96e-03, Δt = 3.91e-04, gts = 256, steps = 25856 . 100%|██████████| 100/100 [00:03&lt;00:00, 26.91it/s] . kdv τ = 5.19e-05, Δt = 2.44e-05, gts = 4096, steps = 823296 . 100%|██████████| 200/200 [03:50&lt;00:00, 1.15s/it] . kuramoto_sivashinsky τ = 9.55e-07, Δt = 1.95e-04, gts = 2048, steps = 514048 . 100%|██████████| 250/250 [05:08&lt;00:00, 1.23s/it] . nls τ = 7.73e-05, Δt = 6.14e-06, gts = 1024, steps = 513024 . 100%|██████████| 500/500 [02:16&lt;00:00, 3.67it/s] . harmonic_osc τ = 1.74e-04, Δt = 1.22e-05, gts = 2048, steps = 821248 . 100%|██████████| 400/400 [03:32&lt;00:00, 1.88it/s] . . . def check_solutions(names): fig,ax = plt.subplots(figsize=(16,10)) with h5.File(dataset_file, &#39;r&#39;) as fp: for name in names: gp = fp[name] t = gp[&#39;t&#39;][...] us = gp[&#39;unew&#39;][...] them = gp[&#39;u&#39;][...] diff = np.abs(us - them) print(f&#39;{name:25s}:{np.max(diff):.1e}&#39;) ax.plot(t, np.max(diff, axis=1), label=name) ax.loglog() ax.legend(fontsize=22) . check_solutions(names) . burgers :1.0e-08 kdv :4.9e-06 kuramoto_sivashinsky :4.1e-05 nls :2.3e-05 harmonic_osc :1.1e-05 . Raissi, M., &amp; Karniadakis, G. E. (2018). Hidden physics models: Machine learning of nonlinear partial differential equations. Journal of Computational Physics, 357, 125–141. https://doi.org/10.1016/j.jcp.2017.11.039 | Rudy, S. H., Brunton, S. L., Proctor, J. L., &amp; Kutz, J. N. (2017). Data-driven discovery of partial differential equations. Science Advances, 3(4), e1602614. https://doi.org/10.1126/sciadv.1602614 | Schmelzer, T., &amp; Trefethen, L. N. (2007). Evaluating Matrix Functions for Exponential Integrators Via Caratheodory-Fejer Approximation and Contour Integrals. Electronic Transactions on Numerical Analysis, 29, 1–18. | Hochbruck, M., &amp; Ostermann, A. (2010). Exponential integrators. In Acta Numerica (Vol. 19, Number May 2010, pp. 209–286). https://doi.org/10.1017/S0962492910000048 | Hochbruck, M., &amp; Ostermann, A. (2005). Explicit Exponential Runge–Kutta Methods for Semilinear Parabolic Problems. SIAM Journal on Numerical Analysis, 43(3), 1069–1090. https://doi.org/10.1137/040611434 | Cox, S. M., &amp; Matthews, P. C. (2002). Exponential time differencing for stiff systems. Journal of Computational Physics, 176(2), 430–455. https://doi.org/10.1006/jcph.2002.6995 | Hochbruck, M., &amp; Ostermann, A. (2005). Exponential Runge-Kutta methods for parabolic problems. Applied Numerical Mathematics, 53(2-4), 323–339. https://doi.org/10.1016/j.apnum.2004.08.005 | Rothauge, K., Haber, E., &amp; Ascher, U. (2016). The Discrete Adjoint Method for Exponential Integration. 1–28. http://arxiv.org/abs/1610.02596 | Krogstad, S. (2005). Generalized integrating factor methods for stiff PDEs. Journal of Computational Physics, 203(1), 72–88. https://doi.org/10.1016/j.jcp.2004.08.006 | .",
            "url": "https://johnjmolina.github.io/MLKyoto/gaussian%20processes/2020/08/27/Gaussian-Processes-Applications-Hidden-Physics-Models.html",
            "relUrl": "/gaussian%20processes/2020/08/27/Gaussian-Processes-Applications-Hidden-Physics-Models.html",
            "date": " • Aug 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Gaussian Processes 1 - Theory",
            "content": "References . The standard textbook on Gaussian Processes (GP) is that of Rasmussen and Williams. The book by Murphy on Machine Learning also has a nice intro to GP and how they connect with other ML methods. Finally, the &quot;Matrix Cookbook&quot; has an extensive list of identities that are helpful for the GP derivations. . Gaussian Processes for Machine Learning. C. E. Rasmussen and C. K. I. Williams, Cambridge, the MIT Press (2006) | Machine Learning : A Probabilistic Perspective. K. P. Murphy, Cambridge, the MIT Press (2012) | The Matrix Cookbook. K. B. Petersen and M. S. Pedersen (2012) | . Preliminaries . Manipulating GP will require a bit of matrix algebra and the use of some not very well know identities (at least to the author). Thus, we will start by giving (without proof) the main results needed to derive the basic GP equations. . Let $ Sigma$ be a block matrix, defined as begin{align} Sigma &amp;= begin{pmatrix} A &amp; C D &amp; B end{pmatrix} end{align} . From the sub-matrices, define $E$ and $F$ as begin{align} E &amp;= A - C B^{-1} D F &amp;= B - D A^{-1} C end{align} . Determinant . The determinant of $ Sigma$ can be written in terms of that of $A$ and $F$, or $B$ and $E$, as . begin{align} det{ Sigma} &amp;= det{A} cdot det{F} = det{B} cdot det{E} end{align} Inverse . The matrix inverse of $ Sigma$ can also be expressed in block form as . begin{align} Sigma^{-1} &amp;= begin{pmatrix} widetilde{A} &amp; widetilde{C} widetilde{D} &amp; widetilde{B} end{pmatrix} &amp;= begin{pmatrix} E^{-1} &amp; - A^{-1} C F^{-1} -F^{-1} D A^{-1}&amp; F^{-1} end{pmatrix} &amp;= begin{pmatrix} A^{-1} + A^{-1} C F^{-1} D A^{-1} &amp; -E^{-1} C B^{-1} -B^{-1} D E^{-1} &amp; B^{-1} + B^{-1} D E^{-1} C B^{-1} end{pmatrix} end{align} Derivatives . When &quot;training&quot; our GP models, it will be useful to be able to compute derivatives of these block matrices with respect to the hyper-parameters $ Theta$. In particular, we will need the derivatives of the matrix inverse and the log of the determinant. These are given by . begin{align} frac{ partial}{ partial theta} A^{-1} &amp;= -A^{-1} frac{ partial A}{ partial theta} A^{-1} frac{ partial}{ partial theta} log{ left( det{A} right)} &amp;= text{tr}{ left(A^{-1} frac{ partial A}{ partial theta} right)} end{align} Symmetric Matrices . In the case that $ Sigma$ is a symmetric matrix, which is the only case we will be interested in here, $ Sigma = Sigma^{t}$, which in turn implies that $A=A^t$, $B = B^{t}$, $D = C^{t}$, the block form of the matrix inverse (also symmetric) can be written as . begin{align} Sigma^{-1} &amp;= begin{pmatrix} widetilde{A} &amp; widetilde{C} widetilde{C}^t &amp; widetilde{B} end{pmatrix} &amp;= begin{pmatrix} E^{-1} &amp; - A^{-1} C F^{-1} - F^{-1} C^{t} A^{-1} &amp; F^{-1} end{pmatrix} &amp;= begin{pmatrix} A^{-1} + A^{-1} C F^{-1} C^{t} A^{-1}&amp; - E^{-1} C B^{-1} -B^{-1} C^{t} E^{-1} &amp; B^{-1} + B^{-1} C^{t} E^{-1} C B^{-1} end{pmatrix} end{align}with . begin{align} widetilde{A}^{-1} = E &amp;= A - C B^{-1} C^{t} widetilde{B}^{-1} = F &amp;= B - C^{t} A^{-1} C end{align} The following form of the relations will be particularly useful for the derivations below begin{align} widetilde{C} &amp;= -E^{-1} C B^{-1} = - widetilde{A} C B^{-1} widetilde{B} &amp;= B^{-1} + B^{-1} C^t E^{-1} C B^{-1} = B^{-1} + B^{-1} C^t widetilde{A} C B^{-1} Longrightarrow widetilde{B} - B^{-1} = B^{-1} C^t widetilde{A}C B^{-1} end{align} . (Multi-variate) Gaussians / GP . Now we can turn our attention to GP. As defined in Rasmussen and Williams, a GP is &quot;a collection of random variables, any finite number of which have a joint Gaussian distribution&quot;. What does this mean? . Previously, for the Bayesian Parameter Estimation problem, we were given some data and a model (which on prior information was assumed to explain the data), and tasked with finding the distribution of the parameters that could explain the data. That is, we wanted to infer or learn the parameters from the data. However, this only works if we know the model. What happens when we don&#39;t posses this information? . This leads us to the much trickier problem of &quot;Non-parametric Bayesian Inference&quot;. Since we don&#39;t have a model to parametrize, we take the function values themselves to be the parameters! So it&#39;s not so much that there are no parameters, it&#39;s just that there is an infinite number of them. Instead of learning the parameters in some model, we will try to learn the function itself from the data. . In the specific case of GP, we assume that the value of the function at each point (e.g., $x(t)$) is a random variable, and that they are all correlated, with a joint Gaussian distribution. Thus, the joint probability distribution for the $x = (x(t_1), x(t_2), ldots x(t_n)) = (x_1, x_2, ldots x_n)$ is given by a multi-variate Gaussian, specified by some mean $ mu$ and (symmetric) covariance matrix $ Sigma$. We express this as . begin{align}x sim mathcal{N}( mu, Sigma) end{align} which is to be interpreted according to begin{align} p(x lvert mu, Sigma) &amp;= frac{1}{ sqrt{ det{ left(2 pi Sigma right)}}} exp{ left[- frac{1}{2} delta x^t Sigma^{-1} delta x right]} qquad left( delta x= x - mu right) int p(x lvert mu, Sigma) , mathrm{d}x &amp;= 1 end{align} . By definition the first and second moments are given by the average and covariance matrix . begin{align} langle x rangle equiv int x p(x lvert mu, Sigma) , mathrm{d}x &amp;= mu left langle delta x_i delta x_j right rangle equiv int delta x_i delta x_j p(x lvert mu, Sigma) , mathrm{d}x &amp;= Sigma_{ij} end{align} Digression on Gaussian Integrals . To compute the marginal and conditional distributions we will need to manipulate the quadratic expressions appearing in the exponential. . In particular, we will need to compute integrals of the form . begin{align} I(A, b, c) &amp;= int exp{ left[- frac{1}{2} x^t A x + x^t b + c right]} , mathrm{d}x end{align}where $A$ is a symmetric symmetric. This integral can be performed easily by completing the square, as follows . begin{align} - frac{1}{2} x^t A x + x^t b &amp;= - frac{1}{2} x^t A x + b^t x &amp;= - frac{1}{2} left[x^t A x - b^t x - x^t b right] &amp;= - frac{1}{2} left[x^t A^t x - b^t A^{-t} A^{t} x - x^t b right] &amp;= - frac{1}{2} left[ left(x^t - b^t A^{-t} right) A^t x - x^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - x^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - left(x - A^{-1} b + A^{-1} b right)^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - left(x - A^{-1} b right)^t b - b^t A^{-t} b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A x - left(x - A^{-1} b right)^t A A^{-1}b right] + frac{1}{2} b^t A^{-1} b &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A left(x - A^{-1} b right) right] + frac{1}{2} b^t A^{-1} b end{align} where we have repeatedly used the fact that a scalar is by definition symmetric, so $ alpha= alpha^t$, $x^t y = y^t x$, $x^t A y = y^t A^t x$, and so on ($ alpha$ a scalar, $x$ and $y$ vectors, and $A$ a square matrix). . begin{align} I(A, b, c)&amp;= int exp{ left[- frac{1}{2}x^t A x + x^t b + c right] mathrm{d}x} &amp;= exp{ left[ frac{1}{2}b^t A^{-1} b + c right]} underbrace{ int exp{ left[- frac{1}{2} left(x - A^{-1}b right)^t A left(x - A^{-1} b right) right]} mathrm{d}x}_{ equiv sqrt{ det{ left(2 pi A^{-1} right)}}} &amp;= sqrt{ det{ left(2 pi A^{-1} right)}} exp{ left[ frac{1}{2} b^t A^{-1} b + c right]} &amp;= sqrt{ frac{(2 pi)^{n}}{ det{A}}} exp{ left[ frac{1}{2} b^t A^{-1} b + c right]} end{align} Marginalization . Now lets consider partitioning our set of points $x$ in two, $x_A$ and $x_B$, which could represent the (known) training data and (unknown) test data, respectively. The joint distribution, is given exactly by the expression above, but we can rewrite it in block form to highlight the contribution of the $x_A$ and $x_B$ points . begin{align} p(x lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^n det{ Sigma}}} exp{ left[- frac{1}{2} delta x^t Sigma^{-1} delta x right]} , , qquad Sigma = begin{pmatrix} Sigma_{AA} &amp; Sigma_{AB} Sigma_{AB}^t &amp; Sigma_{BB} end{pmatrix} &amp;= frac{1}{ sqrt{(2 pi)^n det{ Sigma}}} exp{ left[- frac{1}{2} begin{pmatrix} delta x_A delta x_B end{pmatrix}^t begin{pmatrix} widetilde{ Sigma}_{AA} &amp; widetilde{ Sigma}_{AB} widetilde{ Sigma}_{AB}^t &amp; widetilde{ Sigma}_{BB} end{pmatrix} begin{pmatrix} delta x_A delta x_B end{pmatrix} right]} end{align} Where we used the properties of block matrices to rewrite the inverse of $ Sigma$ into block form. . Given this joint distribution for $x_A$ and $x_B$, what can we say about the distribution for $x_B$, regardless of $x_A$? . By definition, we simply marginalize over $x_A$ begin{align} p(x_B lvert mu, Sigma) &amp;= int p(x_A,x_B lvert mu, Sigma) mathrm{d}x_A end{align} . To evaluate this integral, lets rewrite the terms appearing in the exponent, trying to separate out the $x_A$ and $x_B$ contributions . begin{align} delta x^t Sigma^{-1} delta x &amp;= delta x_A^t left( widetilde{ Sigma}_{AA} delta x_A + widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t left( widetilde{ Sigma}^t_{AB} delta x_A + widetilde{ Sigma}_{BB} delta x_B right) &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} delta x_A + 2 delta x_A^t widetilde{ Sigma}_{AB} delta x_B bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B end{align} Again, using the properties of these block matrices, the determinant in the normalization factor can be conveniently written as . begin{align} det{ Sigma} = det{ Sigma_{AA}} cdot det{ widetilde{ Sigma}_{BB}^{-1}} = det{ Sigma_{BB}} cdot det{ widetilde{ Sigma}_{AA}^{-1}} end{align} Putting all this together, the marginal distribution for $x_B$ takes the form . begin{align} p(x_B lvert mu, Sigma) &amp;= left( frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} underbrace{ int exp{ left[- frac{1}{2} delta x_A^t widetilde{ Sigma}_{AA} delta x_A - delta x_A^t widetilde{ Sigma}_{AB} delta x_B right]} mathrm{d}x_A}_{I( widetilde{ Sigma}_{AA}, - widetilde{ Sigma}_{AB} delta x_B, 0)} right) times left( frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B widetilde{ Sigma}_{BB} delta x_B right]} right) &amp;= left( frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} times sqrt{ frac{(2 pi)^{n_A}}{ det widetilde{ Sigma}_{AA}}} exp{ left[ frac{1}{2} left( widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA}^{-1} left( widetilde{ Sigma}_{AB} delta x_B right) right]} right) times left( frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B widetilde{ Sigma}_{BB} delta x_B right]} right) &amp;= frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^t_B left( widetilde{ Sigma}_{BB}- widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} right) delta x_B right]} end{align} This is almost in the form of a multi-variate Gaussian. We can further simplify it by using the properties of block matrices listed above, since . begin{align} widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} &amp;= widetilde{C}^t widetilde{A}^{-1} widetilde{C} &amp;= left(-B^{-1} C^{t} widetilde{A} right) widetilde{A}^{-1} left(- widetilde{A} C B^{-1} right) &amp;= B^{-1}C^t widetilde{A}C B^{-1} &amp; equiv widetilde{B} - B^{-1} &amp;= widetilde{ Sigma}_{BB} - Sigma_{BB}^{-1} end{align}where we have used the fact that $ widetilde{C}= - widetilde{A}C B^{-1}$. . We thus arrive at the result that the marginal distribution for $x_B$ is also Gaussian, with average $ mu_B$ and covariance matrix $ Sigma_{BB}$. . We can simply read off the distribution for $x_B$ from the original joint distribution! begin{align} p(x_B lvert mu, Sigma) = p(x_B lvert mu_B, Sigma_{BB}) &amp;= frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B Sigma^{-1}_{BB} delta x_B right]} x_B &amp; sim mathcal{N}( mu_B, Sigma_{BB}) end{align} . This is the meaning of the quoted text which says that &quot;any finite number of which have a joint Gaussian distribution&quot;. . Conditioning / GP Predictions . A more useful result comes from considering the conditional distribution. Say we have already measured $x_B$, this would be our training data set, what can we say about the function values $x_A$ at other points? . From Bayes&#39; theorem, this conditional distribution is simply given by . begin{align} p(x_A lvert x_B, mu, Sigma) &amp;= frac{p(x_A, x_B lvert mu, Sigma)}{p(x_B lvert mu, Sigma)} end{align}After some simple manipulations, we will see that this distribution again has the form of a multi-variate Gaussian, although the average and covariance matrices will be a bit more complicated. . To start, let us rewrite the exponent appearing in the numerator, in order to cancel out the exponent in the denominator. begin{align} delta x^t Sigma^{-1} delta x &amp;= delta x_A^t left( widetilde{ Sigma}_{AA} delta x_A+ widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t left( widetilde{ Sigma}^t_{AB} delta x_A + widetilde{ Sigma}_{BB} delta x_B right) &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} delta x_A + 2 delta x_A^t widetilde{ Sigma}_{AB} delta x_B bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t widetilde{ Sigma}_{AB}^t delta x_A bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-t} widetilde{ Sigma}_{AA}^t left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B - widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) bigg] - delta x_B^t underbrace{ bigg[ widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-t} widetilde{ Sigma}_{AB} - widetilde{ Sigma}_{BB} bigg]}_{- Sigma_{BB}^{-1}} delta x_B end{align} . From which we see that the second term on the right hand side will exactly cancel the exponential in the denominator. . Now, lets consider the ratio of normalization constants . begin{align} sqrt{ frac{(2 pi)^{n_B} det{ Sigma_{BB}}}{(2 pi)^{n_A + n_B} det{ Sigma}}} &amp;= sqrt{ frac{ det{ Sigma_{BB}}}{(2 pi)^{n_A} det{ Sigma_{BB}} times det{ widetilde{ Sigma}_{AA}^{-1}}}} &amp;= frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} end{align} As promised, the conditional distribution for $x_A$ (conditioned on $x_B$) is another Gaussian! . begin{align} p(x_A lvert x_B, mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} exp{ left[- frac{1}{2} left(x_A - mu_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA} left(x_A - mu_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) right]} end{align} begin{align} x_A lvert x_B &amp; sim mathcal{N} left( mu_A - widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B, widetilde{ Sigma}_{AA}^{-1} right) end{align} Since it&#39;s more convenient to express all quantities in terms of the block matrices of $ Sigma$, we can rewrite the average and covariance using the following relationships . begin{align} widetilde{ Sigma}_{AA}^{-1} &amp;= Sigma_{AA} - Sigma_{AB} Sigma_{BB}^{-1} Sigma_{AB}^t end{align} begin{align} widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} &amp; equiv widetilde{A}^{-1} widetilde{C} &amp;= - widetilde{A}^{-1} widetilde{A} C B^{-1} &amp;= - C B^{-1} &amp;= - Sigma_{AB} Sigma_{BB}^{-1} end{align} From which we obtain the equivalent expression . begin{align} x_A lvert x_B sim mathcal{N} left( mu_A + Sigma_{AB} Sigma_{BB}^{-1} delta x_B, Sigma_{AA}- Sigma_{AB} Sigma_{BB}^{-1} Sigma^{t}_{AB} right) end{align} This is it! This is (almost) everything we need to do some Machine Learning with GP. This is what we need to evaluate for predicting the values of $x_A$ given noise-free observations of $x_B$. . Noisy Predictions . What about the case when we have noisy measurements for $x_B$? Assume that what we measure is in fact $y = x + epsilon$, with $ epsilon$ additive independent Gaussian noise (zero mean and variance $ sigma^2$), such that $ langle y rangle = langle x rangle$. . In this case, the covariance matrix is given as . begin{align} left langle delta y_i delta y_j right rangle &amp;= left langle ( delta x_i + epsilon_i) ( delta x_j + epsilon_j) right rangle &amp;= underbrace{ left langle delta x_i delta x_j right rangle}_{ Sigma(x_i,x_j)} + underbrace{ langle delta x_i epsilon_j rangle + langle delta x_j epsilon_i rangle}_{0} + underbrace{ langle epsilon_i epsilon_j rangle}_{ sigma^2 delta_{ij}} &amp;= Sigma(x_i, x_j) + sigma_i^2 delta_{ij} end{align} since $x sim mathcal{N}( mu, Sigma)$ and the noise is independent. This shows that $y sim mathcal{N}( mu, Sigma + sigma^2 I)$. . Thus, we can define a joint GP for $y_B$ (the noisy measurements) and $x_A$ (test points), and the conditional distribution has the same form as above, except that we replace $ Sigma_{BB}$ with $ Sigma_{BB} + sigma_B^2 I_BB$ (with $I$ the unit matrix). . begin{align} x_A lvert y_B sim mathcal{N} left( mu_A + Sigma_{AB} left( Sigma_{BB} + sigma_B^2 I_{BB} right)^{-1} delta y_B, Sigma_{AA} - Sigma_{AB} left( Sigma_{BB} + sigma_B^2 I_{BB} right)^{-1} Sigma_{AB}^t right) end{align} There are two possibilities, either we know $ sigma_B$ or we don&#39;t. In the former case, we proceed exactly as before, and in the latter we treat it as an additional hyper-parameter (together with the Kernel hyper-paramters). Note that here, $ sigma_B$ is in principle different for each of the $y_B$, but when it is an unknown hyper-parameter it is easiest to consider constant $ sigma_B$ for all points. . The above expressions give predictions for the noiseless function $x$. If we want predictions for the noisy function $y$, we would simply add $ sigma_A^2 I_{AA}$ to the covariance matrix. . Linear Combinations . One of the benefits of using GP lies in their linearity. If $x$ and $y$ are two GP, then any linear combination of them is also a GP. . In particular, begin{align} x&amp; sim mathcal{N}( mu_x, Sigma_x) y&amp; sim mathcal{N}( mu_y, Sigma_y) A x + B y + c &amp; sim mathcal{N}(A mu_x + B mu_y + c, A Sigma_x A^t + B Sigma_y B^t) end{align} . Unfortunately, products of GP do not result in GP... . A (simple) implementation . For improved numerical stability and computational cost, it is recommended not to compute the matrix inverses appearing in the expressions for the averages and covariances of the conditional distribution. A better approach, which is still quite expensive, is to use the Cholesky decomposition. . If $A$ is a positive definite matrix (i.e., a covariance matrix $ Sigma$), $A$ can be written as the product of a lower-triangular matrix $L$ and its transpose begin{align} A&amp;= L L^t end{align} such that expressions of the form $A^{-1} b = x$, for known $A$ and $b$ can be computed as begin{align} (L L^t)^{-1} b &amp;= x L^{-t} L^{-1} b &amp;= x L^t backslash left(L backslash b right) &amp;= x end{align} . Where we have adopted the backslash notation used by Rasmussen and Williams begin{align} A x&amp;= b x &amp;= A^{-1} b x &amp; equiv A backslash b end{align} where it is assumed that we know $A$, but not necessarily $A^{-1}$, and $b$. This notation is useful to emphasize the fact that we don&#39;t want to calculate $A^{-1}$ explicitly, we just need its product with some vector $b$ (i.e., to solve for $x$). . Using this Cholesky decomposition, a sandwich product of the form $b^{t} A^{-1} c$ would be expressed as begin{align} b^t A^{-1}c &amp;= b^{t}L^{-t} L^{-1} c &amp;= (L^{-1} b)^t (L^{-1} c) &amp;= w^t v end{align} with $v = L backslash c$ and $w = L backslash b$. . In the same way, we can evaluate more complicated expression, such as $A = C^t B^{-1} C$, without directly computing $B^{-1}$. Let $C = (c_1, c_2, ldots, c_n)$, where $c_i$ are the column-vector components of $C$. We have . begin{align} A &amp;= (c_1, c_2, ldots, c_n)^t B^{-1} (c_1, c_2, ldots c_n) &amp;= begin{pmatrix}c_1 c_2 vdots c_n end{pmatrix} begin{pmatrix} B^{-1} c_1 &amp; B^{-1} c_2 &amp; ldots &amp; B^{-1} c_n end{pmatrix} &amp;= begin{pmatrix} c_1^t B^{-1}c_1 &amp; ldots &amp; c_1^t B^{-1} c_n vdots &amp; ddots &amp; vdots c_n^t B^{-1}c_1 &amp; ldots &amp; c_n^t B^{-1} c_n end{pmatrix} end{align}where each term is computed using the expression derived above begin{align} (A)_{ij} &amp;= c_i^t B^{-1} c_j equiv (L backslash c_i)^t (L backslash c_j) end{align} with $L$ now the Cholesky decomposition of $B=LL^t$. . Evaluating terms like $ log{ det{A}}$ is also considerably simplified . begin{align} log det{A} &amp;= log det{LL^t} &amp;= log left( det{L} cdot det{L^t} right) &amp;= 2 log det{L} = 2 sum_i log L_{ii} end{align} Sampling . To sample random functions from a GP, $f sim mathcal{N}( mu, K)$, with mean $ mu$ and covariance $K$, we use the following procedure (see Rasmusen and Williams, pg. 201). Let $n$ be the number of points $x$ at which we want to sample the functions $f$ . Compute the Cholesky decomposition of $K = L L^t$ | Generate $n$ independent random numbers from a uniform Gaussian distribution, $g sim mathcal{N}(0, 1)$ | Compute $f = mu + L g$ | The resulting $f$ have the desired average and covariance begin{align} langle f rangle &amp;= langle{ mu + L g} rangle = mu + L langle g rangle equiv mu langle (f - mu) (f - mu)^t rangle &amp;= langle L g g^t L^t rangle = L langle g g^t rangle L^t = L L^t equiv K end{align} . To improve stability of the Cholesky decomposition, it is suggested to add a small multiple of the identity to $K$, i.e., $K longrightarrow K + epsilon I$. . However, as hinted at above, using the Cholesky decomposition is not the &#39;&#39;best&#39;&#39; way to evaluate the GP. The reason for this is its $ mathcal{O}(n^3)$ complexity. Fortunately, in recent years advanced matrix-matrix algorithms have been developed that allow for exact calculations even on millions of points! . Excercise . We include the usual boiler-plate code. Note that we are using jax extensively (this rules out running on windows!) . import jax.numpy as np import pandas as pd import numpy as onp import pymc3 as pm import theano as th import theano.tensor as tt import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.patheffects as PathEffects from scipy import optimize from scipy.interpolate import interp1d from jax import grad, jit, vmap, jacfwd, jacrev, random from jax.numpy.lax_numpy import _wraps from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from functools import partial,reduce from pymc3.gp.util import plot_gp_dist mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} fancycolors = [mpl.colors.to_hex(c) for c in [[0.6, 0.6, 0.6],[0.7, 0.3, 1],[0.3, 0.7, 1],[0.2, 0.9, 0.9], [0.3, 1, 0.7],[0.7, 1, 0.3],[0.9, 0.9, 0.2],[1, 0.7, 0.3], [1, 0.3, 0.7], [0.9, 0.2, 0.9],[1.0, 1.0, 1.0]]] threecolors = [mpl.colors.to_hex(c) for c in [[0.1, 0.15, 0.4],[1, 0.2, 0.25],[1.0, 0.775, 0.375]]] fourcolors = [mpl.colors.to_hex(c) for c in [[0.9, 0.6, 0.3],[0.9, 0.4, 0.45],[0.5, 0.65, 0.75],[0.42, 0.42, 0.75]]] def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) def plot_quantiles(ax, xdata, ydata,*, axis, quantiles, colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=axis) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_sig2= quantiles_sig[1:-1] quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . First, let&#39;s define some of the most common kernel functions . begin{align} k(x_1,x_2) &amp;= eta^2 x_1 x_2 &amp;( text{Linear}) k(x_1,x_2) &amp;= eta^2 delta_{x_1,x_2} &amp;( text{White Noise}) k(x_1,x_2) &amp;= eta^2 exp{ left[- frac{ lvert x_1-x_2 rvert}{2l} right]} &amp;( text{Ornstein Uhlenbeck}) k(x_1,x_2) &amp;= eta^2 left(1 + frac{ sqrt{3 (x_1 - x_2)^2}}{l} right) exp{ left[- frac{ sqrt{3(x_1-x_2)^2}}{l} right]} &amp;( text{Matern} ,3/2) k(x_1,x_2) &amp;= eta^2 left(1 + frac{ sqrt{5 (x_1 - x_2)^2}}{l} + frac{5(x_1 - x_2)^2}{3 l^2} right) exp{ left[- frac{ sqrt{5(x_1 - x_2)^2}}{l} right]}&amp; ( text{Matern 5/2}) k(x_1,x_2) &amp;= eta^2 exp{ left[- frac{(x_1-x_2)^2}{2l^2} right]} &amp;( text{Square Exponential}) end{align} Note: Jax has yet to add support for general outer operations, but we can easily create a jax version of outer subtract by modyfing the corresponding code for outer. . @_wraps(onp.subtract.outer) def subtract_outer(a,b,out=None): if out: raise NotImplementedError(&quot;The &#39;out&#39; argument to outer is not supported&quot;) a, b = np.lax_numpy._promote_dtypes(a, b) return np.ravel(a)[:,None] - np.ravel(b) def K_Linear(x1, x2, l=1.0): return np.outer(x1, x2) def K_WhiteNoise(x1, x2, l=1.0): r = np.abs(subtract_outer(x1,x2)) return np.where(r == 0, 1.0, 0.0) def K_OrnsteinUhlenbeck(x1, x2, l=1.0): return np.exp(-0.5*np.abs(subtract_outer(x1,x2))/l) def K_Matern32(x1,x2, l=1.0): r = np.sqrt(3)*np.abs(subtract_outer(x1,x2))/l return (1 + r)*np.exp(-r) def K_Matern52(x1,x2, l=1.0): r = np.sqrt(5)*np.abs(subtract_outer(x1,x2))/l return (1 + r + r**2/3)*np.exp(-r) def K_SquareExp(x1, x2, l=1.0): return np.exp(-0.5*(subtract_outer(x1,x2)/l)**2) def Cholesky(K, ϵ): try: L = np.linalg.cholesky(K + np.diag(ϵ)) return L except: print(&quot;Unexpected Error&quot;) raise . 1. Drawing functions from a GP prior . Grid the functions $f(x)$ on a grid of $x$ values in the range $[-10,10]$, with a grid spacing of $ Delta x = 0.2$ ($n=101$ grid points). Set $ mu = 0$ and $ eta = l = 1$. . Generate $n times 4$ independent normally distributed random numbers | Use this set of random numbers to sample $4$ random functions $f sim mathcal{N}(0, K)$ (over the $n$ grid points) using the procedure outlined above, for each of the kernels $K$ we have defined. | . You should obtain something similar to the plot below . #collapse fig, axes = plt.subplots(figsize=(18,9), ncols=3, nrows=2, sharex=True) Ks = [K_Linear, K_WhiteNoise, K_OrnsteinUhlenbeck, K_Matern32, K_Matern52, K_SquareExp] Kl = [&#39;Linear&#39;, &#39;White Noise&#39;, &#39;Ornstein Uhlenbeck&#39;, &#39;Matern 3/2&#39;, &#39;Matern 5/2&#39;, &#39;Square Exp.&#39;] x = np.linspace(-10, 10, num=101) # Δx = 0.2 μ = np.zeros_like(x) # zero average ns = 4 key = random.PRNGKey(0) key,subkey = random.split(key) gs = random.normal(subkey, shape=μ.shape + (ns,)) for ax, K, lbl in zip(axes.flatten(), Ks, Kl): L = Cholesky(K(x, x), np.ones_like(x)*1e-6) fs= μ[:,None] + np.einsum(&#39;ij,j...-&gt;i...&#39;, L, gs) addtxt(ax, 0.1, 0.9, lbl, fs=18) for i in range(ns): ax.plot(x, fs[:,i], color=fourcolors[i], alpha=0.9) ax.set_ylim(-2,2) ax.set_xlim(x[0],x[-1]) for row in axes: row[0].set_ylabel(r&#39;$f$&#39;) for ax in row: ax.set_xlabel(r&#39;$x$&#39;) fig.tight_layout() . . Since we reused the random numbers, the corresponding functions obtained from the Ornstein-Uhlenbeck, Matern and Square-Exponential Kernels show very similar behavior and allow for easy comparison. Below we plot the orange curves for each of the kernels. . #collapse fig, ax = plt.subplots(figsize=(16,9)) for i in range(2,6): K,lbl = Ks[i], Kl[i] L = Cholesky(K(x, x), np.ones_like(x)*1.0e-6) fs= μ[:] + np.einsum(&#39;ij,j-&gt;i&#39;, L, gs[:,0]) ax.plot(x, fs, color=fancycolors[2*i-2], label=lbl, alpha=0.8) ax.set_xlabel(r&#39;$x$&#39;, fontsize=22) ax.set_ylabel(r&#39;$f$&#39;, fontsize=22) ax.legend() fig.tight_layout() . . 2. Verify the statistical properties of the GP prior . Now, let us verify the properties of the GP. Use the Square-Exponential kernel, with $ eta = 1.5$, $l=0.75$, $ mu(x) = 0$, and use the same $x$ grid as above. . Draw $n_s = 500$ random functions from this GP prior and plot them. You should obtain something similar to the following graph (here $3$ of the $500$ curves have been colorized for visualization purposes). . #collapse η,l = 1.5, 0.75 K = K_SquareExp(x, x, l) L = η*Cholesky(K, np.ones_like(x)*1e-6) key,subkey = random.split(key) ns = 500 fs = μ[:,None] + np.einsum(&#39;ij,j...-&gt;i...&#39;, L, random.normal(subkey, shape=μ.shape + (ns,))) . . #collapse fig, ax = plt.subplots(figsize=(16,9)) for i in range(ns): ax.plot(x,fs[:,i],lw=1,color=color[&#39;superfine&#39;], alpha=0.2) for i,c in enumerate([color[&#39;light&#39;], color[&#39;mid&#39;], color[&#39;dark&#39;]]): ax.plot(x,fs[:,i],color=c) ax.set_xlim(x[0],x[-1]) ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(f&#39;$f$&#39;) plt.show() . . Now, compute the single-point average and variance (over the ensemble of functions), i.e., . begin{align} langle f(x) rangle = frac{1}{n_s} sum_{i=1}^{n_s} f^{(i)}(x) equiv mu(x) langle delta f(x) ^2 rangle = frac{1}{n_s} sum_{i=1}^{n_s} left(f^{(i)}(x) - mu(x) right)^2 equiv eta^2 end{align}with $f^{(i)}$ the $i$-th function from the sample. . As a bonus, compute the quantile intervals $(0.1587, 0.8413)$, $(.0228, 0.9772)$, and $(.0014, 0.9986)$, by definition these should correspond to $ pm eta$, $ pm 2 eta$, and $ pm 3 eta$ intervals. Below, we show a plot of the average $ langle f(x) rangle$, together with the quantiles (dashed lines show the theoretical values). . #collapse fig, ax = plt.subplots(figsize=(16,9)) f_avg = np.average(fs, axis=-1) plot_quantiles(ax, x, fs-f_avg[:,None], axis=-1, quantiles=quantiles_sig, colors=colors[::2]) for i in range(1,4): ax.hlines([-i*η,i*η], x[0], x[-1], ls=&#39;--&#39;, color=color[&#39;superfine&#39;]) ax.set_xlim(x[0],x[-1]) ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f$&#39;) fig.tight_layout() plt.show() . . Now, let&#39;s compute the covariance of the function values at different $x$ points, and compare with the theoretical expressions: begin{align} delta f(x_i) delta f(x_j) &amp;= a(x_i, x_j) langle a rangle &amp; equiv k(x_1, x_2) &amp;= eta^2 exp{ left[- frac{(x_1-x_2)^2}{2 l^2} right]} langle a^2 rangle - langle a rangle^2 &amp;= langle delta f(x_i)^2 delta f(x_j)^2 rangle - langle delta f(x_i) delta f(x_j) rangle^2 &amp; equiv k(x_1, x_1) k(x_2, x_2) + k(x_1,x_2)^2 &amp;= eta^{4} left(1 + exp{ left[- frac{(x_1-x_2)^2}{2l^2} right]} right) end{align} . See wikipedia for a list of higher-order moments of multi-variate Gaussians. . #collapse def correlate(f): nx,ns = f.shape δf = np.array(f - np.average(f, axis=-1)[:,None]) avg,sig = [], [] for dx in range(nx): xend = -dx if dx &gt; 0 else nx cor = δf[:xend]*δf[dx:] avg.append(np.average(cor)) sig.append(np.var(cor)) return np.array(avg), np.array(sig) avg,sig = correlate(fs) . . The plot below shows the average two-point correlation $a= delta f(x_i) delta f(x_j)$, as a function of the separation $ lvert{x_i-x_j} lvert$. The filled region corresponds to the $ pm sigma$ interval. . #collapse fig, ax = plt.subplots(figsize=(16,9)) dx = x-x[0] # sampled data ax.plot(dx/l, avg/η**2, color=color[&#39;mid&#39;], alpha=0.8, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=12, label=&#39;Numerical&#39;) ax.fill_between(dx/l, (avg-np.sqrt(sig))/η**2, (avg+np.sqrt(sig))/η**2, color=color[&#39;dark_trans&#39;]) # theory kx = η**2*np.exp(-0.5*(dx/l)**2) ax.plot(dx/l, kx/η**2, color=color[&#39;dark&#39;], ls=&#39;-&#39;, lw=3, zorder=10, label=&#39;Theory&#39;) ax.plot(dx/l, (kx + np.sqrt(η**4 + kx**2))/η**2, color=color[&#39;dark&#39;], ls=&#39;--&#39;, lw=4) ax.plot(dx/l, (kx - np.sqrt(η**4 + kx**2))/η**2, color=color[&#39;dark&#39;], ls=&#39;--&#39;, lw=4) ax.set_xlim(0,4) ax.set_xlabel(r&#39;$(x_1-x_2)/l$&#39;, fontsize=22) ax.set_ylabel(r&#39;$ left langle delta f(x_1) ; delta f(x_2) right rangle/η^2$&#39;, fontsize=22) ax.legend(fontsize=22) fig.tight_layout() plt.show() . . 3. GP for Machine Learning . Assume you have the following $20$ training points . #collapse x = np.linspace(0,1,num=100) def hidden_function(x): return 2.0 + np.exp(-np.abs(x)/0.2)*np.sin(2*np.pi/0.25*x) onp.random.seed(13579) samples = onp.sort(onp.random.choice(len(x), replace=False, size=20)) x_samples = x[samples] f_samples = hidden_function(x_samples) training_data = pd.DataFrame(data = {&#39;x&#39;:x_samples, &#39;f&#39;:f_samples}) training_data . . x f . 0 0.000000 | 2.000000 | . 1 0.040404 | 2.694292 | . 2 0.050505 | 2.741803 | . 3 0.212121 | 1.717956 | . 4 0.222222 | 1.788399 | . 5 0.313131 | 2.208924 | . 6 0.333333 | 2.163571 | . 7 0.373737 | 2.004896 | . 8 0.393939 | 1.936078 | . 9 0.454545 | 1.906280 | . 10 0.565657 | 2.058928 | . 11 0.656566 | 1.973258 | . 12 0.666667 | 1.969105 | . 13 0.737374 | 1.992183 | . 14 0.747475 | 1.998489 | . 15 0.797980 | 2.017283 | . 16 0.848485 | 2.008885 | . 17 0.919192 | 1.990957 | . 18 0.929293 | 1.990608 | . 19 0.959596 | 1.992993 | . #collapse fig, ax = plt.subplots(figsize=(16,9)) #ax.plot(x, hidden_function(x), color=color[&#39;mid&#39;]) ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, color=color[&#39;dark&#39;], ms=14, label=&#39;Training points&#39;) ax.set_xlabel(r&#39;$x$&#39;, fontsize=22) ax.set_ylabel(r&#39;$f$&#39;, fontsize=22) ax.legend(fontsize=22) fig.tight_layout() . . Let us use GP to learn the function, and make predictions for its value at new test points $x_ star$, i.e. we need to compute $p(f_ star lvert x_ star, x, f)$. . Assume that on prior information you now that $ mu(x) = 2.0$. . Inferring the hyper-parameters . The first thing we need to do is to infer the kernel hyper-parameters. Assuming a square exponential kernel, we need the amplitude and length-scale of the correlations, $ eta$ and $l$, respectively. . This is a simple parameter estimation problem. . Let $ Theta = ( eta, l)$, and $D$ the measured data, the posterior distribution we are interested in is then . begin{align} P( Theta lvert D, I) propto mathcal{L}( Theta) Pi( Theta) end{align}with $L( Theta)$ the likelihood of observing the data (given $ Theta$), and $ Pi( Theta)$ the prior for $ Theta$. As usual, it&#39;s best to work with the logarithm of this quantity. . Assuming a GP likelihood, $ mathcal{L} sim mathcal{N}( mu, K_ Theta)$, we have . begin{align} L = ln{ mathcal{L}( Theta)} &amp;= - frac{1}{2} delta f^t K^{-1} delta f - frac{1}{2} ln{ left[(2 pi)^n det{K} right]} &amp;= - frac{1}{2} left[ delta f^t K^{-1} delta f + n ln{2 pi} + ln{ det{K}} right] - ln{ mathcal{L}( Theta)}&amp;= frac{1}{2} left[ delta f^t K^{-1} delta f + ln{ det{K}} + n ln{2 pi} right] end{align}with $ delta f = f - mu$ and $n$ the number of training points. . For the prior $ Pi( Theta)$, we assume $ eta$ and $l$ are independent, and since both are scale parameters, we use Jeffrjeys prior, such that begin{align} Pi( Theta)&amp;= Pi( eta) Pi(l) propto frac{1}{ eta} times frac{1}{l} - ln{ Pi( Theta)}&amp;= ln{ eta} + ln{l} + text{const} end{align} . Then, we can either settle for a point-wise solution, maximizing the log-likelihood to obtain the &#39;&#39;best&#39;&#39; estimate for the hyper-parameters, or we can go full Bayesian and estimate the posterior with some MC simulations. In any case it is useful to compute the gradients of this log-likelihood (MC packages like PYMC do this automatically, and using jax we don&#39;t have to worry about doing it by hand). . Maximum Aposteriori Estimate . begin{align} - partial_ eta ln{ Pi( eta)} &amp;= frac{1}{ eta} - partial_l ln{ Pi(l)} &amp;= frac{1}{l} - partial_ theta ln{ mathcal{L}( Theta)} &amp;= frac{1}{2 theta} left[ mathrm{Tr}{ left(K^{-1} Q_ theta right)} - delta f^t K^{-1}Q_ theta K^{-1} delta f right] end{align}with $ theta$ one of the kernel parameters, $ theta_1 = eta$ or $ theta_2 = l$, and $Q_ theta$ defined as . begin{align} Q_ theta(x_1,x_2)&amp;= theta partial_ theta K(x_1,x_2) Q_{ eta}(x_1,x_2)&amp;= 2 K(x_1,x_2) Q_{l}(x_1,x_1) &amp;= frac{(x_1 - x_2)^2}{l^2} K(x_1,x_2) end{align} Using the Cholesky decomposition of $K = L L^t$, we can evaluate each of the terms appearing in these expressions as follows begin{align} delta f^t K^{-1} delta f &amp;= left(L backslash delta f right)^t left(L backslash delta f right) = left lVert L backslash delta f right rVert ^2 ln det{K} &amp;= 2 sum_i ln{L_{ii}} text{Tr}{ left(K^{-1} Q right)} &amp;= text{Tr}{ left(L^{-1} Q L^{-t} right)} delta f^t K^{-1} Q K^{-1} delta f &amp;= left(L backslash delta f right)^t L^{-1} Q L^{-t} (L backslash delta f) L^{-1} Q L^{-t} &amp;= begin{pmatrix}L backslash r_1 &amp; cdots L backslash r_n end{pmatrix} begin{pmatrix}r_1 &amp; cdots &amp; r_n end{pmatrix} &amp; equiv Q L^{-t} = left(L^{-1} Q right)^t = begin{pmatrix}L backslash q_1 &amp; cdots L backslash q_n end{pmatrix}^t end{align} . with $q_i$ the columns of the symmetric $Q$ matrix. . #collapse K = K_SquareExp @jit def logp(p, μ, x, f, ϵ): η,l = np.exp(p) L = Cholesky(η**2*K(x, x, l), ϵ) # K = L L^t v = np.linalg.solve(L, f - μ) # v = (L δf) return (0.5*np.dot(v,v) + np.sum(np.log(np.diag(L)))) + np.log(η) + np.log(l) gradlogp = jit(grad(logp)) @jit def gradlogp_SE(p, μ, x, f, ϵ): def partial(Q, L, v): R = np.transpose(np.array([np.linalg.solve(L, qi) for qi in Q])) # = R^t = L | Q LQL = np.array([np.linalg.solve(L, ri) for ri in R]) # = L^{-1} Q L^{-t} return 1 + 0.5*(np.trace(LQL) - np.transpose(v) @ LQL @ v) η,l = np.exp(p) dx2 = (subtract_outer(x,x)/l)**2 Q = η**2*np.exp(-0.5*dx2) L = np.linalg.cholesky(Q+np.diag(ϵ)) # K + ϵI = L L^t v = np.linalg.solve(L, f - μ) # v = (L | δf) #deta Q = 2*Q dη = partial(Q, L, v) #dl Q = dx2*Q/2 dl = partial(Q, L, v) return np.array([dη, dl]) . . def hyperparams(init,μ,x,f,noise): opt0 = optimize.minimize(logp, init, args=(μ,x,f,noise), method=&#39;Nelder-Mead&#39;, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) opt1 = optimize.minimize(logp, opt0[&#39;x&#39;], args=(μ,x,f,noise), method=&#39;BFGS&#39;, jac = gradlogp, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) return [opt0, opt1] . avg_samples = np.ones_like(x_samples)*2.0 noise_samples = np.ones_like(x_samples)*1.0e-6 opts = hyperparams(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples) for o in opts: p = np.exp(o[&#39;x&#39;]) print(f&#39; t loss = {o[&quot;fun&quot;]:12.6e}, niter = {o[&quot;nit&quot;]:5d}, Converged = {o[&quot;success&quot;]:6b} : {o[&quot;message&quot;]}&#39;) print(f&#39; t t η = {p[0]:6.3f}, l = {p[1]:6.3f}&#39;) ηopt,lopt = p . loss = -4.343454e+01, niter = 84, Converged = 1 : Optimization terminated successfully. η = 1.229, l = 0.116 loss = -4.343454e+01, niter = 3, Converged = 1 : Optimization terminated successfully. η = 1.229, l = 0.116 . Just to be sure, we can compare our hand-coded gradient with the automatic differentiated version provided by jax. . np.allclose(gradlogp(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples), gradlogp_SE(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples)) . DeviceArray(True, dtype=bool) . Take home message : always use jax! . Predicting test values . Now, use the conditional GP distribution (with the optimal hyperparameters just obtained) to predict the values of the function for test points $x_ star in [-1.5, 1.5]$, with $ Delta x_ star = 0.015$, for $n_ star = 201$ points. . Recall the definition of the conditional GP begin{align} f_ star lvert f &amp; sim mathcal{N} left( mu_ star + K_{ star}^t K^{-1} delta f, K_{ star star} - K_{ star}^t K^{-1} K_{ star} right) K &amp;= K(X, X) K_ star &amp;= K(X, X_ star) K_{ star star} &amp;= K(X_ star, X_ star) end{align} where $X=(x_1,x_2, cdots, x_n)$ and $X_{ star} = (x_{1 star}, x_{2 star}, cdots, x_{n_ star})$, are the design matrices for the training and test points. . #collapse def GPpost(x_new, x_obs, f_obs, noise_obs, μ, K): K_obs_obs = K(x_obs, x_obs) + np.diag(noise_obs) # K K_new_new = K(x_new, x_new) # K** K_new_obs = K(x_new, x_obs) # (K*)^t L = np.linalg.cholesky(K_obs_obs) # α = K δ f = L^t (L | δ f) α = np.linalg.solve(L.transpose(), np.linalg.solve(L, f_obs - μ(x_obs))) # μpost = μ(x*) + (K*)^t K δy(x) = μ(x*) + (K*)^t . α μpost = μ(x_new) + np.dot(K_new_obs, α) # Kpost = K** - (K*)^t K | K* # = K** - W # W_ij = v_i . v_j # v_i = (L | c_i) ; c_i the i-th column of K*, i-th row of (K*)^t V = np.array([np.linalg.solve(L, c) for c in K_new_obs]) # V = [v_1, v_2, ... ]^t Kpost = K_new_new - np.einsum(&#39;ik,jk-&gt;ij&#39;,V,V) return μpost, Kpost . . x = np.linspace(-1.5, 1.5, num=201) μpost, Kpost = GPpost(x, x_samples, f_samples, noise_samples, lambda x: 2.0, lambda x1,x2: ηopt**2*K(x1,x2,lopt)) σpost = np.sqrt(np.diag(Kpost)) . finterp = interp1d(x_samples, f_samples, kind=&#39;cubic&#39;) xinterp = np.linspace(x_samples[0], x_samples[-1], num=100) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;, ls=&#39;--&#39;) ax.legend(fontsize=22) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) . (1.0, 3.0) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;) ax.legend(fontsize=22) . &lt;matplotlib.legend.Legend at 0x7fa193ecdd10&gt; . Predicting test values from noisy data . Now assume that you have noisy training data, with $ sigma = 0.04$. . key = random.PRNGKey(12345) key,subkey = random.split(key) σ = 0.04 f_noisy = f_samples + σ*random.normal(subkey, shape=f_samples.shape) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) #ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mew=1, label=&#39;Training&#39;) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) ax.legend(fontsize=22) plt.show() . Optimize the hyperparameters $( eta, l)$ and make predictions for the test points in this case. . #collapse @jit def logp(p, μ, x, f, noise): η,l = np.exp(p) L = Cholesky(η**2*K(x, x, l), noise) # K = L L^t v = np.linalg.solve(L, f - μ) # v = (L δf) return (0.5*np.dot(v,v) + np.sum(np.log(np.diag(L)))) + np.log(η) + np.log(l) gradlogp = jit(grad(logp)) . . #collapse def hyperparams(init,μ,x,f,noise): opt0 = optimize.minimize(logp, init, args=(μ,x,f,noise), method=&#39;Nelder-Mead&#39;, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) opt1 = optimize.minimize(logp, opt0[&#39;x&#39;], args=(μ,x,f,noise), method=&#39;BFGS&#39;, jac = gradlogp, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) return [opt0, opt1] . . opts = hyperparams(np.zeros(2), avg_samples, x_samples, f_samples, np.ones_like(x_samples)*σ**2) for o in opts: p = np.exp(o[&#39;x&#39;]) print(f&#39; t loss = {o[&quot;fun&quot;]:12.6e}, niter = {o[&quot;nit&quot;]:5d}, Converged = {o[&quot;success&quot;]:6b} : {o[&quot;message&quot;]}&#39;) print(f&#39; t t η = {p[0]:6.3f}, l = {p[1]:6.3f}&#39;) ηopt,lopt = p . loss = -3.405535e+01, niter = 95, Converged = 1 : Optimization terminated successfully. η = 0.215, l = 0.037 loss = -3.405535e+01, niter = 3, Converged = 1 : Optimization terminated successfully. η = 0.215, l = 0.037 . μpost, Kpost = GPpost(x, x_samples, f_noisy, np.ones_like(x_samples)*0.05**2, lambda x: 2.0, lambda x1,x2: ηopt**2*K(x1,x2,lopt)) σpost = np.sqrt(np.diag(Kpost)) . finterp = interp1d(x_samples, f_noisy, kind=&#39;cubic&#39;) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mew=2, label=&#39;Training&#39;) #ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid_highlight&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;, ls=&#39;--&#39;) ax.legend(fontsize=22) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) plt.show() .",
            "url": "https://johnjmolina.github.io/MLKyoto/gaussian%20processes/2020/07/03/Gaussian-Processes-Theory.html",
            "relUrl": "/gaussian%20processes/2020/07/03/Gaussian-Processes-Theory.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bayesian Model Selection",
            "content": "Motivation . This is the third post, dealing with the problem of Model Selection and corresponding to Chapter 4 of Sivia and Skilling&#39;s book. The problem we wish to tackle is the following: Given a signal, how many peaks is there most evidence for, and what are the positions and amplitudes of those peaks. If our prior information included knowledge of the number of peaks, figuring out their positions and amplitudes would be a simple excercise in parameter estimation. However, here we are faced with the more challenging task of choosing between different models (i.e., the one with $M=2$, $M=3$,... number of peaks). . References . This topic is discussed in some detail in Sivia&#39;s book (Ch. 4, Least-Squares Extensions), as well as Jaynes&#39; (Ch. 20, Model Comparison). . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | . In addition, Sivia also has several papers covering this material, with an emphasis on analyzing spectral data . Molecular spectroscopy and Bayesian spectral analysis—how many lines are there? D. S. Sivia and C. J. Carlile, J. Chem. Phys. 96, 170 (1992) | Bayesian analysis of quasieleastic neutron scattering data D. S. Sivia, C. J. Carlile, W. S. Howells, and S. Konig, Physica B 182, 341 (1992) | A Bayesian approach to extracting structure-factor amplitudes from powder diffraction data D. S. Sivia and W. I. F. David, Acta Cryst. A 50, 703 (1994) | The Bayesian approach to reflectivity dataD. S. Sivia and J. R. P. Webster, Physica B 248, 327 (1998) | . Introduction . Let&#39;s say we have to decide between two models, $M=1$ and $M=2$, given some experimental data $D$. How to decide between them? We should look at the posterior ratio . begin{align} O_{21} &amp; equiv frac{P(M_2 lvert D, I)}{P(M_1 lvert D, I)} &amp;= frac{ frac{P(D lvert M_1, I) P(M_1 lvert I)}{P(D lvert I)}}{ frac{P(D lvert M_2, I) P(M_2 lvert I)}{P(D lvert I)}} &amp;= frac{P(D | M_2)}{P(D|M_1)} cdot frac{P(M_2)}{P(M_1)} &amp;= frac{Z_2}{Z_1} cdot frac{ Pi_2}{ Pi_1} end{align}where $Z_i = P(D|M_i,I)$ is the evidence for model $M_i$, and $ Pi_i = P(M_i lvert I)$ the prior. This ratio of likelihoods $Z_2/Z_1$ is also known as the &#39;Bayes Factor&#39;. . We can compute the likelihood $P(D|M_i, I)$ by marginalizing over the parameters $ Theta= Theta_i$ of model $M_i$. To simplify the notation we drop the index $i$, but it should be clear that all quantities are defined on a per/model basis . begin{align} Z equiv P(D | M, I) &amp;= int mathrm{d} Theta , P(D, Theta lvert M, I) &amp;= int mathrm{d} Theta , P(D lvert Theta, M, I) cdot P( Theta lvert M, I) &amp;= int mathrm{d} Theta , L( Theta_i) cdot Pi( Theta) end{align}with $L( Theta) = P(D| Theta, M, I)$ and $ Pi( Theta) = P( Theta lvert M, I)$ the likelihood of the data and prior for the parameters of model $M$, respectively. . Recall that the integrand that appears here is proportional to posterior for $ Theta$, which is the main quantity of interest in a parameter estimation problem begin{align} P( Theta lvert D, M, I) &amp;= frac{ overbrace{P(D lvert Theta, M, I)}^{ text{Likelihood} , L} overbrace{P( Theta lvert M, I)}^{ text{Prior} , Pi}}{ underbrace{P(D lvert M, I)}_{ textrm{Evidence} ,Z}} &amp;= frac{L( Theta) Pi( Theta)}{Z} end{align} Where the evidence integral was usually ignored since it is just a normalizing factor. However, we now see that for a Model Selection problem these $Z$ are the main objects of interest. . Evaluating this integral by brute force will only be possible for very simple models (low dimensions). However, following Jaynes (ch. 20), we can rewrite these factors into a more telling form. . Let $L^ star = L( Theta^ star) = max_{ Theta}{L}$, the evidence is now begin{align} Z &amp;= L^ star underbrace{ int text{d} Theta , frac{L( Theta)}{L^ star} cdot Pi( Theta)}_{ textrm{Ockham factor} ,W} = L^ star W end{align} where $W$ is the so-called &#39;Ockham factor&#39;. It is this term which will protect us from over-fitting. By definition, the prior $ Pi( Theta) = P( Theta | M)$ integrates to one, so we see that the factor of $L/L^ star$ is picking out the fraction of prior probability mass contained in the high-likelihood regions. Quoting from Jaynes: &quot;...the Ockham factor is the ratio by which the parameter space is contracted by the information in the data, which expresses how much the vagueness of our prior infomration deteriorates the performance of [the model], by placing prior probability outside its high-likelihood region&quot; . Finally, we can write the Bayes factor appearing in the posterior ratio as begin{align} O_{21} &amp;= underbrace{ frac{L_2^ star}{L_1^ star}}_{ textrm{Goodness of fit}} cdot underbrace{ frac{W_2^{ phantom{ star}}}{W_1^{ phantom{ star}}}}_{ textrm{Model robustness}} cdot underbrace{ frac{ Pi_2^{ phantom{ star}}}{ Pi_1^{ phantom{ star}}}}_{ textrm{Prior ratio}} end{align} . To see how this could favor one model over another, let&#39;s use the Laplace approximation to try and evaluate the integral in the Ockham factor. begin{align} W &amp;= int textrm{d} Theta e^{ mathcal{L}} Pi( Theta) mathcal{L}&amp;= ln{ frac{L( Theta)}{L^ star}} end{align} expanding $ mathcal{L}$ around the maximum, $ Theta^ star$, we have begin{align} mathcal{L}&amp; simeq mathcal{L}^ star + left. nabla mathcal{L} right lvert_{ Theta^ star} cdot ( Theta - Theta^ star) + frac{1}{2}( Theta- Theta^ star)^{t} cdot left. nabla nabla mathcal{L} right lvert_{ Theta^*} cdot( Theta- Theta^ star) &amp;= frac{1}{2}( Theta - Theta^ star)^t cdot left. nabla nabla mathcal{L} right lvert_{ Theta^ star}( Theta- Theta^ star) end{align} since $ mathcal{L}^ star = ln{L^ star/L^ star} = 0 = left. nabla mathcal{L} right lvert_{ Theta^ star}$ by definition. . Thus, we can write the Ockham factor as begin{align} W &amp;= int text{d} Theta , exp{ left[- frac{1}{2}( Theta- Theta^ star)^t cdot Sigma^{-1} cdot( Theta- Theta^ star) right]} Pi( Theta) Sigma &amp;= left( left.- nabla nabla mathcal{L} right lvert_{ Theta^ star} right)^{-1} end{align} . Now assume that $ Pi( Theta)$ is constant, $ Pi( Theta) = 1/ Omega$, with $ Omega$ the allowed volume in parameter space. begin{align} W &amp; simeq frac{ sqrt{(2 pi)^p det{ Sigma}}}{ Omega} end{align} with $p$ the number of parameters for this model, $ Theta = ( Theta_1, Theta_2, ldots, Theta_p)$. . Recall that $ Sigma$ is the covariance matrix, so that $ det Sigma$ is related to the volume of the parameter region that is consistent with the data. In this case, larger error bars are actually a good thing! They are telling us that the model allows for more values that can explain the data, and thus is more robust, and should be preferred. . How many lines are there? . Let our parameters be denoted as . begin{align} M &amp;: textrm{Number of Line Peaks} A_i &amp;: textrm{Amplitude of peak} ,i X_i &amp;: textrm{Position of peak} , i b_1 &amp;: textrm{Intercept for linear background signal} b_2 &amp;: textrm{Slope for linear background signal} D= {x_k,y_k } &amp;: textrm{Experimental data} end{align}With the following prior information . begin{align} I &amp;: textrm{Lines peaks are measured as gaussians of fixed width } W textrm{, subject to Gaussian noise of variance } sigma, textrm{ in the presence of a linear background signal}. end{align}The inference problem to solve is then, given a spectra, how many peaks are there (at what positions and with what magnitudes)? . The posterior probability we are interested in is $P(M lvert D I)$, i.e., the probability that the signal corresponds to an $M$-peak model. From Bayes theorem, we have the following . begin{align} P(M, D lvert I) &amp;= P(M lvert D,I)P(D lvert I) = P(D lvert M,I)P(M lvert I) P(M lvert D,I) &amp;= frac{P(D lvert M,I) P(M lvert I)}{P(D lvert I)} end{align}Now, if we assign uniform priors for $P(M lvert I) = 1/M_{ text{max}}$, with $M=1, ldots, M_{ text{max}}$, the posterior becomes proportional to the evidence begin{align} P(M lvert D, I) &amp; propto P(D lvert M, I) = Z_M end{align} where the normalization constant can be evaluated later, since begin{align} sum_{M=0}^{M_{ text{Max}}} P(M lvert D, I)&amp;= alpha sum_M P(D lvert M, I)= 1 alpha^{-1} equiv M_{ text{max}} P(D lvert I) &amp;= sum_M P(D lvert M, I) end{align} . To evaluate the evidence , we &quot;simply&quot; marginalize over the model parameters, . begin{align} Z_M &amp;= int textrm{d}b_1 int textrm{d} b_2 int textrm{d}A^{M} int textrm{d}X^M P(D, b_1, b_2, {A_i, X_i } lvert M, I) &amp;= int textrm{d}b_1 int textrm{d}b_2 int textrm{d}A^{M} int textrm{d}X^M P(D lvert b_1, b_2, {A_i,X_i }, M, I) P(b_1, b_2, {A_i,X_i } lvert M, I) end{align}The prior can be decomposed as begin{align} P(b_1, b_2, {A_i, X_i } lvert M, I) &amp;= P(b_1, b_2 lvert {A_i, X_i }, M, I) , P( {A_i, X_i } lvert M, I) &amp;= P(b_1, b_2 lvert I) , P( {A_i, X_i } lvert M, I) end{align} where we use the fact that the background signal parameters do not depend on the number of lines (or peak parameters). Thus, this prior $P(b_1,b_2 lvert I)$ can be absorbed into the normalization constant $ alpha$. . For the peak position and amplitude parameters we assume independent uniform priors, such that $X_{ textrm{min}} le X le X_{ textrm{max}}$ and $0 le A le A_{ textrm{max}}$ begin{align} P( {A_i,X_i } lvert M, I) &amp;= prod_{i} P(A_i,X_i lvert M,I) &amp;= prod_i P(A_i lvert M,I) P(X_i lvert M, I) &amp;= left(A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right)^{-M} end{align} . Therefore, the posterior becomes . begin{align} P(M lvert D, I) &amp; propto frac{1}{ left[A_{ text{max}} left(X_ text{max} - X_ text{min} right) right]^M} int textrm{d}b_1 int textrm{d}b_2 int text{d}A^M int text{d}X^M P(D lvert b_1, b_2, {A_i,X_i }, M, I) end{align} Our prior information says that the measurements are independent, subject to Gaussian noise of amplitude $ sigma_k$. Then, the Likelihood for the data, given the M-peak model and its parameters becomes . begin{align} P(D lvert b_1, b_2, {A_i, x_i }, M, I) &amp; propto P( {D_k } lvert b_1, b_2, {A_i, x_i }, M, I) &amp;= prod_k P(D_k lvert b_1, b_2, {A_i, x_i }, M, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{1}{ sqrt{2 pi}} left( frac{y_k - f(x_k; b_1, b_2, {A_i, X_i })}{ sigma_k} right)^2 right]} &amp; propto exp{ left(- frac{ chi^2}{2} right)} chi^2 &amp;= sum_k left( frac{y_k - f_k}{ sigma_k} right)^2 end{align}where the factors of $ sqrt{2 pi sigma_k^2}$ can be absorbed into the normalization constant since they don&#39;t depend on $M$, but only the number of data points $N$ (the $ sigma_k$ are not model parameters). . We therefore have the following for the posterior (evidence) . begin{align} P(M lvert D, I) propto frac{1}{ left[A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right]^M} int textrm{d}b_1 int textrm{d}b_2 int textrm{d}A^M int textrm{d}X^M exp{ left(- frac{ chi^2}{2} right)} end{align} A brute force approach to this $2M + 2$ dimensional integral is out of the question. However, we can proceed analytically by using the Laplace approximation, expanding around the minimum of $ chi^2$. Let $ Theta$ denote the model parameters, $ Theta = (b_1, b_2, {A_i }, {X_i })$, with $ Theta_0$ the minimum, such that . begin{align} chi^2&amp; simeq chi^2_0 + frac{1}{2} left( Theta - Theta_0 right)^t cdot left. nabla nabla chi^2 right lvert_{ Theta_0} cdot left( Theta - Theta_0 right) &amp;= chi^2_0 + left( Theta- Theta_0 right)^t cdot Sigma^{-1} cdot left( Theta- Theta_0 right) Sigma^{-1} &amp;= frac{1}{2} left. nabla nabla chi^2 right lvert_{ Theta_0} end{align} Putting this expression back into the evidence integral gives us . begin{align} P(M lvert D, I) &amp; propto frac{1}{ left[A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right]^M} e^{- chi^2_0/2} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot Sigma^{-1} cdot left( Theta- Theta_0 right) right]} &amp; propto frac{M!}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M} e^{- chi^2_0/2} sqrt{(2 pi)^{2M+2} det{ Sigma}} end{align}where the factor of $M!$ is needed to account for all possible permutations of the peaks (i.e., the labels we use are arbitrary and should not matter). . The determinant of the Hessian matrix can be simplified as follows, begin{align} det{ Sigma} &amp;= frac{1}{ det{ Sigma^{-1}}} = frac{1}{ det{ left( frac{1}{2} left. nabla nabla chi^2 right lvert_{ Theta} right)}} = frac{2^{2M+2}}{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}} end{align} from which we finally obtain begin{align} P(M lvert D, I) &amp; propto frac{M!}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M} e^{- chi^2_0/2} sqrt{ frac{(2 pi)^{2M+2} 2^{2M + 2}}{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}}} &amp;= frac{M! (4 pi)^{M+1}}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M sqrt{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}}} e^{- chi^2_0/2} end{align} . We dropped all the proportionality constants that did not depend on $M$, but we can now recover them by enforcing the normalization condition, $ sum_k P(M lvert D, I) = 1$. . As always, it is more convenient to work with the logarithm of the posterior, . begin{align} ln{P(M lvert D, I)}&amp;= - frac{ chi_0^2}{2} + ln{M!} + M Big[ ln{4 pi} - ln big(A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) big) Big] - frac{1}{2} ln{ left[ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)} right]} + ln{4 pi} end{align}Thus, within the approximations we have made, the model selection problem can be effectively reduced to a series of minimization problems (one for each of the models). Due to the non-linearity, Sivia (Ch. 4.2.1) advocates for the following numerical algorithm . ($M=0$) Find the optimum values for the 2d problem (b_1, b_2) | | ($M=1$) Perform a 1d scan over the new peak position $x_1$, by minimizing with respect to $(b_1, b_2, A_1)$, using the optimum values for ($M=0$) as initial guess. For the initial $A_1$, we can use the minimum of signal as a guide. Take the $x_1$ value resulting in the lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, x_1)$, using the optimum values obtained at step (1.) as an initial guess. | | ($M=2$) Perform a 1d scan over the new peak position $x_2$, by minimizing with respect to the background and amplitude parameters, $(b_1, b_2, A_1, A_2)$. At each point in this scan, the positions of the peaks are fixed to ($x_1^{(M=1)}, x_2$), with the initial guess taken from the optimum value for $M=1$. Take the $x_2$ value resulting in the lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, A_2, x_1, x_2)$, using the optimum values obtained at step (1.) as an initial guess. $ vdots$ | | ($M=m$) Perform a 1d scan over the new peak position $x_m$, by minimizing with respect to the background and amplitude parameters, $(b_1, b_2, A_1, A_2, ldots A_m)$. At each point in this scan, the positions of the peaks are fixed to ($x_1^{(M=m-1)}, ldots, x_{m-1}^{(M=m-1)}, x_m$), with the initial guess taken from the optimum value for $M=m-1$. Take the $x_m$ value resulting int he lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, ldots , A_m, x_1, ldots, x_m)$, using the optimum values obtained at step (1.) as an initial guess. | | . Additional hints and best practices: . Scale all parameters to be unity and dimensionless | Add small multiples of identity to the Hessian matrix: $ nabla nabla chi^2 longrightarrow nabla nabla chi^2 + epsilon I$ | . Numerical Example . import jax import jax.numpy as np import numpy as onp import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import pickle from numpy import random from scipy import integrate from scipy import optimize from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from functools import partial,reduce import matplotlib.patheffects as PathEffects mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) . Let&#39;s define our ideal peak and background signals . begin{align} f_{ mathrm{background}}(x; b_1, b_2) &amp;= b_1 + b_2 x f_{ text{peak}}(x; A, X) &amp;= A exp{ left(- frac{(x-X)^2}{2 w^2} right)} end{align}with $w=0.01$ the width of the peaks. Then, for an $M$-peak model, the ideal signal would be given as begin{align} f(x; b_1, b_2, {A_i, X_i }) &amp;= f_{ mathrm{background}}(x;b_1, b_2) + sum_{i=1}^M f_{ text{peak}}(x; A_i, X_i) end{align} . def ideal_bckg(b1,b2,x): &quot;&quot;&quot;Linear Backbround Signal&quot;&quot;&quot; return b1 + b2*x def ideal_peak(xi,ai,x): &quot;&quot;&quot;Ideal Gaussian Peak&quot;&quot;&quot; return ai*np.exp(-(x-xi)**2/(2*0.01**2)) def squash(b1,b2,pos,amp): &quot;&quot;&quot;Return signal parameters as single array&quot;&quot;&quot; n = len(pos) p = np.empty(2 + 2*n, dtype=pos.dtype) p = jax.ops.index_update(p, jax.ops.index[0],b1) p = jax.ops.index_update(p, jax.ops.index[1],b2) p = jax.ops.index_update(p, jax.ops.index[2::2],pos) p = jax.ops.index_update(p, jax.ops.index[3::2],amp) return p def unsquash(p): &quot;&quot;&quot;Return signal parameters independently&quot;&quot;&quot; return p[0], p[1], p[2::2], p[3::2] def ideal_signal0(p, x): &quot;&quot;&quot;Ideal (noiseless) signal&quot;&quot;&quot; b1,b2,xi,ai = unsquash(p) return ideal_bckg(b1,b2,x) + np.sum(ideal_peak(xi,ai,x), axis=-1) ideal_signal = jit(vmap(ideal_signal0, in_axes=(None, 0), out_axes=0)) . Let&#39;s generate some synthetic data for a five peak signal (roughly corresponding to Fig. 4.4 in Sivia&#39;s book) . peaks_xi = (np.array([11.5, 11.55, 12.15, 12.8, 13.5])-11.0)/3.0 peaks_ai = np.array([1.8, 7.0, 3.5, 3.8, 8.9])/10 peaks = np.vstack([peaks_xi, peaks_ai]).transpose() line_b1 = 0.3 line_b2 = 0.02 params = squash(line_b1, line_b2, peaks_xi, peaks_ai) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . fig, [ax,bx] = plt.subplots(figsize=(18,9),ncols=2) theta = np.linspace(0, 1., num=300) signal = ideal_signal(params, theta) ax.plot(theta, signal, color=color[&#39;mid&#39;]) ax.plot(theta, ideal_bckg(line_b1, line_b2, theta), color=color[&#39;mid_highlight&#39;], ls=&#39;:&#39;, lw=4) bx.plot(theta-0.5, ideal_peak(theta, 1, 0.5), color=color[&#39;mid&#39;]) for x0,a0 in zip(peaks_xi, peaks_ai): baseline = ideal_bckg(line_b1, line_b2, x0) ax.vlines(x0, baseline, baseline + a0, alpha=0.8, color=color[&#39;superfine&#39;]) ax.set_title(r&#39;Pure Signal&#39;) bx.set_title(r&#39;Ideal Peak&#39;) bx.set_xlim(-0.06, 0.06) ax.set_ylim(0.2, 1.3) fig.tight_layout() plt.show() . Now let&#39;s add some noise to this ideal signal, to see how our peak finding algorithm behaves. We generate 5 noisy datasets, with increasing amplitude of the signal noise. . def noisy_signal(μ, σ, n): &quot;&quot;&quot;Given ideal signal μ, return simulated measurement, averaged over n samples, with variance σ&quot;&quot;&quot; counts = onp.random.normal(μ, σ, size=(n,)+μ.shape) avg = onp.average(counts, axis=0) err = onp.std(counts, axis=0) return pd.DataFrame({&#39;avg&#39;:avg, &#39;err&#39;:err}) random.seed(12345) σs = np.linspace(0.02, 0.1, num=5) datas = [noisy_signal(signal, σ, 100) for σ in σs] . fig, axes = plt.subplots(figsize=(18,18), ncols=2, nrows=3, sharex=True, sharey=True) for ax,data,σ in zip(axes.flatten(), datas, σs): ax.errorbar(theta, data[&#39;avg&#39;], data[&#39;err&#39;], ls=&#39;None&#39;, lw=1.5, alpha=0.8, color=color[&#39;mid_highlight&#39;]) ax.plot(theta, data[&#39;avg&#39;] , marker=&#39;o&#39;, ms=6, mfc=&#39;None&#39;, mew=1.2, color=color[&#39;dark&#39;], ls=&#39;None&#39;) for ax,σ in zip(axes.flatten()[:-1], σs): for x0,a0 in zip(peaks_xi, peaks_ai): baseline = ideal_bckg(line_b1, line_b2, x0) ax.vlines(x0, baseline, baseline + a0, alpha=0.8, color=color[&#39;superfine&#39;]) addtxt(ax, 0.1, 0.9, f&#39;$ sigma = {σ:3.2f}$&#39;, fs=18) fig.tight_layout() . To find the optimum parameters, we minimize the $ chi^2$, begin{align} chi^2 &amp;= sum_k left( frac{y_k - f_k}{ sigma_k} right)^2 end{align} . However, keep in mind that the algorithm outlined above requires three seperate minimizations: . Background only begin{align} f_k = f_{ text{background}}(x_k; b_1, b_2) end{align} | Background and peak amplitudes begin{align} f_k = f(x_k; b_1, b_2, {A_i, underline{X_i} }) end{align} where the underline refers to parameters that are held fixed (i.e., the peak positions) | Full minimization begin{align} f_k = f(x_k; b_1, b_2, {A_i,X_i }) end{align} | . In practice we minimize the average deviations, i.e., $ chi^2/N$. So in the end, we should not forget to rescale both $ chi^2$ and the Hessian $ nabla nabla chi^2$. . Let&#39;s define the corresponding loss functions. Thanks to the fact that we are using jax, we get the gradients and the hessians for free (basically). This greatly improves the convergence of the minimization routines. . @jit def loss_bckg(p, x, y, dy): b1,b2 = p ymodel = ideal_bckg(b1, b2, x) return np.mean(((y-ymodel)/dy)**2) dloss_bckg = jit(grad(loss_bckg)) @jit def loss(p, x, y, dy): ymodel = ideal_signal(p, x) return np.mean(((y - ymodel)/dy)**2) dloss = jit(grad(loss)) hess_loss= hessian(loss) @jit def loss_fixpos(p, pos, x, y, dy): param = squash(p[0], p[1], pos, p[2:]) return loss(param, x, y, dy) dloss_fixpos = jit(grad(loss_fixpos)) . Thanks to the fact that we are using jax, we get the gradients and the hessians for free (basically). This greatly improves the convergence of the minimization routines. . The peak finding algorithm can then be written as follows . def findNewPeak(x,y,dy,old_guess): &quot;&quot;&quot;Determine number of positions of peaks in a given signal x : measured x data y : measured y data dy : measured y uncertainty &quot;&quot;&quot; ### 1D scan over x to locate peak candidate def fitBckAmp(init, peak_pos, x, y, dy): &quot;&quot;&quot;Optimize background and amplitude parameters given position of the peaks x : measured x data y : measured y data dy : measured y uncertainty peak_pos : fixed position of peaks init : inital guess for background (b, m) and peak amplitudes (ai) &quot;&quot;&quot; opt0 = optimize.minimize(loss_fixpos, init, args=(peak_pos, x, y, dy), method=&#39;BFGS&#39;, jac=dloss_fixpos, options={&#39;disp&#39;:0}) return opt0 old_b1, old_b2, old_pos, old_amp = unsquash(old_guess) old_n = len(old_pos) #### #### initial guess for background and amplitude parameters #### init0 = np.zeros(2 + old_n + 1) init0 = jax.ops.index_update(init0, jax.ops.index[:2], old_guess[:2]) # b_1,b_2 init0 = jax.ops.index_update(init0, jax.ops.index[2:2+old_n], old_amp) # a_i (i=1,n-1) init0 = jax.ops.index_update(init0, jax.ops.index[old_n], np.minimum(np.min(old_amp)*0.25, np.min(y))) #a_n # fixed position parameters pos0 = np.zeros(old_n + 1) pos0 = jax.ops.index_update(pos0, jax.ops.index[:old_n], old_pos) opt0 = [fitBckAmp(init0,jax.ops.index_update(pos0, jax.ops.index[old_n], x0),x,y,dy) for x0 in x] # scan over x, optimizing (b1,b2,ai) idx0 = np.argmin(np.array([dmy[&#39;fun&#39;] for dmy in opt0])) # best guess for new peak @ min(chi2) p0 = opt0[idx0][&#39;x&#39;] # optimized (b1,b2,ai) parameters ### ### Full optimization in 2n + 2 parameters space ### pos0 = jax.ops.index_update(pos0, jax.ops.index[old_n], x[idx0]) init = squash(p0[0], p0[1], pos0, p0[2:]) opt1 = optimize.minimize(loss, init, args=(x,y,dy), method=&#39;Nelder-Mead&#39;) opt = optimize.minimize(loss, opt1[&#39;x&#39;], args=(x,y,dy), method=&#39;BFGS&#39;, jac=dloss) opt[&#39;chi2&#39;] = opt[&#39;fun&#39;]*len(x) opt[&#39;H&#39;] = hess_loss(opt[&#39;x&#39;], x, y, dy)*len(x) opt[&#39;H_inv&#39;]= np.linalg.inv(opt[&#39;H&#39;]) #rescale approximate hessian given by bfgs opt[&#39;hess_inv&#39;] = opt[&#39;hess_inv&#39;]/len(x) opt[&#39;hess&#39;] = np.linalg.inv(opt[&#39;hess_inv&#39;]) return opt def fitBackground(x,y,dy): &quot;&quot;&quot; Fit background signal &quot;&quot;&quot; opt = optimize.minimize(loss_bckg, np.array([np.min(y), 0.0]), args=(x,y,dy), method=&#39;Nelder-Mead&#39;) return opt[&#39;x&#39;] def findPeaks(x,y,dy,Mmax): opts = [] oldguess = fitBackground(x,y,dy) print(&#39;*** Searching for Peaks ***&#39;) for M in range(1,Mmax+1): opt = findNewPeak(x,y,dy,oldguess) opts.append(opt) oldguess = opt[&#39;x&#39;] print(f&quot; t M = {M}: t χ2 = {opt[&#39;chi2&#39;]:8.3e}, niter = {opt[&#39;nit&#39;]:4d}, Converged = {opt[&#39;success&#39;]:3b} : {opt[&#39;message&#39;]}&quot;) return opts . opts = [findPeaks(theta, data[&#39;avg&#39;].values, data[&#39;err&#39;].values, 10) for data in datas] . *** Searching for Peaks *** M = 1: χ2 = 9.849e+03, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 3.237e+03, niter = 9, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 1.626e+03, niter = 12, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 1.466e+02, niter = 14, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.382e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.285e+00, niter = 19, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.214e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 3.159e+00, niter = 24, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 3.097e+00, niter = 25, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 3.034e+00, niter = 28, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 2.663e+03, niter = 7, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 8.824e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 4.246e+02, niter = 12, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 3.677e+01, niter = 15, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.118e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.051e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.006e+00, niter = 19, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.964e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.928e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.898e+00, niter = 22, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 1.181e+03, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 4.212e+02, niter = 9, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 2.119e+02, niter = 11, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 2.007e+01, niter = 14, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.538e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.467e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.424e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 3.383e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 3.343e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 3.304e+00, niter = 18, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 6.756e+02, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 2.144e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 1.084e+02, niter = 13, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 1.188e+01, niter = 12, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 2.853e+00, niter = 13, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 2.758e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 2.687e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.630e+00, niter = 14, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.577e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.539e+00, niter = 15, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 4.331e+02, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 1.453e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 7.171e+01, niter = 10, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 8.578e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 2.993e+00, niter = 14, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 2.903e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 2.825e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.757e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.713e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.679e+00, niter = 18, Converged = 1 : Optimization terminated successfully. . begin{align} ln{P(M lvert D, I)}&amp;= - frac{ chi_0^2}{2} + ln{M!} + M Big[ ln{4 pi} - ln big(A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) big) Big] - frac{1}{2} ln{ left[ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)} right]} + ln{4 pi} end{align} def dump(fname, obj): with open(fname, &#39;wb&#39;) as fp: pickle.dump(obj, fp, protocol=pickle.HIGHEST_PROTOCOL) def load(fname): with open(fname, &#39;rb&#39;) as fp: return pickle.load(fp) . def logposterior(vol, opt): &quot;&quot;&quot;Calculate log posterior probability vol: prior volume term (Amax (Xmax - Xmin)) opt: result of minimization procedure Returns: log(P(M|D,I)) = -0.5 χ^2 + log(M!) + M[log(4π) - log(vol)] - 0.5log(det( nabla nabla χ^2)) &quot;&quot;&quot; def logfact(n): &quot;&quot;&quot;Computes log(n!)&quot;&quot;&quot; return np.sum([np.log(i) for i in range(1,n+1)]) hess = opt[&#39;H&#39;] M = (len(opt[&#39;x&#39;]) - 2)//2 logp = -opt[&#39;chi2&#39;]/2 + logfact(M) + M*np.log(4*np.pi/vol) - 0.5*np.log(np.linalg.det(hess)) return logp def normalize(logp): &quot;&quot;&quot;Computed normalized posteriors&quot;&quot;&quot; α = reduce(np.logaddexp, logp) return logp - α def estimates(opt): &quot;&quot;&quot;Compute optimum position/amplitude and background parameters&quot;&quot;&quot; Θ = opt[&#39;x&#39;] Σ = opt[&#39;H_inv&#39;] sig = np.sqrt(2.0*np.diagonal(Σ)) b1,b2,amp,pos = unsquash(opt[&#39;x&#39;]) db1,db2,damp,dpos = unsquash(sig) return [b1,db1], [b2, db2], [amp, damp], [pos, dpos] . logps, bests = [], [] for opt, data in zip(opts, datas): y = data[&#39;avg&#39;].values arange = integrate.simps(y,x=theta) / integrate.simps(ideal_signal(squash(np.min(y), 0.0, np.array([0.5]), np.array([1.0])), theta), x=theta) xrange = np.max(theta) - np.min(theta) logp = normalize(np.array([logposterior(arange*xrange, opt_m) for opt_m in opt])) bests.append(np.argmax(logp)) # for each signal (corresponding to different σ) find the model M with the hight posterior probability logps.append(np.array([lp / np.log(10) for lp in logp])) # report log posteriors in log10 basis . fig, ax = plt.subplots(figsize=(12,6)) for lp,sig,c in zip(logps, σs, mpl.cm.viridis(np.linspace(0,1,len(σs)))): ax.plot(np.arange(1,11), lp, marker=&#39;o&#39;, color=c, label=f&#39;$ sigma = {sig:.2f}$&#39;, alpha=0.9) ax.set_xlim(3,9) ax.set_ylim(-12,1) ax.set_ylabel(r&#39;$ log_{10}{P left(M| D I right)}$&#39;, fontsize=22) ax.set_xlabel(r&#39;Number of peaks $M$&#39;, fontsize=22) ax.legend(fontsize=22) plt.show() . We see that for the three signals with the larger signal/noise ratio, we are able to correctly infer the number of peaks, i.e., the posterior $P(M=5 lvert D, I)$ maximum. For the two signals with the large noise amplitude, we have a flatter posterior, with $P(M=4 lvert D, I) gtrsim P(M=5 lvert D, I)$. . Now, let&#39;s plot the peak positions and amplitudes for the model $M$ with the highest posterior, for each of the five signals. . fig, ax = plt.subplots(figsize=(12,9)) print(f&#39;Background parameters:&#39;) colors = mpl.cm.viridis(np.linspace(0, 1, len(σs))) for i,opt in enumerate(opts): best,sig,c = bests[i], σs[i], colors[i] best_inter, best_slope, best_pos, best_amp = estimates(opt[best]) print(f&#39;σ = {sig:.2f} m = {best_slope[0]:.3f} +/- {best_slope[1]:.3f}, b = {best_inter[0]:.3f} +/- {best_inter[1]:.3f}&#39;) ax.errorbar(best_pos[0], best_amp[0], xerr=best_pos[1], yerr=best_amp[1], ls=&#39;none&#39;, marker=&#39;s&#39;, color=c, mew=3, mfc=&#39;None&#39;, alpha=0.6, label=f&#39;$ sigma = {sig:.2f}$&#39;) ax.plot(peaks_xi, peaks_ai, marker=&#39;x&#39;, ms=22, mew=3, ls=&#39;None&#39;, color=&#39;C2&#39;) ax.legend(fontsize=22) ax.set_xlabel(r&#39;Peak Position $x$&#39;, fontsize=22) ax.set_ylabel(r&#39;Amplitude&#39;, fontsize=22) plt.show() . Background parameters: σ = 0.02 m = 0.020 +/- 0.004, b = 0.300 +/- 0.002 σ = 0.04 m = 0.022 +/- 0.008, b = 0.299 +/- 0.005 σ = 0.06 m = 0.021 +/- 0.013, b = 0.299 +/- 0.007 σ = 0.08 m = 0.009 +/- 0.017, b = 0.308 +/- 0.010 σ = 0.10 m = 0.012 +/- 0.021, b = 0.306 +/- 0.012 . As expected, for the three signals with the smallest noise we are able to correctly infer not only the number of peaks, but their positions and amplitudes. For the signals with large noise, the two peaks at $x sim 0.2$ become indistinguishable, merging into one peak with a larger amplitude. .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/model%20selection/2020/06/19/Model-Selection-How-Many-Lines.html",
            "relUrl": "/data%20analysis/model%20selection/2020/06/19/Model-Selection-How-Many-Lines.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Bayesian Parameter Estimation with Outliers",
            "content": "Motivation / Disclaimer . This is the second in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . Having covered the basics of Bayesian Data Analysis, following Chapters 2 and 3 of the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling, we now consider a slightly more complicated case. . Now, we study how to perform parameter estimation in the presence of outliers. . References . This topic is discussed in some detail in Sivia&#39;s book (Ch. 8, Least-Squares Extensions), as well as Jaynes&#39; (Ch. 21, Outliers and Robustness). . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | . In addition, we also found the following paper to be very useful and in line with the discussions found in the books mentioned above . Data analysis recipes: Fitting a model to data, D. W. Hogg, J. Bovy, D. Lang, arXiv:1008.4686 (2010) | . Below we basically follow Jaynes, using the data of Hogg et al. . Intro . Let us consider that our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. Furthermore, assume that on prior information, we know that the results of any given measurement can be either good or bad. . When analyzing the data, in order to determine the parameters $ Theta$, we want to make sure that we use a robust method that is able to account for these outliers. As always with these parameter estimation problems, the quantity we are interested is the following posterior distribution begin{align} P( Gamma lvert D, I) &amp; propto P(D lvert Gamma ,I) P( Gamma lvert I) &amp;= L( Gamma) , Pi( Gamma) L( Gamma) &amp;= P(D, lvert Gamma ,I) Pi( Gamma) &amp;= P( Gamma lvert I) end{align} with $ Gamma=( Theta, Xi)$, where $ Theta$ is parametrizing the target function $f(X; Theta)$, and $ Xi$ refers to all the remaining parameters (e.g., those related to the goodness/badness of the points). . The &quot;Mixture&quot; or &quot;Two-Model&quot; Model . To evaluate the likelihood, we consider the case of good/bad points separately. If a point is good, it is assumed to be sampled from a distribution $P_{ text{Good}}=G$, whereas bad points are sampled from $P_{ text{Bad}}= B$. . Assuming Gaussian errors, we set begin{align} P(y_k lvert mathrm{Good}_k, Gamma, I) = G(y_k lvert Theta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 right) P(y_k lvert mathrm{Bad}_k, Gamma, I) = B(y_k lvert Theta, eta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 + eta^2 right) end{align} Good points will be normally distributed around the true values $f(X_k; Theta)$, with a standard deviation given by the measured error bars. Bad points, on the other hand, will be distributed around the true values with a much larger variance, given by $ sigma_k^2 + eta^2$. Here, $ eta^2$ is an additional parameter which defines the variance of the bad points. I think that whether or not this represents the true physical origin behind the outliers is of secondary importance (as we are not trying to make inferences about this process). What matters here is that we can build a probabilistic model that can explain the data, i.e., the fact that we can have good/bad points, and that the bad points can show very large deviations with respect to the true values. I think we would obtain roughly the same results if we uncoupled the Bad data from the model (as done in Jaynes treatment), begin{align} y_k lvert text{Bad} sim mathcal{N} left(c, eta^2 right) end{align} with $c$ an additional nuissance parameter to be learned from the data. . Assuming independent measurements, the full likelihood $D= {y_k }_{k=1}^N$ can be expressed as a product of likelihoods for each individual measurement, such that . begin{align} L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[P(y_k lvert Gamma, I) Big] &amp;= prod_k Big[P big(y_k,( mathrm{Good}_k text{ or } mathrm{Bad}_k) lvert Gamma, I big) Big] &amp;= prod_k Big[P big(y_k, mathrm{Good}_k lvert Gamma, I big) + P big(y_k, mathrm{Bad}_k lvert Gamma, I big) Big] &amp;= prod_k Big[P big( mathrm{Good}_k lvert Gamma, I big) P(y_k lvert mathrm{Good}_k, Gamma, I) + P big( mathrm{Bad}_k lvert Gamma, I big) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] end{align} Where we have simply used marginalization over the Good/Bad state of the points. Now, let us assume that the probability of observing a given sequence of good/bad points is invariant under permutations. What matters is just the number of good/bad points, not the order in which they were obtained. Let $u$, which we don&#39;t know, be the probability that any given point is good, Jaynes calls this the &quot;purity&quot; of the data. The likelihood is then given by begin{align} P( mathrm{Good}_k lvert Gamma, I)&amp;= u L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[uP(y_k lvert mathrm{Good}_k, Gamma, I) + (1-u) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . The posterior for $ Theta$, which are the parameters we are actually interested in, is obtained by marginalizing over $u$ and $ eta$ begin{align} P( Theta lvert D, I)&amp;= iint mathrm{d}u mathrm{d} eta , P( Theta, u, eta lvert D, I) &amp;= frac{ iint mathrm{d}u mathrm{d} eta , L( Theta, u, eta) Pi( Theta, u, eta)}{ iiint mathrm{d} Theta mathrm{d}u mathrm{d} eta ,L( Theta, u, eta) Pi( Theta, u, eta)} end{align} where we have explicitly included the normalization constant in the definition. . We can use this model, with hyper-parameters $u$ and $ eta$, to compute the posterior for $ Theta$, and thus find the &quot;best-fit&quot; to the data, in the presence of outliers. . Digression on the two-model model . Let us look in a bit more detail into what exactly we are calculating here. For ease of reading (to avoid having to write $P$ all over the place), we will again follow Jaynes and rewrite the prior $ Pi( Gamma)$ as begin{align} Pi( Theta, u, eta)&amp;= P( Theta, u, eta lvert I) &amp;= P( Theta lvert I) P(u, eta lvert Theta, I) &amp;= Pi( Theta) H(u, eta lvert Theta, I) end{align} . where $ Pi( Theta)=P( Theta lvert I)$ and $H(u, eta lvert Theta, I)=P(u, eta lvert Theta, I)$. . Now, defining the &quot;pseudo-likelihood&quot; $ bar{L}$ as begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) end{align} . The marginalized posterior for $ Theta$ becomes begin{align} P( Theta lvert D, I) &amp;= frac{ bar{L}( Theta) Pi( Theta)}{ int mathrm{d} Theta bar{L}( Theta) Pi( Theta)} end{align} . To understand what this pseudo-likelihood is giving us, let&#39;s first expand the definition of $L( Theta, u, eta)$ in terms of the good and bad likelihoods $G$ and $B$. We obtain the following begin{align} L( Theta, u, eta) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u)B(y_k lvert Theta, eta) Big] &amp;= u^N prod_i G(y_i lvert Theta) +u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) prod_{i ne j} G(y_i lvert Theta) &amp; quad + u^{N-2}(1-u)^{2} sum_{j&lt;k} B(y_j lvert Theta, eta) B(y_k lvert Theta, eta) prod_{i ne j,k} G(y_i lvert Theta) &amp; quad vdots &amp; quad + (1-u)^{N} prod_{j}^N B(y_j lvert Theta, eta) &amp;=u^N L( Theta) &amp; quad+ u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) L_j( Theta) &amp; quad+ u^{N-2}(1-u)^2 sum_{j&lt;k}^N B(y_j lvert Theta, eta)B(y_k lvert Theta, eta) L_{jk}( Theta) &amp; quad vdots &amp; quad+ (1-u)^N B(y_1 lvert Theta, eta)B(y_2 lvert Theta, eta) cdots B(y_N lvert Theta, eta) &amp;= L^{(0)}( Theta, u, eta)+ L^{(1)}( Theta, u, eta) + L^{(2)}( Theta, u, eta) + ldots + L^{(N)}( Theta, u, eta) end{align} . Here, we use $L( Theta) = prod_i G(y_i lvert Theta)$ to denote the likelihood of the pure model, i.e., one without outliers. Likewise $L_j( Theta)$ is the likelihood of observing $ {y_{i ne j} }$ from the pure model, $L_{jk}$ that of observing $ {y_{i ne j,k} }$, etc. . Plugging this expression into the definition of $ bar{L}$, begin{align*} bar{L} &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , left[L^{(0)}( Theta, u, eta) + L^{(1)}( Theta, u, eta) + cdots + L^{(N)}( Theta, u, eta) right] H(u, eta lvert Theta, I) end{align*} we would obtain the following for the first term begin{align} iint mathrm{d}u mathrm{d} eta L^{(0)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^N L( Theta) H(u, eta lvert Theta, I) &amp;= L( Theta) int mathrm{d}u , u^N int mathrm{d} eta , H(u, eta lvert Theta,I) &amp;= L( Theta) underbrace{ int mathrm{d}u , u^N H(u lvert Theta, I)}_{ text{Probability that all points are good}} end{align} where in the second to last step we marginilize $h(u, eta lvert Theta, I)$ over $ eta$. Notice that the integral in the last line evaluates to the probability all points are good, regardless of $u$ and conditioned on $ Theta$ and $I$, begin{align*} P( text{All points good} lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good}, u lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good} lvert u, Theta, I) P(u lvert Theta, I) &amp;= int mathrm{d}u , u^N P(u lvert Theta, I) = int mathrm{d}u , u^N H(u lvert Theta, I) end{align*} . We would obtain something similar for the second term . begin{align} iint mathrm{d}u mathrm{d} eta , L^{(1)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^{N-1}(1-u) B(y_j lvert Theta, eta) L_j( Theta) H(u, eta lvert Theta, I) &amp;= L_j( Theta) underbrace{ int B(y_j lvert Theta, I) underbrace{ mathrm{d} eta int mathrm{d}u , u^{N-1}(1-u) H(u, eta lvert Theta, I)}_{ textrm{Probability that any given point } y_j textrm{ is bad,} atop textrm{all others are good, and } eta textrm{ lies in } ( eta, eta+ mathrm{d} eta) textrm{, conditioned on } Theta textrm{ and } I.}}_{ textrm{Probability that the } j textrm{-th point is bad, with value } y_j, atop textrm{and all others are good, conditioned on } Theta textrm{ and } I.} end{align} Thus, we see that the pseudo-likelihood can be expressed as . begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= big( textrm{Probability all data is good} big) times big( textrm{Likelihood for all data} big) &amp; ,+ sum_j big( textrm{Probability only } x_j textrm{ is bad} big) times big( textrm{Likelihood without } x_j big) &amp; ,+ sum_{j&lt;k} big( textrm{Probability only } x_j textrm{ and } x_k textrm{ are bad} big) times big( textrm{Likelihood without } x_j textrm{ or } x_k big) &amp; ; vdots &amp; , + sum_j big( textrm{Probability only } x_j textrm{ is good} big) times big( textrm{Likelihood with only } x_j big) &amp; , + big( textrm{Probability all data is bad} big) end{align}which is just a weighted-average of the &quot;pure&quot; likelihoods $L( Theta)$, over all possible combinations of good/bad points, weighted over the corresponding prior probabilities. Note that the sums here are over non-repeating indices. . The Exponential Model . Above, we saw that we could express the likelihood as a sum of likelihoods for all possible combinations of good/bad points. Then, the pseudo-likelihood was obtained as an average over the hyper-parameters $u$ and $ eta$. . Consider the contribution to the likelihood form the term with exactly $M$ good points ($m=N-M$ bad points) begin{align} L^{(m)}( Theta, u, eta)&amp;= sum_{j_1 &lt; j_2 &lt; cdots&lt; j_m}u^{M}(1-u)^{m} L_{j_1, ldots,j_m}( Theta) B(y_{j_1} lvert Theta, eta) ldots B(y_{j_m} lvert Theta, eta) &amp;= sum_{j_1&lt;j_2&lt; cdots&lt; j_m} u^{M}(1-u)^{m} Big( prod_{i ne j_1, ldots,j_m} G(y_i lvert Theta) Big) Big(B(y_{j_1} lvert Theta, eta) cdots B(y_{j_m} lvert Theta, eta) Big) end{align} . Up to now we haven&#39;t really cared about which point are good/bad. However, we could just as well introduce additional hyperparameters $q_i$ to keep track of this information. Let begin{align} q_i = begin{cases} 1 &amp; textrm{point } i textrm{ is good} 0 &amp; textrm{point } i textrm{ is bad} end{cases} end{align} . Such that $q_{j_1} = q_{j_2}= cdots = q_{j_m}= 0$, $q_{i ne j_1, j_2, cdots j_m} = 1$, and $M= sum_i q_i$. The likelihood term we are considering then becomes begin{align} L^{(m)}( Theta, u, eta)&amp;= underbrace{ sum_{j_1&lt;j_2&lt; cdots&lt;j_m} sum_{q_1,q_2, ldots q_N} delta_{q_{j_1}, cdots,q_{j_m}}^{0} delta_{i ne j_1, cdots j_m}^{1}}_{ sum_{ {q_j }_ text{m bad}}} underbrace{ phantom{ prod_i^N}u^M(1-u)^m quad}_{P( {q_k } lvert Gamma, I)} underbrace{ prod_i^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{= P(D lvert Gamma, {q_k },I)} end{align} . allowing us to write down the full likelihood as begin{align} L( Theta, u, eta) &amp;= sum_{ {q_j }_{m=0}} u^{N-m}(1-u)^{m} prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp;+ sum_{ {q_j }_{m=1}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^q_i times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp; vdots &amp;+ sum_{ {q_j }_{m=N}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) L( Theta, u, eta) &amp;= sum_{q_1,q_2, ldots q_n} underbrace{ phantom{ Pi_i}u^{N-m} (1-u)^m quad}_{= P( {q_i } lvert Theta, u, eta, I)} underbrace{ prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{P(D lvert {q_i }, Theta, u, eta) = L( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots,q_N}P( {q_i } lvert Theta, u, eta, I) times P(D lvert {q_i }, Theta, u, eta) end{align} . Recall the definition of the posterior begin{align} P( Theta, u, eta lvert D, I)&amp; propto L( Theta, u, eta) Pi( Theta, u, eta) &amp;= sum_{q_1, ldots q_N} L( {q_i }, Theta, u, eta) underbrace{P( {q_i } lvert Theta, u, eta, I) P( Theta, u, eta lvert I)}_{= P( {q_i }, Theta, u, eta lvert I) equiv Pi( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots q_N} L( {q_i }, theta, u, eta) Pi( {q_i }, Theta, u, eta) &amp; propto sum_{q_1, ldots q_N} P( Theta, u, eta, {q_i } lvert D, I) end{align} Thus, we see that the posterior for the &quot;mixture&quot; model is nothing but the marginalized posterior (over $q_i$) of the &quot;exponential&quot; model, which considers the goodness/badness of each of the points explicitly (with a global purity value $u$). . Within the exponential model, we then have begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) end{align} . As always, it&#39;s more convenient to work with the logarithm of the likelihood begin{align} ln{L} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} ln{G(y_i lvert Theta)} + sum_{ text{bad}} ln{B(y_i lvert Theta, eta)} &amp;= sum_{ text{good}} ln{ left[ frac{1}{ sqrt{2 pi sigma_i^2}} exp left( frac{-(y_i - f(X_i; Theta))^2}{2 sigma_i^2} right) right]} + sum_{ text{bad}} ln{ left[ frac{1}{ sqrt{2 pi( sigma_i^2+ eta^2)}} exp{ left( frac{-(y_i-f(X_i; Theta))^2}{2( sigma_i^2+ eta^2)} right)} right]} &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . Numerical Example (Mixture Model) . We will use the data given in the paper by Hogg, Bovy and Lang. While they consider the case of correlated x/y noise, here we will simply consider the case of noise in $y$. . #import numpy as np #### If using windows... import jax import jax.numpy as np import numpy as onp import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano as th import theano.tensor as tt from scipy.optimize import minimize ### remove all jax stuff on windows... from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from matplotlib.collections import EllipseCollection from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib.collections as clt . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_sig2= quantiles_sig[1:-1] quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, *,nbins,scatter, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2, nbins) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) def colorbar(axes, mappable, *, loc=&quot;right&quot;, size=&quot;5%&quot;, pad=.1): &quot;&quot;&quot;Add colorbar to axes&quot;&quot;&quot; divider = make_axes_locatable(axes) cax = divider.append_axes(loc, size=size, pad=0.1) cb = plt.colorbar(mappable, cax=cax) return cb . # data taken from Hogg et al. x = np.array([201.,244, 47,287,203, 58,210,202,198,158,165,201,157,131,166,160,186,125,218,146], dtype=np.float64) y = np.array([592.,401,583,402,495,173,479,504,510,416,393,442,317,311,400,337,423,334,533,344], dtype=np.float64) σx = np.array([ 9, 14, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 5, 6, 6, 5, 9, 8, 6, 5], dtype=np.float64) σy = np.array([ 61, 25, 38, 15, 21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22], dtype=np.float64) ρxy= np.array([-0.84, 0.31, 0.64, -0.27, -0.33, 0.67, -0.02, -0.05, -0.84, -0.69, -.30, -0.46, -0.03, 0.50, 0.73, -0.52, 0.90, 0.40, -0.78, -0.56], dtype=np.float64) i_sort = np.argsort(x) x_sorted = x[i_sort] def computeErrors(): # Compute full sigma matrix Σ = onp.zeros((len(y), 2, 2)) Σ[:,0,0] = σx**2 Σ[:,0,1] = Σ[:,1,0] = σx*σy*ρxy Σ[:,1,1] = σy**2 Σinv = np.array([np.linalg.inv(s) for s in Σ]) # Diagonalize n = len(Σ) λ,η,θ = onp.zeros((n,2)), onp.zeros((n,2,2)), onp.zeros(n) for i, σ in enumerate(Σ): w, v = onp.linalg.eig(σ) # unordered eigenvalues and eigenvectors w, v = onp.real_if_close(w), onp.real_if_close(np.transpose(v)) w, v = zip(*sorted(zip(w,v), reverse=True)) # descending order λ[i,:] = onp.array(w) η[i,:,:]= onp.array(v) θ[i] = onp.arctan2(v[0][1], v[0][0])*180/np.pi # CCW angle with respect to x-axis of major axis (eigen-vector) return np.array(λ),np.array(η),np.array(θ),np.array(Σ) λ,η,θ,Σ = computeErrors() . The full covariance matrix for the data given by Hogg et al. can be computed as begin{align*} Sigma^2 &amp;= begin{pmatrix} sigma_x^2 &amp; rho_{xy} sigma_x sigma_y rho_{xy} sigma_x sigma_y &amp; sigma_y^2 end{pmatrix} end{align*} with the values of $ sigma_x$, $ sigma_y$, and $ rho_{xy}$ given in the paper. . Let&#39;s plot the full data, using ellipses to represent the $1 sigma$ contours. . fig,ax = plt.subplots(figsize=(16,9)) ax.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;]) ec = EllipseCollection(2*np.sqrt(λ[:,0]), 2*np.sqrt(λ[:,1]), θ, units=&#39;xy&#39;, offsets=np.column_stack((x,y)), transOffset=ax.transData, edgecolor=color[&#39;dark&#39;], facecolor=&#39;None&#39;) ax.add_collection(ec); ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . Ignoring the correlated erros, we consider only the case of measurement errors in $y$, given by $ sigma_y$. The data now looks like this . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x,y,σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . (Naive) Least-Squares . Before trying our Bayesian solution, let us consider a naive (least-squares) fit to the data... . def func(x,θ): return θ[0]*x + θ[1] @jit def chi2(θ): return np.sum((y-func(x,θ))**2/σy**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 fit = minimize(chi2, np.ones(2), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) fit_ihess = np.linalg.inv(hess_chi2(fit[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 289.963723 Iterations: 13 Function evaluations: 15 Gradient evaluations: 15 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(50,300) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange,fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.legend() plt.show() . This is clearly not what we want. . Least-Squares extension . Now let&#39;s consider the (marinalized) mixture model, assuming uniform priors, all we need is to maximize the likelihood begin{align} L( Gamma) &amp;= P(D lvert Gamma, I) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . def func(x,θ): return θ[0]*x + θ[1] def normal(x,μ,σ2): return 1.0/np.sqrt(2*np.pi*σ2)*np.exp(-(x-μ)**2/(2*σ2)) @jit def loss(Γ): # = ln L θ,u,η = Γ[:2],Γ[2],np.exp(Γ[3]) # Γ = (Θ, u, ln(η)) ymodel = func(x,θ) loss_g = normal(y, ymodel, σy**2) loss_b = normal(y, ymodel, σy**2 + η**2) return -np.sum(np.log(u*loss_g + (1-u)*loss_b)) grad_loss = jit(grad(loss)) # nabla ln(L) hess_loss = hessian(loss) # nabla nabla ln(L) bounds = ((None,None), (None,None), (0,1), (None,None)) opt3 = minimize(loss, np.array([1.0, 1.0, 0.5, 1.0]), method=&#39;L-BFGS-B&#39;, bounds=bounds, options={&#39;maxiter&#39;:500}) opt2 = minimize(loss, opt3[&#39;x&#39;], method=&#39;TNC&#39;, bounds=bounds, jac=grad_loss, options={&#39;maxiter&#39;:200, &#39;disp&#39;:1}) opt = minimize(loss, opt2[&#39;x&#39;], method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 print(opt3[&#39;x&#39;]) print(opt2[&#39;x&#39;]) print(opt[&#39;x&#39;]) . Optimization terminated successfully. Current function value: 110.170025 Iterations: 1 Function evaluations: 3 Gradient evaluations: 3 [ 2.2374954 35.27694601 0.80797022 5.63764122] [ 2.23749529 35.27693403 0.80796973 5.63763495] [ 2.2374953 35.27693402 0.80796886 5.63763506] . Recall that within the Laplace approximation, the (posterior) average is given by the optimum, and the variance estimates are obtained from the diagonal of the inverse Hessian matrix (evaluated at the optimum value). . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot;μ_{l} = {avg:.3e}; σ_{l} = {sig:.3e} t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e}&quot;) . μ_m = 2.237e+00; σ_m = 1.100e-01 2.018e+00 &lt; m &lt; 2.457e+00 μ_b = 3.528e+01; σ_b = 1.860e+01 -1.930e+00 &lt; b &lt; 7.248e+01 μ_u = 8.080e-01; σ_u = 1.032e-01 6.015e-01 &lt; u &lt; 1.014e+00 μ_logη = 5.638e+00; σ_logη = 3.922e-01 4.853e+00 &lt; logη &lt; 6.422e+00 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(x_sorted[0],x_sorted[-1]) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.set_ylim(-100,800) ax.legend() plt.show() . This is much better, and all we had to do was introduce two additional hyper-parameters, the data purity $u$, and the bad variance $ eta$. . MC solution . Now let&#39;s see how an MC simulation for this mixture model performs . begin{align} P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} Pi( Gamma) &amp;= P(m,b u, eta lvert I) end{align}We will assume that the priors are independent of each other. For simplicity, we will also assume uniform priors for all parameters except $ eta$, for which we use Jeffreys&#39; prior begin{align} P(m,b,u, eta lvert I)&amp;= P(m lvert I) times P(b lvert I) times P(u lvert I) times P( eta lvert I) P(m lvert I)&amp;: textrm{Uniform} P(b lvert I)&amp;: textrm{Uniform} P( eta lvert I)&amp; propto frac{1}{ eta} end{align} . Finally, we note that PYMC requires us to specify log-likelihood $ ln{L( Gamma)}$ for our problem . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u): r2 = (y_obs - y_g)**2 G = tt.exp(-r2/(2*σ_obs**2)) / (tt.sqrt(2*onp.pi*σ_obs**2)) B = tt.exp(-r2/(2*(σ_obs**2 + η**2))) / (tt.sqrt(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(tt.log(u*G + (1-u)*B)) with pm.Model() as model_mix: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u}) # make sure there are no divergences with initial point for RV in model_mix.basic_RVs: print(RV.name, RV.logp(model_mix.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 likelihood -162.7700867258116 . with model_mix: trace_mix = pm.sample(10000, tune=20000, progressbar=True, random_seed = 1123581321) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logη, u, b, m] INFO:pymc3:NUTS: [logη, u, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 60000/60000 [01:03&lt;00:00, 950.70draws/s] The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. . pm.traceplot(trace_mix) plt.show(); . pm.plot_posterior(trace_mix, var_names=[&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_mix, var_names=[&#39;u&#39;, &#39;logη&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo,hi = pm.stats.hpd(trace_mix[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (Laplace)&quot;) print(f&quot; t t {lo:.3e} &lt; {l} &lt; {hi:.3e} t (MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (MC) -1.930e+00 &lt; b &lt; 7.248e+01 (Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (MC) 6.015e-01 &lt; u &lt; 1.014e+00 (Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (MC) . We see that the Laplace approximation gave us a pretty good estimate of the reliability! . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_mix[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Mixture Model MC&#39;) ax.legend(loc=4) plt.show() . Here we show our predictions for $y$, obtained from the posterior distribution given by the MC simulation, i.e., begin{align} langle y(x) rangle &amp;= int textrm{d} Theta f(x; Theta) p( Theta lvert D, I) end{align} where the shaded regions are showing the $ pm sigma$ and $ pm 2 sigma$ intervals. Note that this could also have been computed within the Laplace approximation. The benefit of the MC is that such averages are trivial to compute from the trace of the simulation. Again, we see excellent agreement between the MC predictions and the fit to the mixture-model. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace_mix[&#39;b&#39;], trace_mix[&#39;m&#39;], scatter=True, nbins=40, colors=[color[&#39;mid&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) ax.set_xlim(-100,100) ax.set_ylim(1.5, 3.0) plt.show() . Numerical Example (Exponential Model) . MC solution . We are now ready to try the MC solution to the full exponential model. Recall that in this case begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) ln{L( Gamma)} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . We use the same priors as above for $ Theta$, $u$, $ eta$, and use a Bernoulli distribution for the $ {q_k }$, such that . begin{align} Pi( {q_k }, u) &amp;= P( {q_k }, u lvert I) &amp;= P( {q_k } lvert u, I) P(u lvert I) &amp; propto P( {q_k } lvert u, I) &amp;= prod_k P(q_k lvert u, I) &amp;= prod_k u^{q_k} times left(1-u right)^{1-q_k} &amp;= u^M (1-u)^{N-M} end{align}with $M= sum_k q_k$ . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u, q): r2 = (y_obs - y_g)**2 G = -0.5*q*(r2/σ_obs**2 + tt.log(2*onp.pi*σ_obs**2)) B = -0.5*(1-q)*(r2/(σ_obs**2 + η**2) + tt.log(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(G + B) with pm.Model() as model_exp: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) q = pm.Bernoulli(&#39;q&#39;, p=u, shape=x.shape) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u, &#39;q&#39;:q}) # make sure there are no divergences with initial point for RV in model_exp.basic_RVs: print(RV.name, RV.logp(model_exp.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 q -13.862943611198906 likelihood -148.95080195551284 . with model_exp: trace_exp = pm.sample(20000, tune=20000, progressbar=True, random_seed = 1123581321) . Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) CompoundStep INFO:pymc3:CompoundStep &gt;NUTS: [logη, u, b, m] INFO:pymc3:&gt;NUTS: [logη, u, b, m] &gt;BinaryGibbsMetropolis: [q] INFO:pymc3:&gt;BinaryGibbsMetropolis: [q] Sampling 2 chains, 0 divergences: 100%|██████████| 80000/80000 [02:13&lt;00:00, 600.12draws/s] The number of effective samples is smaller than 10% for some parameters. WARNING:pymc3:The number of effective samples is smaller than 10% for some parameters. . pm.traceplot(trace_exp, var_names=[&#39;m&#39;, &#39;b&#39;,&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_exp, [&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;q&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo0,hi0 = pm.stats.hpd(trace_mix[l]) lo,hi = pm.stats.hpd(trace_exp[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (mixture model - Laplace)&quot;) print(f&quot; t t {lo0:.3e} &lt; {l} &lt; {hi0:.3e} t (mixture model - MC)&quot;) print(f&quot; t t {lo :.3e} &lt; {l} &lt; {hi :.3e} t (exponential model - MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (mixture model - Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (mixture model - MC) 2.002e+00 &lt; m &lt; 2.469e+00 (exponential model - MC) -1.930e+00 &lt; b &lt; 7.248e+01 (mixture model - Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (mixture model - MC) -4.059e+00 &lt; b &lt; 7.498e+01 (exponential model - MC) 6.015e-01 &lt; u &lt; 1.014e+00 (mixture model - Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (mixture model - MC) 5.566e-01 &lt; u &lt; 9.479e-01 (exponential model - MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (mixture model - Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (mixture model - MC) 4.961e+00 &lt; logη &lt; 6.652e+00 (exponential model - MC) . As expected, the marginalized distributions for the model parameters $m$, $b$, $u$, and $ eta$ are essentially the same, regardless of whether we look at the mixture model or the exponential model. . The Good, the Bad, and the Maybes . The benefit of using the exponential model, with additional hyper-parameters for the good/bad state of each point, is that we can make inferences about the state of the points. Here, we are again following Jayne&#39;s (see Chapter 4, Elementary Hypothesis Testing). The odds for a given hypothesis, in this case, the odds for point $i$ to be a good point is defined as . begin{align} O(q_i = 1 | D, I) &amp; equiv frac{P(q_i = 1 | D, I)}{P(q_i = 0 | D, I)} end{align}where $P(q lvert D, I)$ is given by marginalization of the full posterior begin{align} P(q_i lvert D, I) &amp;= int mathrm{d} Theta mathrm{d}u mathrm{d} eta , sum_{ {q_j }_{j ne i}} P( Theta, u, eta, {q_j } lvert D, I) end{align} . Since the points generated from the MC simulation are drawn in proportion to the full posterior, the marginalized distribution for $q_i$ is obtained by simply taking the trace over all other parameters. That is, we can approximate $P(q_i lvert D, I)$ from the histogram of the $q_i$ values themselves. . It&#39;s more convenient to look at this quantity in decibels, begin{align} e(q_i = 1 | D, I) &amp; equiv 10 log_{10}{O(q_i = 1 | D I)} end{align} Jayne&#39;s calls $e$ the evidence, but this conflicts with the (somewhat widespread) use of evidence to denote the normalization constant (partition function) in the definition of the posterior. We&#39;ll just call it $e$. . belief = pd.DataFrame({&#39;P_good&#39;:onp.array([1/2, 2/3, 4/5, 10/11, 0.954, 100/101, 0.999, 0.9999])}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.500000 | 1.00000 | 0.000000 | . 1 0.666667 | 2.00000 | 3.010300 | . 2 0.800000 | 4.00000 | 6.020600 | . 3 0.909091 | 10.00000 | 10.000000 | . 4 0.954000 | 20.73913 | 13.167905 | . 5 0.990099 | 100.00000 | 20.000000 | . 6 0.999000 | 999.00000 | 29.995655 | . 7 0.999900 | 9999.00000 | 39.999566 | . This provides three scales for measuring our degrees of belief in $q_i$: $p(q_i =1)$, $O(q_i = 1)$, and $e(q_i = 1)$. It turns out we have a better intuition for $e$, with $3$db corresponding to a factor of 2 (in the odds), $10$db to a factor of 10, and $13$db to a probability of $95%$ (i.e. the $2 sigma$ criteria). . Let&#39;s calculate these quantities for each of the data points. The probability that a given $q_i$ is good is simply the fraction of points (in the parameter space) where $q_i = 1$, regardless of the values of the other $q_j$ (or the other parameter values). . fig, ax = plt.subplots() ax.plot(trace_exp[&#39;q&#39;][::100,0], marker=&#39;o&#39;, ls=&#39;None&#39;, ms=4, color=color[&#39;dark&#39;]); ax.set_xlabel(&#39;Trace&#39;) ax.set_ylabel(r&#39;$q_0$&#39;) plt.show() . This is what the trace for $q_0$ looks like (note that we only show every 100 points) . belief = pd.DataFrame({&#39;P_good&#39;:onp.average(trace_exp[&#39;q&#39;], axis=0)}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.778600 | 3.516712 | 5.461368 | . 1 0.000400 | 0.000400 | -33.977663 | . 2 0.000000 | 0.000000 | -inf | . 3 0.000350 | 0.000350 | -34.557799 | . 4 0.966475 | 28.828486 | 14.598218 | . 5 0.945725 | 17.424689 | 12.411650 | . 6 0.943750 | 16.777778 | 12.247344 | . 7 0.944525 | 17.026138 | 12.311161 | . 8 0.934400 | 14.243902 | 11.536290 | . 9 0.904800 | 9.504202 | 9.779156 | . 10 0.970875 | 33.334764 | 15.228974 | . 11 0.875375 | 7.024072 | 8.465890 | . 12 0.873675 | 6.916089 | 8.398606 | . 13 0.952650 | 20.119324 | 13.036134 | . 14 0.955125 | 21.284123 | 13.280558 | . 15 0.841175 | 5.296238 | 7.239675 | . 16 0.938275 | 15.200891 | 11.818690 | . 17 0.954950 | 21.197558 | 13.262858 | . 18 0.963750 | 26.586207 | 14.246564 | . 19 0.959125 | 23.464832 | 13.704174 | . Notice that there are only three negative evidence values, corresponding to the three obvious outlier. This means that all other points are more likely to be good than bad ($p&gt;1/2$). If we want, we can now label our points to show our belief in their good/bad state. Note, that this is purely a cosmetic procedure, our estimate for the model parameters, $m$ and $b$, is obtained by averaging over all possible configurations of the system, consistent with our measured data and prior information. . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;superfine&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, alpha=0.6, mec=&#39;k&#39;, mew=1.5, lw=1.5) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_exp[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Exponential Model MC&#39;) im = ax.scatter(x,y,c=np.clip(belief[&#39;e_good&#39;].values, a_min=-10, a_max=10), cmap=&#39;coolwarm&#39;) ax.legend(loc=4) cb = colorbar(ax,im) cb.set_label(r&#39;$e(q_i = 1)$&#39;) plt.show() . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "relUrl": "/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Bayesian Parameter Estimation",
            "content": "Motivation / Disclaimer . This is the first in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . We start with the basics of Bayesian Data Analysis, following the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling. Dr. Sivia is a research scientist at the Rutherford Appleton Lab and Prof. Skilling was at the DAMPT (Cambridge) before becoming a founding director of the Maximum Entropy Data Consultants group. The book is written by/for physicist and includes advanced topics (such as Maximum Entropy and Nested Sampling) that we hope to cover in future posts. . References . While mainly following Sivia&#39;s book, we have also found the following references extremely helpful . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | Bayesian Data Analysis, Third Edition. A. Gelman, J. B. Carlin, H. Stern et al., Chapman &amp; Hall/CRC Texts in Statistical Science (2013) | &quot;Frequentism and Bayesianism&quot; Blog-Post series (I, II, III, IV, IV) by Jake VanderPlas | Michael Betancourt&#39;s writings on probability theory. | . Preliminaries . Basic Rules . The probability $P(X lvert I)$ expresses our belief that $X$ is true given prior information $I$. The two basic rules of probability theory are just the sum and product rules, $$ begin{align} P(X lvert I) + P( overline{X} lvert I) &amp;= 1 &amp; textrm{(Sum Rule)} label{e:sum_rule} P(X,Y lvert I) &amp;= P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Product Rule)} notag &amp;=P(Y lvert X, I) times P(X lvert I) label{e:product_rule} end{align}$$ where $ overline{X}$ denotes the negation of $X$. See Sivia or Jaynes for a derivation, originally due to Cox, which shows that these are the only rules that will guarantee consistency. . From the product rule, one can easily derive Bayes&#39; Theorem $$ begin{align*} overbrace{P(X lvert Y, I)}^{ small{ mathrm{posterior}}} &amp;= frac{ overbrace{P(Y lvert X,I)}^{ small{ mathrm{likelihood}}} times overbrace{P(X lvert I)}^{ small{ mathrm{prior}}}}{ underbrace{P(Y lvert I)}_{ small{ mathrm{evidence}}}}&amp; textrm{(Bayes&#39; Theorem)} label{e:bayes} end{align*}$$ which states that the &quot;posterior&quot; is proportional to the &quot;likelihood&quot; times the &quot;prior&quot;, with the &quot;evidence&quot; a normalization constant. Posterior, likelihood and prior are the standard terms for these quantities, but the &quot;evidence&quot; label seems not to be widespread (we are following Sivia&#39;s nomenclature here). Note that there is nothing special about these labels, one&#39;s prior can be another&#39;s posterior, and your posterior today can be your prior tomorrow. Basically, the posterior is what we want to calculate, but can&#39;t (at least not directly), the prior is what we start with, and the likelihood is something we can easily calculate. . The sum rule can be extended to the to a series of mutually exclusive and exhaustive set of propositions $ {Y_k }$, such that $ sum_k P(Y_k lvert I) = 1$, and in the continuum limit we obtain the following marginalization property . $$ begin{align*} P(X lvert I) &amp;= int textrm{d}Y P(X, Y lvert I) = int textrm{d}Y P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Marginalization)} label{e:marginalization} end{align*}$$This is incredibly useful when there are unknown quantities that are required to compute the likelihood, but which are not really of interest to us. These so-called nuisance parameters can then be introduced and integrated out. We are being a bit careless here, and intermixing probabilities with probability densities. Strictly speaking, if $Y$ is a real random variable we should say that the probability for $Y$ to be within the range $y le Y le y+ Delta y$ is . $$ begin{align*} P(y le Y le y + Delta y)= rho(Y=y) Delta t end{align*}$$with $ rho$ the probability density function. However, following Sivia we will use the same symbol for both quantities, as it should be apparent from the context which one we are referring to. . Bayesian Data Analysis . It is easier to recognize how this Bayesian framework fits within a Data Analysis problem if we rewrite Bayes theorem in the form . $$ begin{align*} P( textrm{Hypothesis} lvert textrm{Data}, I) &amp;= frac{P( textrm{Data} lvert textrm{Hypothesis}, I) times P( textrm{Hypothesis} lvert I)}{P( textrm{Data}, I)} label{e:bayes_hypothesis} end{align*}$$or as . $$ begin{align*} P( Theta lvert D, I) &amp;= frac{P(D lvert Theta, I) times P( Theta lvert I)}{P(D lvert I)} label{e:bayes_theta} &amp; propto P(D lvert Theta, I) times P( Theta lvert I) end{align*}$$where $ Theta$ denotes the parameters of our model, $D$ our measured experimental/simulation data. The questions we are looking to answer, which these posteriors allow us to formulate are the following: &quot;What does my data say about my hypothesis&quot; or &quot;What does my data say about the model parameters&quot;? For what follows, we will focus exclusively on the parameter estimation problem. . One shot or sequential analysis? . Assume that our experimental data consists of a series of $N$ measurements, $D = {D_k }_{k=1}^N$. Our prior information $I$ specifies the model we believe explains this data, and we want to infer the parameters of this model. . Thus, what we want is the posterior of $ Theta$ given $D$, which gives $$ begin{align*} P( Theta lvert D,I) &amp; propto P( {D_k }_{k=1}^N lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert D_1, Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=3}^N lvert Theta, I) times P(D_2 lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp; vdots &amp;= left( Pi_{k=1}^N P(D_k lvert Theta, I) right) times P( Theta lvert I) end{align*}$$ . In step $2$, we are using the product rule to peel off one of the $D_i$, then in step $3$, we assume that (given the prior and the model parameters), knowledge of $D_1$ does not give us any extra information regarding the probabilities of observing $ {D_k }_{k ge 2}$, which means that they are independent. This procedure can be repeated until the original (one-shot) likelihood, which quantifies the probability of obtaining the entire set of data, is written as a product of individual likelihoods. From this, it is easy to see that we can analyze the data in one-shot, or sequentially, as the measurements arrive. The two routes are equivalent. . To see this, consider the expression obtained after peeling off m elements from the original likelihood. The posterior is given as $$ begin{align*} P( Theta lvert D,I)&amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) big( Pi_{j=1}^{m} P(D_j lvert Theta, I) big) P( Theta lvert I) &amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) times P( Theta lvert {D_j }_{j=1}^m, I) end{align*}$$ where the second and third terms in the rhs of the first equation are nothing but the likelihood and the prior after obtaining the first $m$ measurements. Thus, we can analyze the first $m$ data points, obtaining a posterior $P( Theta lvert {D_j }_{j=1}^m, I)$, and then use this as a prior when analyzing the remaining $N-m$ points. . What we can never ever do, however, is to use the posterior $P( Theta lvert {D_j }_{j=1}^m, I)$ to re-analyze the $ {D_j }_{j=1}^m$ measurements! . Laplace&#39;s Approximation, Best Estimates and Error Bars . Everything we can say about our parameters, given the Data and prior information, is contained in the posterior $P( Theta lvert D, I)$. However, this quantity can be quite complicated to deal with, particularly when the number of parameters exceeds 3. Therefore, it can be useful to develop approximations to this posterior. In particular, if the distribution is simple enough we can try to approximate it with a Gaussian distribution. Then, all that are needed are the first two moments, i.e., the average and the variance. The average give the peak of the distribution (the most likely point) and the variance an estimate of its spread, and thus of the reliability of our estimate. . This approximation is commonly referred to as Laplace&#39;s approximation, and amounts to a Taylor expansion of the logarithm of the posterior around the maximum (truncated to second order). Consider the general multi-dimensional case with $p$ parameters, $ Theta = ( Theta^{1}, Theta^{2}, ldots, Theta^{p})$, and let $ Theta_0 = text{argmax}{P( Theta lvert D, I)}$ . $$ begin{align*} L &amp;= ln{P( Theta lvert D, I)} = ln P &amp;= L lvert_{ Theta_0} + nabla L lvert_{ Theta_0} cdot( Theta - Theta_0) + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) + mathcal{O} big(( Theta- Theta_0)^3 big) &amp; simeq L lvert_{ Theta_0} + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) end{align*}$$where $ nabla= nabla_{ Theta}$, $A^t$ denotes the transpose of $A$, $( cdot)$ a matrix-matrix multiplication, and $ nabla nabla L$ is the (Hessian) matrix of second derivatives. Notice that the first-order term vanishes, since $ nabla L lvert_{ Theta_0} = 0$ by definition. . We can then re-exponentiate this expression to obtain an approximation to our original posterior distribution $$ begin{align*} P( Theta lvert D,I) propto exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot left(- nabla nabla L lvert_{ Theta_0} right) cdot left( Theta- Theta_0 right) right]} end{align*}$$ . Multi-variate Gaussians . Recall the definition of a multi-variate Gaussian distribution for a random $d$-dimensional vector $A$, expressed as $A sim mathcal{N}( mu, Sigma)$ $$ begin{align*} mathcal{N}(A lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^d det{ Sigma}}} exp{ left[- frac{1}{2} left(A- mu right)^t cdot Sigma^{-1} cdot left(A- mu right) right]} left langle A right rangle &amp;= mu left langle(A^i - mu^i)(A^j - mu^j) right rangle &amp;= Sigma^{ij} int text{d}A mathcal{N}(A lvert mu, Sigma) &amp;=1 end{align*}$$ where $ langle cdot rangle$ denotes an average over $P_{ mathcal{N}}$. . Such distributions have many interesting properties. In particular, if A is partitioned into $B$ and $C$, $A= begin{pmatrix}B C end{pmatrix}$, the marginalization over $B$ ($C$), results in another multi-variate Gaussian distribution for $C$ ($B$) $$ begin{align*} P(B lvert mu, Sigma) &amp;= int textrm{d}C mathcal{N} left( left. begin{pmatrix}B C end{pmatrix} right lvert begin{pmatrix} mu_B mu_C end{pmatrix}, begin{pmatrix} Sigma_{BB} &amp; Sigma_{BC} Sigma_{CB}&amp; Sigma_{CC} end{pmatrix} right) &amp;= frac{1}{ sqrt{(2 pi)^{d_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} left(B - mu_B right)^t Sigma_{BB}^{-1} left(B - mu_B right) right]} &amp; equiv mathcal{N}( mu_{B}, Sigma_{BB}) end{align*}$$ This can be easily proved by completing the squares in the exponent of the joint distribution. . Thus, since each component of $A$ is itself described by a Gaussian distribution, the best estimate is given by $$ begin{align*} A &amp;= mu pm sigma sigma &amp;= sqrt{ text{diag}{ Sigma}} end{align*}$$ . Finally, we see that the Laplace approximation of the posterior distribution for $ Theta$ is nothing but a multi-variate Gaussian, with average and covariance matrix given by $$ begin{align*} mu&amp; rightarrow Theta_0 Sigma&amp; rightarrow (- nabla nabla L lvert_{ Theta_0})^{-1} end{align*}$$ . Bayesian Parameter Estimation . Fitting with (known) noise . Let&#39;s see how this Bayesian formalism can be applied in practice. Our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. . The posterior for $ Theta$, given the Data $D$ and the prior information $I$, is then $$ begin{align*} P( Theta lvert D, I) &amp; propto P(D lvert Theta, I) times P( Theta lvert I) &amp; propto Pi_k P(y_k lvert Theta, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{ big(y_k - f(X_k; Theta) big)^2}{2 sigma_k^2} right]} end{align*}$$ where we have assumed that : . The prior $P( Theta lvert I)$ is a constant | The $y_k$ are independent of each-other (on knowing $ Theta$ and $I$), such that $P(y_i,y_j lvert Theta, I) = P(y_i lvert Theta, I) P(y_j lvert Theta, I)$ for $i ne j$. | The measurement error, and thus the likelihood, is Gaussian with variance $ sigma_k^2$ | . Let&#39;s see what the best-estimate would be in this case. . In practice, it will be more convenient to work with the logarithm of the probabilities, than with the probabilities themselves. Since $ ln$ is a monotonically increasing function, the maximum of $P$ will coincide with the maximum of $ ln{P}$. The log posterior is given by . $$ begin{align*} L &amp;= textrm{constant} - sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma_k^2} &amp;= textrm{constant} - frac{1}{2} chi^2 end{align*}$$where $ chi^2= sum_k(y_k - f_k)^2/ sigma_k^2$ is the sum of the squared residuals, weighted by the inverse of the error, and all terms not depending on $ Theta$ have been absorbed into the normalization constant. . The best estimate $ Theta_0$, which maximizes the posterior, is found by setting the gradient of $L$ to zero, . $$ begin{align*} nabla L = - frac{1}{2} nabla chi^2 = 0 Longrightarrow nabla chi^2 = 0 end{align*}$$which coincides with setting the gradient of $ chi^2$ to zero. Thus, in this case, maximizing the posterior is equivalent to a least-squares fit! . Finally, our reliability estimate is obtained by computing the matrix of second derivatives, evaluated at the optimal value $ Theta_0$ $$ begin{align*} Sigma_{ Theta}^{-1}=- nabla nabla L lvert_{ Theta_0} = frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0} end{align*}$$ . Approximating our posterior as a Gaussian, our best estimate for the model parameters would be $$ begin{align*} Theta = Theta_0 pm sigma_{ Theta} ; qquad left( sigma_{ Theta}= sqrt{ text{diag}( Sigma_{ Theta})} right) end{align*}$$ . Fitting with uknown noise . What happens in the case where don&#39;t have an estimate of the error bars? How would we derive the best-estimate in this case? What quantity should we minimize? . The solution is simply, use marginalization and average over all possible values of $ sigma$ (assumed to be the same for all points) $$ begin{align} P( Theta lvert D, I) &amp;= int textrm{d} sigma , P( Theta, sigma lvert D, I) &amp; propto int textrm{d} sigma , P(D lvert Theta, sigma, I) times P( Theta, sigma lvert I) end{align}$$ where the last step uses Bayes&#39; to write the integrand as the likelihood and the prior. Notice that this is the average of $P(D lvert Theta, sigma, I)$ over $P( Theta, sigma lvert I)$. . Under the same simplifying assumptions as above, i.e., independent measurements, uniform prior for $ Theta$, and Gaussian noise, we can again decompose the likelihood of $D$ into a product of likelihoods for the individual measurements $$ begin{align} P( Theta lvert D, I) &amp; propto int_0^ infty textrm{d} sigma left( Pi_k frac{1}{ sqrt{2 pi} sigma} exp{ left[- frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} right]} right) times frac{1}{ sigma} &amp;= (2 pi)^{-N/2} int_0^ infty frac{ textrm{d} sigma}{ sigma} sigma^{-N} exp{ left[- frac{1}{2 sigma^2} sum_k left(y_k - f(X_k; Theta) right)^2 right]} end{align}$$ . Here we have used Jeffreys&#39; prior for $ sigma$, $P( sigma lvert I) propto 1/ sigma$, as it is a scale-parameter. Depending on your prior information (e.g., do you know the order of magnitude?), there may be more appropriate priors, but we leave that for a future discussion. See Sivia and or Jaynes for a more in depth review of such issues. In practice, if $N$ is large enough this will not make a big difference, choosing Jeffreys prior or a constant prior for $ sigma$ will give essentially the same predictions. . The integral over $ sigma$ can be done analytically through the following variable transformation, with $S = sum_k (y_k - f_k)^2$ $$ begin{align} t &amp;= frac{ sum_k left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} = frac{S}{2 sigma^2} frac{ textrm{d} sigma}{ sigma} &amp;= - frac{ textrm{d}t}{2t} end{align}$$ . The posterior is then $$ begin{align} P( Theta lvert D, I) &amp; propto (2 pi)^{-N/2} int_0^ infty frac{ textrm{d}t}{2t} left( frac{2t}{S} right)^{N/2} e^{-t} &amp;= frac{ pi^{-N/2}}{2} underbrace{ left( int_0^ infty frac{ textrm{d}t}{t} t^{N/2} e^{-t} right)}_{= Gamma(N/2)}S^{-N/2} end{align}$$ . where the quantity in parenthesis is independent of $ Theta$ and equal to the Gamma function of $N/2$. . As before, let&#39;s look at the log-posterior . $$ begin{align} L &amp;= textrm{constant} - frac{N}{2} ln{S} end{align}$$The best-estimate is again that which maximizes $L$ $$ begin{align} nabla L &amp;= - frac{N}{2} frac{ nabla S}{S}= 0 Longrightarrow nabla S = 0 end{align}$$ which in this case corresponds to minimizing $ ln{S}$ or just $S$. So far it seems that the procedure is the same, minimize the sum of the squared residuals (weighted by the magnitude of the error, if known). The differences come when we consider the reliability estimate. . The Hessian is now $$ begin{align} nabla nabla L &amp;= - frac{N}{2} left[ frac{ nabla nabla S}{S} - frac{ left( nabla S right) left( nabla S right)}{S^2} right] end{align}$$ . which reduces to the following when evaluated at the optimum point $ Theta_0$ (since $ nabla S lvert_{ Theta_0} equiv 0$) $$ begin{align} nabla nabla L lvert_{ Theta_0} &amp;= - frac{N}{2} frac{ nabla nabla S lvert_{ Theta_0}}{S_0} = - frac{1}{2} left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} end{align}$$ . Within the Laplace approximation, the covariance matrix for the posterior distribution of $ Theta$ is then given by $$ begin{align} Sigma_ Theta^{-1} &amp;= frac{1}{2} left( left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} right) end{align}$$ . This expression is very similar to the one we obtained above, in the case where the measurement error $ sigma$ was known, $ Sigma_ theta^{-1}=- frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0}$. Here, we have replaced $ sigma_k^2$ with an estimate derived from the data $S_0/N$ . $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{ sigma_k^2} &amp; longrightarrow frac{S}{S_0/N} = frac{N}{S_0} sum_k left(y_k - f(X_k; Theta) right)^2 sigma_k^2 &amp; longrightarrow frac{S_0}{N} = frac{1}{N} sum_{k} left(y_k - f(X_k; Theta_0) right)^2 end{align}$$However, $S_0/N$ is not necessarily the best estimate for the amplitude of the noise, as we will see below. . ... about the noise . What does the data say about the noise, i.e., what is $P( sigma lvert D, I)$? . To be able to compute this quantity, we again make use of marginalization, treating the model parameters as nuisance parameters and integrating over them. $$ begin{align} P( sigma lvert D, I) &amp;= int textrm{d} Theta P( sigma, Theta lvert D, I) &amp; propto int textrm{d} Theta P(D lvert sigma, Theta, I) P( sigma, Theta lvert I) end{align}$$ . Assuming $ sigma$ and $ Theta$ are independent, $P( sigma, Theta lvert I)= P( sigma lvert I) P( Theta lvert I)$, and taking a uniform prior for $ Theta$ and Jeffreys prior for $ sigma$, we have $$ begin{align} P( sigma lvert D, I) propto sigma^{-(N+1)} int textrm{d} Theta exp{ left[- frac{S( Theta)}{2 sigma^2} right]} end{align}$$ where $S$ is still the sum of squared residuals $S = sum_k (y_k - f(X_k; Theta))$. We get one factor of $1/ sigma$ from the normalization constant of the likelihood for each data measurement, and one from the prior. . Without having to assume anything regarding the form of the model function $f(X)$, we can proceed by again using the Laplace approximation, now expanding $S( Theta)$ around the minimum $ Theta_0$ (which corresponds to the maximum of the posterior $P( Theta lvert D, I)$) $$ begin{align} S simeq S_0 + nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) + frac{1}{2} left( Theta - Theta_0 right)^t cdot nabla nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) end{align}$$ . The posterior $P( sigma lvert D, I)$ is then $$ begin{align} P( sigma lvert D, I)&amp; propto sigma^{-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot frac{ nabla nabla S lvert_{ Theta_0}}{2 sigma^2} cdot left( Theta- Theta_0 right) right]} &amp; propto sigma^{p-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} end{align}$$ where the $p$-dimensional integral over $ Theta$ gives us a factor of $ sqrt{(2 pi)^p det{ Sigma}_{ sigma}}$, with $ Sigma_ sigma^{-1} = - frac{1}{2 sigma^2} nabla nabla S lvert_{ Theta_0}$. . To find the best-estimate for $ sigma$ we maximize the log posterior $$ begin{align} L &amp;= ln{P( sigma lvert D, I)} = textrm{const} + left(p - N - 1 right) ln{ sigma} - frac{S_0}{2 sigma^2} frac{ textrm{d}L}{ textrm{d} sigma} &amp;= (p-N-1) frac{1}{ sigma} + frac{S_0}{ sigma^3} frac{ textrm{d}^2L}{ textrm{d}^2 sigma} &amp;= (N+1-p) frac{1}{ sigma^2} - frac{3 S_0}{ sigma^4} end{align}$$ . Setting $ text{d} L / text{d} sigma lvert_{ sigma_0} = 0$, we get $$ begin{align} sigma_0^2 = frac{S_0}{N+1-p} end{align}$$ . with the reliability estimates given by the inverse of the Hessian $$ begin{align} left. frac{ textrm{d}^2L}{ textrm{d}^2 sigma} right lvert_{ sigma_0} &amp;= frac{(N+1-p) sigma_0^2 - 3S_0}{ sigma_0^4} &amp;= frac{-2(N+1-p)}{ sigma_0^2} end{align}$$ . we get $$ begin{align} sigma = sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align}$$ . Example : Fitting a Straigh-Line (known measurement error) . Now, let&#39;s see how all this works out in practice. For convenience, we will use the jax library and take advantage of its automatic differentiation capabilities, which allow us to calculate the gradients and Hessians exactly (to machine precision). If you are on Windows this will not work, in that case remove all the jax commands. . Within the Laplace approximation, we can work out the best estimates for all our quantities by simply maximizing the appropriate posterior. However, to compare with the full solution, we will also make use of the PYMC3 library, which allows us to perform Hamiltonian Monte-Carlo simulations and generate samples which converge to those of the posterior. For the minimiztion, we make use of the scipy library. . Please install the following packages to run the code below . numpy | matplotlib | pymc3 | theano | scipy | jax | . #import numpy as np #### If using windows... import jax.numpy as np import numpy as onp import numpy.random as random import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano import theano.tensor as tt from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, scatter=False, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . We define a linear and quadratic function of a scalar variable $x$, which takes as first argument a list of parameters $ Theta$. We then define a routine to create noisy versions a function. . def linear_func(θ, x): return θ[0] + θ[1]*x # y = mx + b def quadratic_func(θ, x): return θ[0] + θ[1]*x + θ[2]*x**2 # y = a x^2 + bx + c def make_noisy_func(f, σ): &quot;&quot;&quot;Returns function that computes noisy measurements for y ~ N(f, σ^2), such that y will be normally distributed around f, with variance σ^2&quot;&quot;&quot; return lambda θ,x : f(θ,x) + σ*random.randn(len(x)) . Now let&#39;s simulate measuring using a noisy linear function (intercept $b=2$, slope $m=1$). We discretize $x$ over $N_p=10$ points evenly spaced in the range $[0,2]$, and simulate $N=8$ draws. Computing the average and variance over these $N$ sets of measurements gives us our best estimate and error bar for the value of $f$ at $x_i$. . random.seed(12345) nparams,func = 2, linear_func #nparams,func = 3, quadratic_func _θ0,σ0 = np.array([2.0, 1.0, 0.6]), 0.25 # exact parameter values θ0 = _θ0[:nparams] measurement = make_noisy_func(func, σ0) npoints,ndraws= 10,8 x_sample = np.linspace(0, 2.0, num=npoints) y_sample = np.array([measurement(θ0,x_sample) for i in range(ndraws)]) y_avg, y_err = np.average(y_sample, axis=0), np.std(y_sample, axis=0) fig, ax = plt.subplots() for yi in y_sample: ax.plot(x_sample, yi, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, label=&#39;Estimate&#39;, color=color[&#39;dark&#39;]) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . The symbols represent our &quot;measurement&quot;, 8 for each of the 10 $x$ values we are sampling. The average and variance of these measurements would give your experimental estimate (shown here with the error bar plot). . The &quot;Least-Squares&quot; route . Let&#39;s first attempt a least-squares minimization. . Recall the definition of $ chi^2$ $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(x_k; Theta) right)}{ sigma_k^2} end{align}$$ . For simple functions, it is trivial to work out the first and second derivatives of this quantity, but jax does all the work for us! As shown above, the best-estimate for $ Theta$ is given by minimizing $ chi^2$, and the reliability estimates are given by (twice) the inverse of the Hessian $$ begin{align*} Theta &amp;= Theta_0 pm sqrt{ text{diag}( Sigma_{ Theta})} Sigma_{ Theta}&amp;= 2 left( nabla nabla chi^2 lvert_{ Theta_0} right)^{-1} end{align*}$$ . For a the simple linear model we are considering, we can easily work out the analytical solution . $$ begin{align} partial_m chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right) x_k}{ sigma_k^2} qquad &amp; partial_b chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right)}{ sigma_k^2} nabla chi^2 &amp;= begin{pmatrix} partial_m chi^2 partial_b chi^2 end{pmatrix} = begin{pmatrix} sum w_k x_k^2 &amp; sum w_k x_k sum w_k x_k &amp; sum w_k end{pmatrix} begin{pmatrix} m b end{pmatrix} - begin{pmatrix} sum w_k x_k y_k sum w_k y_k end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} begin{pmatrix}m b end{pmatrix} - begin{pmatrix}p q end{pmatrix} end{align}$$ with $ alpha = sum w_k x_k^2$, $ beta = sum_k w_k$, $ gamma= sum_k w_k x_k$, $w_k = 2/ sigma_k^2$, $p= sum_k w_k x_k y_k$ and $q= sum_k w_k y_k$. The optimum solution is then . $$ begin{align*} nabla chi^2 &amp;= 0 begin{pmatrix}m b end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix}^{-1} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta- gamma^2} begin{pmatrix} beta p - gamma q alpha q - gamma p end{pmatrix} end{align*}$$Finally, the covariance matrix $ Sigma_{ Theta} = 2 ( nabla nabla chi^2)^{-1}$ is just . $$ begin{align*} nabla nabla chi^2 &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} Sigma_{ Theta} &amp;= 2 frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} end{align*}$$ @jit def chi2(θ): return np.sum((y_avg-func(θ,x_sample))**2/y_err**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 def exact_sol(xk,yk,dyk): wk = 2/dyk**2 α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_avg, y_err) . opt = minimize(chi2, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_chi2(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 1.118355 Iterations: 5 Function evaluations: 7 Gradient evaluations: 7 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.2f}]&#39;) . |θ - Θ_exact|_inf = 2.442491e-15 |H - H_exact|_inf = 3.469447e-18 |H_min - H_exact|_inf = 1.040834e-17 1.69 &lt; θ^1 (b) &lt; 2.24 [μ = 1.961, σ = 0.14] 0.79 &lt; θ^2 (m) &lt; 1.24 [μ = 1.018, σ = 0.11] . The exact values used to generate the data were $ Theta^1 = b = 2$ and $ Theta^2 = m = 1$, which is very close to the optimal value obtained here. Furthermore, we see that the approximate of the Hessian returned by the minimization routine coincides with the exact result (given by jax). We use $ pm 2 sigma$ interval as a measure of the error bounds. . The full Bayesian route . Now let&#39;s see how to get the &quot;full&quot; solution, not just a point-estimate. For this we use PYMC3 to generate samples from the posterior. To compare with our previous analysis, we will assume a uniform prior for both $m$ and $b$. . with pm.Model() as model: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) # true function y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) # measured data (accounting for noise) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=y_err, observed=y_avg) # generate samples from the prior (before looking at the data) prior = pm.sample_prior_predictive(samples=1000, random_seed = 123456) # generate samples from the posterior trace = pm.sample(5000, tune = 20000, progressbar=True) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [b, m] INFO:pymc3:NUTS: [b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 50000/50000 [00:20&lt;00:00, 2415.40draws/s] The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. . First, let us try to get an idea for how good/bad our priors are. We can do this by looking at the prior predictive distribution. This simply takes parameters values sampled from the prior, and pushes them through the model, generating corresponding samples for $y$. If we plot $y-b = mx$ we can clearly see that this prior is heavily skewed towards lines with high slopes. Is this what we want? Probably not, but this will depend on our prior information. . fig, ax = plt.subplots() for y_i,b_i in zip(prior[&#39;y&#39;], prior[&#39;b&#39;]): ax.plot(x_sample, y_i-b_i, alpha=0.2, color=&#39;k&#39;) ax.set_xlabel(r&#39;x&#39;) ax.set_ylabel(r&#39;y&#39;) ax.set_ylim(0,2) ax.set_xlim(0,2) plt.show() . PYMC3 comes with many useful post-processing functions to visualize our data. Let&#39;s look at a few of them. . pm.traceplot(trace) plt.show(); . Above, we see the distribution of the model parameters, with different different lines corresponding to different chains or independent runs of the MC simulation. At first glance, there doesn&#39;t seem to be any obvious divergence and we get nice Gaussian-like distributions for all model parameters (this includes the values of the function at the measured points). . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace, var_names=[&#39;m&#39;, &#39;b&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.show(); . &lt;Figure size 504x504 with 0 Axes&gt; . Here we have the $94 %$ credible intervals, corresponding to the $ mu pm 2 sigma$ interval in the case of a Gaussian distribution. The vertical lines indicate the real values that were used to generate the data. . pm.plot_posterior(trace, var_names=[&#39;m&#39;, &#39;b&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fff4d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fc8a10&gt;], dtype=object) . Here we have the posterior distribution for our two model parameters. Notice that the $94 %$ HPD (Highest Posterior Density Interval) corresponds exactly to that obtained using the Laplace approximation. This should not be surprising, when we see the posterior. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace[&#39;y&#39;], quantiles_sig) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, marker=&#39;o&#39;, color=color[&#39;superfine&#39;], alpha=0.5, label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], alpha=1, label=&#39;exact&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.legend() plt.show() . The shaded red regions show the $ sigma$, $2 sigma$ and $3 sigma $ regions $y$ obtained from the posterior samples of y. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace[&#39;b&#39;], trace[&#39;m&#39;], scatter=True, colors=[color[&#39;mid&#39;], color[&#39;light_highlight&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) plt.show() . Here we see the trace generated by the MC simulation, where the distribution of points approximates the the posterior distribution. Also drawn are the $ pm sigma$ and $ pm 2 sigma$ contour levels, which should contain $68 %$ and $95 %$ of the points, respectively. . We can redo the analysis using a &quot;better&quot; prior, e.g., assigning a uniform prior on the angles of the lines instead of their slope, but the results will not vary significantly (unless the number of data points is reduced). . Example : Fitting a straight line (with unknown measurement error) . The &quot;Least-Squares&quot; route . Now let&#39;s analyze the case where the measurement error is not known. As shown above, instead of minimizing $ chi^2$, we should maximize $L=- frac{N}{2} ln{S}$, with $S= sum_k (y_k -f_k)^2$, which coincides with minimizing $S$. Thus, the form of the solution is the same as before, we simply set $w_k=2 , ( sigma_k=1)$, as all the points are given equal weight. Recall that the covariance matrix is $ Sigma_ Theta^{-1} = frac{1}{2} nabla nabla frac{S}{S_0/N} lvert_{ Theta_0}$. . @jit def loss(θ): return np.sum((y_sample[0]-func(θ,x_sample))**2) grad_loss = jit(grad(loss)) hess_loss = hessian(loss) def exact_sol(xk,yk): wk = 2.0*np.ones_like(yk) α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_sample[0]) opt = minimize(loss, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) . Optimization terminated successfully. Current function value: 0.323150 Iterations: 4 Function evaluations: 6 Gradient evaluations: 6 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2*loss(opt[&#39;x&#39;])/npoints) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.3f}]&#39;) . |θ - Θ_exact|_inf = 2.220446e-16 |H - H_exact|_inf = 2.775558e-17 |H_min - H_exact|_inf = 4.163336e-17 1.77 &lt; θ^1 (b) &lt; 2.20 [μ = 1.984, σ = 0.106] 0.96 &lt; θ^2 (m) &lt; 1.32 [μ = 1.140, σ = 0.089] . Within the Gaussian approximation, we can also use this optimum solution to give an estimate of the unknown measurement error . $$ begin{align*} sigma_0^2 &amp;= frac{S_0}{N+1-p} sigma &amp;= sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align*}$$ σa = np.sqrt(loss(opt[&#39;x&#39;])/(npoints + 1 - nparams)) dσa= σa / np.sqrt(2*(npoints + 1 - nparams)) print(f&#39;{σa - 2*dσa:.2f} &lt; σ &lt; {σa + 2*dσa:.2f} [μ = {σa : .3f}, σ = {dσa : .3f}]&#39;) . 0.10 &lt; σ &lt; 0.28 [μ = 0.189, σ = 0.045] . Which is in good agreement with the exact value used to generate the noisy data ($ sigma = 0.25$). . fig, ax = plt.subplots() ax.plot(x_sample, y_sample[0], marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.plot(x_sample, func(opt[&#39;x&#39;],x_sample), ls=&#39;-&#39;, color=color[&#39;dark&#39;], alpha=1, label=&#39;Estimate&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . with pm.Model() as model_sig: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) logσ = pm.Uniform(&#39;logσ&#39;, lower=-6, upper=6) σ = pm.Deterministic(&#39;σ&#39;, tt.exp(logσ)) y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=σ, observed=y_sample[0]) prior_sig = pm.sample_prior_predictive(samples=1000, random_seed = 123456) trace_sig = pm.sample(5000, tune = 50000, progressbar=True) #post_sig = pm.sample_posterior_predictive(trace_uni, model=model_sig, random_seed=4938483) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logσ, b, m] INFO:pymc3:NUTS: [logσ, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 110000/110000 [01:08&lt;00:00, 1606.14draws/s] . plt.figure(figsize=(12,12)) pm.traceplot(trace_sig) plt.show(); . &lt;Figure size 864x864 with 0 Axes&gt; . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.axvline(σ0, 0, 1, c=&#39;C1&#39;) plt.show() . &lt;Figure size 504x504 with 0 Axes&gt; . pm.plot_posterior(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); . Notice how the resulting distribution for $ sigma$ is no longer symmetric. In this case we expect the Gaussian approximation will start to break down. However, the error bounds derived above, $0.1&lt; sigma &lt; 0.28$ are still close to the &quot;real&quot; values provided by the MC trace. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace_sig[&#39;y&#39;], quantiles_sig, label=&#39;prediction&#39;) ax.plot(x_sample, y_sample[0], ls=&#39;None&#39;, marker=&#39;o&#39;, mfc=&#39;None&#39;, mew=2, color=color[&#39;dark&#39;], label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], label=&#39;exact&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fe5629e4c90&gt; . Finally, here is the list of packages in my environment. . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "relUrl": "/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://johnjmolina.github.io/MLKyoto/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johnjmolina.github.io/MLKyoto/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://johnjmolina.github.io/MLKyoto/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://johnjmolina.github.io/MLKyoto/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}