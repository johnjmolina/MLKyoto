{
  
    
        "post0": {
            "title": "Gaussian Processes 1 - Theory",
            "content": "References . The standard textbook on Gaussian Processes (GP) is that of Rasmussen and Williams. The book by Murphy on Machine Learning also has a nice intro to GP and how they connect with other ML methods. Finally, the &quot;Matrix Cookbook&quot; has an extensive list of identities that are helpful for the GP derivations. . Gaussian Processes for Machine Learning. C. E. Rasmussen and C. K. I. Williams, Cambridge, the MIT Press (2006) | Machine Learning : A Probabilistic Perspective. K. P. Murphy, Cambridge, the MIT Press (2012) | The Matrix Cookbook. K. B. Petersen and M. S. Pedersen (2012) | . Preliminaries . Manipulating GP will require a bit of matrix algebra and the use of some not very well know identities (at least to the author). Thus, we will start by giving (without proof) the main results needed to derive the basic GP equations. . Let $ Sigma$ be a block matrix, defined as begin{align} Sigma &amp;= begin{pmatrix} A &amp; C D &amp; B end{pmatrix} end{align} . From the sub-matrices, define $E$ and $F$ as begin{align} E &amp;= A - C B^{-1} D F &amp;= B - D A^{-1} C end{align} . Determinant . The determinant of $ Sigma$ can be written in terms of that of $A$ and $F$, or $B$ and $E$, as . begin{align} det{ Sigma} &amp;= det{A} cdot det{F} = det{B} cdot det{E} end{align} Inverse . The matrix inverse of $ Sigma$ can also be expressed in block form as . begin{align} Sigma^{-1} &amp;= begin{pmatrix} widetilde{A} &amp; widetilde{C} widetilde{D} &amp; widetilde{B} end{pmatrix} &amp;= begin{pmatrix} E^{-1} &amp; - A^{-1} C F^{-1} -F^{-1} D A^{-1}&amp; F^{-1} end{pmatrix} &amp;= begin{pmatrix} A^{-1} + A^{-1} C F^{-1} D A^{-1} &amp; -E^{-1} C B^{-1} -B^{-1} D E^{-1} &amp; B^{-1} + B^{-1} D E^{-1} C B^{-1} end{pmatrix} end{align} Derivatives . When &quot;training&quot; our GP models, it will be useful to be able to compute derivatives of these block matrices with respect to the hyper-parameters $ Theta$. In particular, we will need the derivatives of the matrix inverse and the log of the determinant. These are given by . begin{align} frac{ partial}{ partial theta} A^{-1} &amp;= -A^{-1} frac{ partial A}{ partial theta} A^{-1} frac{ partial}{ partial theta} log{ left( det{A} right)} &amp;= text{tr}{ left(A^{-1} frac{ partial A}{ partial theta} right)} end{align} Symmetric Matrices . In the case that $ Sigma$ is a symmetric matrix, which is the only case we will be interested in here, $ Sigma = Sigma^{t}$, which in turn implies that $A=A^t$, $B = B^{t}$, $D = C^{t}$, the block form of the matrix inverse (also symmetric) can be written as . begin{align} Sigma^{-1} &amp;= begin{pmatrix} widetilde{A} &amp; widetilde{C} widetilde{C}^t &amp; widetilde{B} end{pmatrix} &amp;= begin{pmatrix} E^{-1} &amp; - A^{-1} C F^{-1} - F^{-1} C^{t} A^{-1} &amp; F^{-1} end{pmatrix} &amp;= begin{pmatrix} A^{-1} + A^{-1} C F^{-1} C^{t} A^{-1}&amp; - E^{-1} C B^{-1} -B^{-1} C^{t} E^{-1} &amp; B^{-1} + B^{-1} C^{t} E^{-1} C B^{-1} end{pmatrix} end{align}with . begin{align} widetilde{A}^{-1} = E &amp;= A - C B^{-1} C^{t} widetilde{B}^{-1} = F &amp;= B - C^{t} A^{-1} C end{align} The following form of the relations will be particularly useful for the derivations below begin{align} widetilde{C} &amp;= -E^{-1} C B^{-1} = - widetilde{A} C B^{-1} widetilde{B} &amp;= B^{-1} + B^{-1} C^t E^{-1} C B^{-1} = B^{-1} + B^{-1} C^t widetilde{A} C B^{-1} Longrightarrow widetilde{B} - B^{-1} = B^{-1} C^t widetilde{A}C B^{-1} end{align} . (Multi-variate) Gaussians / GP . Now we can turn our attention to GP. As defined in Rasmussen and Williams, a GP is &quot;a collection of random variables, any finite number of which have a joint Gaussian distribution&quot;. What does this mean? . Previously, for the Bayesian Parameter Estimation problem, we were given some data and a model (which on prior information was assumed to explain the data), and tasked with finding the distribution of the parameters that could explain the data. That is, we wanted to infer or learn the parameters from the data. However, this only works if we know the model. What happens when we don&#39;t posses this information? . This leads us to the much trickier problem of &quot;Non-parametric Bayesian Inference&quot;. Since we don&#39;t have a model to parametrize, we take the function values themselves to be the parameters! So it&#39;s not so much that there are no parameters, it&#39;s just that there is an infinite number of them. Instead of learning the parameters in some model, we will try to learn the function itself from the data. . In the specific case of GP, we assume that the value of the function at each point (e.g., $x(t)$) is a random variable, and that they are all correlated, with a joint Gaussian distribution. Thus, the joint probability distribution for the $x = (x(t_1), x(t_2), ldots x(t_n)) = (x_1, x_2, ldots x_n)$ is given by a multi-variate Gaussian, specified by some mean $ mu$ and (symmetric) covariance matrix $ Sigma$. We express this as . begin{align}x sim mathcal{N}( mu, Sigma) end{align} which is to be interpreted according to begin{align} p(x lvert mu, Sigma) &amp;= frac{1}{ sqrt{ det{ left(2 pi Sigma right)}}} exp{ left[- frac{1}{2} delta x^t Sigma^{-1} delta x right]} qquad left( delta x= x - mu right) int p(x lvert mu, Sigma) , mathrm{d}x &amp;= 1 end{align} . By definition the first and second moments are given by the average and covariance matrix . begin{align} langle x rangle equiv int x p(x lvert mu, Sigma) , mathrm{d}x &amp;= mu left langle delta x_i delta x_j right rangle equiv int delta x_i delta x_j p(x lvert mu, Sigma) , mathrm{d}x &amp;= Sigma_{ij} end{align} Digression on Gaussian Integrals . To compute the marginal and conditional distributions we will need to manipulate the quadratic expressions appearing in the exponential. . In particular, we will need to compute integrals of the form . begin{align} I(A, b, c) &amp;= int exp{ left[- frac{1}{2} x^t A x + x^t b + c right]} , mathrm{d}x end{align}where $A$ is a symmetric symmetric. This integral can be performed easily by completing the square, as follows . begin{align} - frac{1}{2} x^t A x + x^t b &amp;= - frac{1}{2} x^t A x + b^t x &amp;= - frac{1}{2} left[x^t A x - b^t x - x^t b right] &amp;= - frac{1}{2} left[x^t A^t x - b^t A^{-t} A^{t} x - x^t b right] &amp;= - frac{1}{2} left[ left(x^t - b^t A^{-t} right) A^t x - x^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - x^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - left(x - A^{-1} b + A^{-1} b right)^t b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A^t x - left(x - A^{-1} b right)^t b - b^t A^{-t} b right] &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A x - left(x - A^{-1} b right)^t A A^{-1}b right] + frac{1}{2} b^t A^{-1} b &amp;= - frac{1}{2} left[ left(x - A^{-1} b right)^t A left(x - A^{-1} b right) right] + frac{1}{2} b^t A^{-1} b end{align} where we have repeatedly used the fact that a scalar is by definition symmetric, so $ alpha= alpha^t$, $x^t y = y^t x$, $x^t A y = y^t A^t x$, and so on ($ alpha$ a scalar, $x$ and $y$ vectors, and $A$ a square matrix). . begin{align} I(A, b, c)&amp;= int exp{ left[- frac{1}{2}x^t A x + x^t b + c right] mathrm{d}x} &amp;= exp{ left[ frac{1}{2}b^t A^{-1} b + c right]} underbrace{ int exp{ left[- frac{1}{2} left(x - A^{-1}b right)^t A left(x - A^{-1} b right) right]} mathrm{d}x}_{ equiv sqrt{ det{ left(2 pi A^{-1} right)}}} &amp;= sqrt{ det{ left(2 pi A^{-1} right)}} exp{ left[ frac{1}{2} b^t A^{-1} b + c right]} &amp;= sqrt{ frac{(2 pi)^{n}}{ det{A}}} exp{ left[ frac{1}{2} b^t A^{-1} b + c right]} end{align} Marginalization . Now lets consider partitioning our set of points $x$ in two, $x_A$ and $x_B$, which could represent the (known) training data and (unknown) test data, respectively. The joint distribution, is given exactly by the expression above, but we can rewrite it in block form to highlight the contribution of the $x_A$ and $x_B$ points . begin{align} p(x lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^n det{ Sigma}}} exp{ left[- frac{1}{2} delta x^t Sigma^{-1} delta x right]} , , qquad Sigma = begin{pmatrix} Sigma_{AA} &amp; Sigma_{AB} Sigma_{AB}^t &amp; Sigma_{BB} end{pmatrix} &amp;= frac{1}{ sqrt{(2 pi)^n det{ Sigma}}} exp{ left[- frac{1}{2} begin{pmatrix} delta x_A delta x_B end{pmatrix}^t begin{pmatrix} widetilde{ Sigma}_{AA} &amp; widetilde{ Sigma}_{AB} widetilde{ Sigma}_{AB}^t &amp; widetilde{ Sigma}_{BB} end{pmatrix} begin{pmatrix} delta x_A delta x_B end{pmatrix} right]} end{align} Where we used the properties of block matrices to rewrite the inverse of $ Sigma$ into block form. . Given this joint distribution for $x_A$ and $x_B$, what can we say about the distribution for $x_B$, regardless of $x_A$? . By definition, we simply marginalize over $x_A$ begin{align} p(x_B lvert mu, Sigma) &amp;= int p(x_A,x_B lvert mu, Sigma) mathrm{d}x_A end{align} . To evaluate this integral, lets rewrite the terms appearing in the exponent, trying to separate out the $x_A$ and $x_B$ contributions . begin{align} delta x^t Sigma^{-1} delta x &amp;= delta x_A^t left( widetilde{ Sigma}_{AA} delta x_A + widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t left( widetilde{ Sigma}^t_{AB} delta x_A + widetilde{ Sigma}_{BB} delta x_B right) &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} delta x_A + 2 delta x_A^t widetilde{ Sigma}_{AB} delta x_B bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B end{align} Again, using the properties of these block matrices, the determinant in the normalization factor can be conveniently written as . begin{align} det{ Sigma} = det{ Sigma_{AA}} cdot det{ widetilde{ Sigma}_{BB}^{-1}} = det{ Sigma_{BB}} cdot det{ widetilde{ Sigma}_{AA}^{-1}} end{align} Putting all this together, the marginal distribution for $x_B$ takes the form . begin{align} p(x_B lvert mu, Sigma) &amp;= left( frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} underbrace{ int exp{ left[- frac{1}{2} delta x_A^t widetilde{ Sigma}_{AA} delta x_A - delta x_A^t widetilde{ Sigma}_{AB} delta x_B right]} mathrm{d}x_A}_{I( widetilde{ Sigma}_{AA}, - widetilde{ Sigma}_{AB} delta x_B, 0)} right) times left( frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B widetilde{ Sigma}_{BB} delta x_B right]} right) &amp;= left( frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} times sqrt{ frac{(2 pi)^{n_A}}{ det widetilde{ Sigma}_{AA}}} exp{ left[ frac{1}{2} left( widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA}^{-1} left( widetilde{ Sigma}_{AB} delta x_B right) right]} right) times left( frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B widetilde{ Sigma}_{BB} delta x_B right]} right) &amp;= frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^t_B left( widetilde{ Sigma}_{BB}- widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} right) delta x_B right]} end{align} This is almost in the form of a multi-variate Gaussian. We can further simplify it by using the properties of block matrices listed above, since . begin{align} widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} &amp;= widetilde{C}^t widetilde{A}^{-1} widetilde{C} &amp;= left(-B^{-1} C^{t} widetilde{A} right) widetilde{A}^{-1} left(- widetilde{A} C B^{-1} right) &amp;= B^{-1}C^t widetilde{A}C B^{-1} &amp; equiv widetilde{B} - B^{-1} &amp;= widetilde{ Sigma}_{BB} - Sigma_{BB}^{-1} end{align}where we have used the fact that $ widetilde{C}= - widetilde{A}C B^{-1}$. . We thus arrive at the result that the marginal distribution for $x_B$ is also Gaussian, with average $ mu_B$ and covariance matrix $ Sigma_{BB}$. . We can simply read off the distribution for $x_B$ from the original joint distribution! begin{align} p(x_B lvert mu, Sigma) = p(x_B lvert mu_B, Sigma_{BB}) &amp;= frac{1}{ sqrt{(2 pi)^{n_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} delta x^{t}_B Sigma^{-1}_{BB} delta x_B right]} x_B &amp; sim mathcal{N}( mu_B, Sigma_{BB}) end{align} . This is the meaning of the quoted text which says that &quot;any finite number of which have a joint Gaussian distribution&quot;. . Conditioning / GP Predictions . A more useful result comes from considering the conditional distribution. Say we have already measured $x_B$, this would be our training data set, what can we say about the function values $x_A$ at other points? . From Bayes&#39; theorem, this conditional distribution is simply given by . begin{align} p(x_A lvert x_B, mu, Sigma) &amp;= frac{p(x_A, x_B lvert mu, Sigma)}{p(x_B lvert mu, Sigma)} end{align}After some simple manipulations, we will see that this distribution again has the form of a multi-variate Gaussian, although the average and covariance matrices will be a bit more complicated. . To start, let us rewrite the exponent appearing in the numerator, in order to cancel out the exponent in the denominator. begin{align} delta x^t Sigma^{-1} delta x &amp;= delta x_A^t left( widetilde{ Sigma}_{AA} delta x_A+ widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t left( widetilde{ Sigma}^t_{AB} delta x_A + widetilde{ Sigma}_{BB} delta x_B right) &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} delta x_A + 2 delta x_A^t widetilde{ Sigma}_{AB} delta x_B bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t widetilde{ Sigma}_{AB}^t delta x_A bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ delta x_A^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) + delta x_B^t widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-t} widetilde{ Sigma}_{AA}^t left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B - widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) bigg] + delta x_B^t widetilde{ Sigma}_{BB} delta x_B &amp;= bigg[ left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA} left( delta x_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) bigg] - delta x_B^t underbrace{ bigg[ widetilde{ Sigma}_{AB}^t widetilde{ Sigma}_{AA}^{-t} widetilde{ Sigma}_{AB} - widetilde{ Sigma}_{BB} bigg]}_{- Sigma_{BB}^{-1}} delta x_B end{align} . From which we see that the second term on the right hand side will exactly cancel the exponential in the denominator. . Now, lets consider the ratio of normalization constants . begin{align} sqrt{ frac{(2 pi)^{n_B} det{ Sigma_{BB}}}{(2 pi)^{n_A + n_B} det{ Sigma}}} &amp;= sqrt{ frac{ det{ Sigma_{BB}}}{(2 pi)^{n_A} det{ Sigma_{BB}} times det{ widetilde{ Sigma}_{AA}^{-1}}}} &amp;= frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} end{align} As promised, the conditional distribution for $x_A$ (conditioned on $x_B$) is another Gaussian! . begin{align} p(x_A lvert x_B, mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^{n_A} det{ widetilde{ Sigma}_{AA}^{-1}}}} exp{ left[- frac{1}{2} left(x_A - mu_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right)^t widetilde{ Sigma}_{AA} left(x_A - mu_A + widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B right) right]} end{align} begin{align} x_A lvert x_B &amp; sim mathcal{N} left( mu_A - widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} delta x_B, widetilde{ Sigma}_{AA}^{-1} right) end{align} Since it&#39;s more convenient to express all quantities in terms of the block matrices of $ Sigma$, we can rewrite the average and covariance using the following relationships . begin{align} widetilde{ Sigma}_{AA}^{-1} &amp;= Sigma_{AA} - Sigma_{AB} Sigma_{BB}^{-1} Sigma_{AB}^t end{align} begin{align} widetilde{ Sigma}_{AA}^{-1} widetilde{ Sigma}_{AB} &amp; equiv widetilde{A}^{-1} widetilde{C} &amp;= - widetilde{A}^{-1} widetilde{A} C B^{-1} &amp;= - C B^{-1} &amp;= - Sigma_{AB} Sigma_{BB}^{-1} end{align} From which we obtain the equivalent expression . begin{align} x_A lvert x_B sim mathcal{N} left( mu_A + Sigma_{AB} Sigma_{BB}^{-1} delta x_B, Sigma_{AA}- Sigma_{AB} Sigma_{BB}^{-1} Sigma^{t}_{AB} right) end{align} This is it! This is (almost) everything we need to do some Machine Learning with GP. This is what we need to evaluate for predicting the values of $x_A$ given noise-free observations of $x_B$. . Noisy Predictions . What about the case when we have noisy measurements for $x_B$? Assume that what we measure is in fact $y = x + epsilon$, with $ epsilon$ additive independent Gaussian noise (zero mean and variance $ sigma^2$), such that $ langle y rangle = langle x rangle$. . In this case, the covariance matrix is given as . begin{align} left langle delta y_i delta y_j right rangle &amp;= left langle ( delta x_i + epsilon_i) ( delta x_j + epsilon_j) right rangle &amp;= underbrace{ left langle delta x_i delta x_j right rangle}_{ Sigma(x_i,x_j)} + underbrace{ langle delta x_i epsilon_j rangle + langle delta x_j epsilon_i rangle}_{0} + underbrace{ langle epsilon_i epsilon_j rangle}_{ sigma^2 delta_{ij}} &amp;= Sigma(x_i, x_j) + sigma_i^2 delta_{ij} end{align} since $x sim mathcal{N}( mu, Sigma)$ and the noise is independent. This shows that $y sim mathcal{N}( mu, Sigma + sigma^2 I)$. . Thus, we can define a joint GP for $y_B$ (the noisy measurements) and $x_A$ (test points), and the conditional distribution has the same form as above, except that we replace $ Sigma_{BB}$ with $ Sigma_{BB} + sigma_B^2 I_BB$ (with $I$ the unit matrix). . begin{align} x_A lvert y_B sim mathcal{N} left( mu_A + Sigma_{AB} left( Sigma_{BB} + sigma_B^2 I_{BB} right)^{-1} delta y_B, Sigma_{AA} - Sigma_{AB} left( Sigma_{BB} + sigma_B^2 I_{BB} right)^{-1} Sigma_{AB}^t right) end{align} There are two possibilities, either we know $ sigma_B$ or we don&#39;t. In the former case, we proceed exactly as before, and in the latter we treat it as an additional hyper-parameter (together with the Kernel hyper-paramters). Note that here, $ sigma_B$ is in principle different for each of the $y_B$, but when it is an unknown hyper-parameter it is easiest to consider constant $ sigma_B$ for all points. . The above expressions give predictions for the noiseless function $x$. If we want predictions for the noisy function $y$, we would simply add $ sigma_A^2 I_{AA}$ to the covariance matrix. . Linear Combinations . One of the benefits of using GP lies in their linearity. If $x$ and $y$ are two GP, then any linear combination of them is also a GP. . In particular, begin{align} x&amp; sim mathcal{N}( mu_x, Sigma_x) y&amp; sim mathcal{N}( mu_y, Sigma_y) A x + B y + c &amp; sim mathcal{N}(A mu_x + B mu_y + c, A Sigma_x A^t + B Sigma_y B^t) end{align} . Unfortunately, products of GP do not result in GP... . A (simple) implementation . For improved numerical stability and computational cost, it is recommended not to compute the matrix inverses appearing in the expressions for the averages and covariances of the conditional distribution. A better approach, which is still quite expensive, is to use the Cholesky decomposition. . If $A$ is a positive definite matrix (i.e., a covariance matrix $ Sigma$), $A$ can be written as the product of a lower-triangular matrix $L$ and its transpose begin{align} A&amp;= L L^t end{align} such that expressions of the form $A^{-1} b = x$, for known $A$ and $b$ can be computed as begin{align} (L L^t)^{-1} b &amp;= x L^{-t} L^{-1} b &amp;= x L^t backslash left(L backslash b right) &amp;= x end{align} . Where we have adopted the backslash notation used by Rasmussen and Williams begin{align} A x&amp;= b x &amp;= A^{-1} b x &amp; equiv A backslash b end{align} where it is assumed that we know $A$, but not necessarily $A^{-1}$, and $b$. This notation is useful to emphasize the fact that we don&#39;t want to calculate $A^{-1}$ explicitly, we just need its product with some vector $b$ (i.e., to solve for $x$). . Using this Cholesky decomposition, a sandwich product of the form $b^{t} A^{-1} c$ would be expressed as begin{align} b^t A^{-1}c &amp;= b^{t}L^{-t} L^{-1} c &amp;= (L^{-1} b)^t (L^{-1} c) &amp;= w^t v end{align} with $v = L backslash c$ and $w = L backslash b$. . In the same way, we can evaluate more complicated expression, such as $A = C^t B^{-1} C$, without directly computing $B^{-1}$. Let $C = (c_1, c_2, ldots, c_n)$, where $c_i$ are the column-vector components of $C$. We have . begin{align} A &amp;= (c_1, c_2, ldots, c_n)^t B^{-1} (c_1, c_2, ldots c_n) &amp;= begin{pmatrix}c_1 c_2 vdots c_n end{pmatrix} begin{pmatrix} B^{-1} c_1 &amp; B^{-1} c_2 &amp; ldots &amp; B^{-1} c_n end{pmatrix} &amp;= begin{pmatrix} c_1^t B^{-1}c_1 &amp; ldots &amp; c_1^t B^{-1} c_n vdots &amp; ddots &amp; vdots c_n^t B^{-1}c_1 &amp; ldots &amp; c_n^t B^{-1} c_n end{pmatrix} end{align}where each term is computed using the expression derived above begin{align} (A)_{ij} &amp;= c_i^t B^{-1} c_j equiv (L backslash c_i)^t (L backslash c_j) end{align} with $L$ now the Cholesky decomposition of $B=LL^t$. . Evaluating terms like $ log{ det{A}}$ is also considerably simplified . begin{align} log det{A} &amp;= log det{LL^t} &amp;= log left( det{L} cdot det{L^t} right) &amp;= 2 log det{L} = 2 sum_i log L_{ii} end{align} Sampling . To sample random functions from a GP, $f sim mathcal{N}( mu, K)$, with mean $ mu$ and covariance $K$, we use the following procedure (see Rasmusen and Williams, pg. 201). Let $n$ be the number of points $x$ at which we want to sample the functions $f$ . Compute the Cholesky decomposition of $K = L L^t$ | Generate $n$ independent random numbers from a uniform Gaussian distribution, $g sim mathcal{N}(0, 1)$ | Compute $f = mu + L g$ | The resulting $f$ have the desired average and covariance begin{align} langle f rangle &amp;= langle{ mu + L g} rangle = mu + L langle g rangle equiv mu langle (f - mu) (f - mu)^t rangle &amp;= langle L g g^t L^t rangle = L langle g g^t rangle L^t = L L^t equiv K end{align} . To improve stability of the Cholesky decomposition, it is suggested to add a small multiple of the identity to $K$, i.e., $K longrightarrow K + epsilon I$. . However, as hinted at above, using the Cholesky decomposition is not the &#39;&#39;best&#39;&#39; way to evaluate the GP. The reason for this is its $ mathcal{O}(n^3)$ complexity. Fortunately, in recent years advanced matrix-matrix algorithms have been developed that allow for exact calculations even on millions of points! . Excercise . We include the usual boiler-plate code. Note that we are using jax extensively (this rules out running on windows!) . import jax.numpy as np import pandas as pd import numpy as onp import pymc3 as pm import theano as th import theano.tensor as tt import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.patheffects as PathEffects from scipy import optimize from scipy.interpolate import interp1d from jax import grad, jit, vmap, jacfwd, jacrev, random from jax.numpy.lax_numpy import _wraps from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from functools import partial,reduce from pymc3.gp.util import plot_gp_dist mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} fancycolors = [mpl.colors.to_hex(c) for c in [[0.6, 0.6, 0.6],[0.7, 0.3, 1],[0.3, 0.7, 1],[0.2, 0.9, 0.9], [0.3, 1, 0.7],[0.7, 1, 0.3],[0.9, 0.9, 0.2],[1, 0.7, 0.3], [1, 0.3, 0.7], [0.9, 0.2, 0.9],[1.0, 1.0, 1.0]]] threecolors = [mpl.colors.to_hex(c) for c in [[0.1, 0.15, 0.4],[1, 0.2, 0.25],[1.0, 0.775, 0.375]]] fourcolors = [mpl.colors.to_hex(c) for c in [[0.9, 0.6, 0.3],[0.9, 0.4, 0.45],[0.5, 0.65, 0.75],[0.42, 0.42, 0.75]]] def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) def plot_quantiles(ax, xdata, ydata,*, axis, quantiles, colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=axis) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_sig2= quantiles_sig[1:-1] quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . First, let&#39;s define some of the most common kernel functions . begin{align} k(x_1,x_2) &amp;= eta^2 x_1 x_2 &amp;( text{Linear}) k(x_1,x_2) &amp;= eta^2 delta_{x_1,x_2} &amp;( text{White Noise}) k(x_1,x_2) &amp;= eta^2 exp{ left[- frac{ lvert x_1-x_2 rvert}{2l} right]} &amp;( text{Ornstein Uhlenbeck}) k(x_1,x_2) &amp;= eta^2 left(1 + frac{ sqrt{3 (x_1 - x_2)^2}}{l} right) exp{ left[- frac{ sqrt{3(x_1-x_2)^2}}{l} right]} &amp;( text{Matern} ,3/2) k(x_1,x_2) &amp;= eta^2 left(1 + frac{ sqrt{5 (x_1 - x_2)^2}}{l} + frac{5(x_1 - x_2)^2}{3 l^2} right) exp{ left[- frac{ sqrt{5(x_1 - x_2)^2}}{l} right]}&amp; ( text{Matern 5/2}) k(x_1,x_2) &amp;= eta^2 exp{ left[- frac{(x_1-x_2)^2}{2l^2} right]} &amp;( text{Square Exponential}) end{align} Note: Jax has yet to add support for general outer operations, but we can easily create a jax version of outer subtract by modyfing the corresponding code for outer. . @_wraps(onp.subtract.outer) def subtract_outer(a,b,out=None): if out: raise NotImplementedError(&quot;The &#39;out&#39; argument to outer is not supported&quot;) a, b = np.lax_numpy._promote_dtypes(a, b) return np.ravel(a)[:,None] - np.ravel(b) def K_Linear(x1, x2, l=1.0): return np.outer(x1, x2) def K_WhiteNoise(x1, x2, l=1.0): r = np.abs(subtract_outer(x1,x2)) return np.where(r == 0, 1.0, 0.0) def K_OrnsteinUhlenbeck(x1, x2, l=1.0): return np.exp(-0.5*np.abs(subtract_outer(x1,x2))/l) def K_Matern32(x1,x2, l=1.0): r = np.sqrt(3)*np.abs(subtract_outer(x1,x2))/l return (1 + r)*np.exp(-r) def K_Matern52(x1,x2, l=1.0): r = np.sqrt(5)*np.abs(subtract_outer(x1,x2))/l return (1 + r + r**2/3)*np.exp(-r) def K_SquareExp(x1, x2, l=1.0): return np.exp(-0.5*(subtract_outer(x1,x2)/l)**2) def Cholesky(K, ϵ): try: L = np.linalg.cholesky(K + np.diag(ϵ)) return L except: print(&quot;Unexpected Error&quot;) raise . 1. Drawing functions from a GP prior . Grid the functions $f(x)$ on a grid of $x$ values in the range $[-10,10]$, with a grid spacing of $ Delta x = 0.2$ ($n=101$ grid points). Set $ mu = 0$ and $ eta = l = 1$. . Generate $n times 4$ independent normally distributed random numbers | Use this set of random numbers to sample $4$ random functions $f sim mathcal{N}(0, K)$ (over the $n$ grid points) using the procedure outlined above, for each of the kernels $K$ we have defined. | . You should obtain something similar to the plot below . Since we reused the random numbers, the corresponding functions obtained from the Ornstein-Uhlenbeck, Matern and Square-Exponential Kernels show very similar behavior and allow for easy comparison. Below we plot the orange curves for each of the kernels. . 2. Verify the statistical properties of the GP prior . Now, let us verify the properties of the GP. Use the Square-Exponential kernel, with $ eta = 1.5$, $l=0.75$, $ mu(x) = 0$, and use the same $x$ grid as above. . Draw $n_s = 500$ random functions from this GP prior and plot them. You should obtain something similar to the following graph (here $3$ of the $500$ curves have been colorized for visualization purposes). . Now, compute the single-point average and variance (over the ensemble of functions), i.e., . begin{align} langle f(x) rangle = frac{1}{n_s} sum_{i=1}^{n_s} f^{(i)}(x) equiv mu(x) langle delta f(x) ^2 rangle = frac{1}{n_s} sum_{i=1}^{n_s} left(f^{(i)}(x) - mu(x) right)^2 equiv eta^2 end{align}with $f^{(i)}$ the $i$-th function from the sample. . As a bonus, compute the quantile intervals $(0.1587, 0.8413)$, $(.0228, 0.9772)$, and $(.0014, 0.9986)$, by definition these should correspond to $ pm eta$, $ pm 2 eta$, and $ pm 3 eta$ intervals. Below, we show a plot of the average $ langle f(x) rangle$, together with the quantiles (dashed lines show the theoretical values). . Now, let&#39;s compute the covariance of the function values at different $x$ points, and compare with the theoretical expressions: begin{align} delta f(x_i) delta f(x_j) &amp;= a(x_i, x_j) langle a rangle &amp; equiv k(x_1, x_2) &amp;= eta^2 exp{ left[- frac{(x_1-x_2)^2}{2 l^2} right]} langle a^2 rangle - langle a rangle^2 &amp;= langle delta f(x_i)^2 delta f(x_j)^2 rangle - langle delta f(x_i) delta f(x_j) rangle^2 &amp; equiv k(x_1, x_1) k(x_2, x_2) + k(x_1,x_2)^2 &amp;= eta^{4} left(1 + exp{ left[- frac{(x_1-x_2)^2}{2l^2} right]} right) end{align} . See wikipedia for a list of higher-order moments of multi-variate Gaussians. . The plot below shows the average two-point correlation $a= delta f(x_i) delta f(x_j)$, as a function of the separation $ lvert{x_i-x_j} lvert$. The filled region corresponds to the $ pm sigma$ interval. . 3. GP for Machine Learning . Assume you have the following $20$ training points . x f . 0 0.000000 | 2.000000 | . 1 0.040404 | 2.694292 | . 2 0.050505 | 2.741803 | . 3 0.212121 | 1.717956 | . 4 0.222222 | 1.788399 | . 5 0.313131 | 2.208924 | . 6 0.333333 | 2.163571 | . 7 0.373737 | 2.004896 | . 8 0.393939 | 1.936078 | . 9 0.454545 | 1.906280 | . 10 0.565657 | 2.058928 | . 11 0.656566 | 1.973258 | . 12 0.666667 | 1.969105 | . 13 0.737374 | 1.992183 | . 14 0.747475 | 1.998489 | . 15 0.797980 | 2.017283 | . 16 0.848485 | 2.008885 | . 17 0.919192 | 1.990957 | . 18 0.929293 | 1.990608 | . 19 0.959596 | 1.992993 | . Let us use GP to learn the function, and make predictions for its value at new test points $x_ star$, i.e. we need to compute $p(f_ star lvert x_ star, x, f)$. . Assume that on prior information you now that $ mu(x) = 2.0$. . Inferring the hyper-parameters . The first thing we need to do is to infer the kernel hyper-parameters. Assuming a square exponential kernel, we need the amplitude and length-scale of the correlations, $ eta$ and $l$, respectively. . This is a simple parameter estimation problem. . Let $ Theta = ( eta, l)$, and $D$ the measured data, the posterior distribution we are interested in is then . begin{align} P( Theta lvert D, I) propto mathcal{L}( Theta) Pi( Theta) end{align}with $L( Theta)$ the likelihood of observing the data (given $ Theta$), and $ Pi( Theta)$ the prior for $ Theta$. As usual, it&#39;s best to work with the logarithm of this quantity. . Assuming a GP likelihood, $ mathcal{L} sim mathcal{N}( mu, K_ Theta)$, we have . begin{align} L = ln{ mathcal{L}( Theta)} &amp;= - frac{1}{2} delta f^t K^{-1} delta f - frac{1}{2} ln{ left[(2 pi)^n det{K} right]} &amp;= - frac{1}{2} left[ delta f^t K^{-1} delta f + n ln{2 pi} + ln{ det{K}} right] - ln{ mathcal{L}( Theta)}&amp;= frac{1}{2} left[ delta f^t K^{-1} delta f + ln{ det{K}} + n ln{2 pi} right] end{align}with $ delta f = f - mu$ and $n$ the number of training points. . For the prior $ Pi( Theta)$, we assume $ eta$ and $l$ are independent, and since both are scale parameters, we use Jeffrjeys prior, such that begin{align} Pi( Theta)&amp;= Pi( eta) Pi(l) propto frac{1}{ eta} times frac{1}{l} - ln{ Pi( Theta)}&amp;= ln{ eta} + ln{l} + text{const} end{align} . Then, we can either settle for a point-wise solution, maximizing the log-likelihood to obtain the &#39;&#39;best&#39;&#39; estimate for the hyper-parameters, or we can go full Bayesian and estimate the posterior with some MC simulations. In any case it is useful to compute the gradients of this log-likelihood (MC packages like PYMC do this automatically, and using jax we don&#39;t have to worry about doing it by hand). . Maximum Aposteriori Estimate . begin{align} - partial_ eta ln{ Pi( eta)} &amp;= frac{1}{ eta} - partial_l ln{ Pi(l)} &amp;= frac{1}{l} - partial_ theta ln{ mathcal{L}( Theta)} &amp;= frac{1}{2 theta} left[ mathrm{Tr}{ left(K^{-1} Q_ theta right)} - delta f^t K^{-1}Q_ theta K^{-1} delta f right] end{align}with $ theta$ one of the kernel parameters, $ theta_1 = eta$ or $ theta_2 = l$, and $Q_ theta$ defined as . begin{align} Q_ theta(x_1,x_2)&amp;= theta partial_ theta K(x_1,x_2) Q_{ eta}(x_1,x_2)&amp;= 2 K(x_1,x_2) Q_{l}(x_1,x_1) &amp;= frac{(x_1 - x_2)^2}{l^2} K(x_1,x_2) end{align} Using the Cholesky decomposition of $K = L L^t$, we can evaluate each of the terms appearing in these expressions as follows begin{align} delta f^t K^{-1} delta f &amp;= left(L backslash delta f right)^t left(L backslash delta f right) = left lVert L backslash delta f right rVert ^2 ln det{K} &amp;= 2 sum_i ln{L_{ii}} text{Tr}{ left(K^{-1} Q right)} &amp;= text{Tr}{ left(L^{-1} Q L^{-t} right)} delta f^t K^{-1} Q K^{-1} delta f &amp;= left(L backslash delta f right)^t L^{-1} Q L^{-t} (L backslash delta f) L^{-1} Q L^{-t} &amp;= begin{pmatrix}L backslash r_1 &amp; cdots L backslash r_n end{pmatrix} begin{pmatrix}r_1 &amp; cdots &amp; r_n end{pmatrix} &amp; equiv Q L^{-t} = left(L^{-1} Q right)^t = begin{pmatrix}L backslash q_1 &amp; cdots L backslash q_n end{pmatrix}^t end{align} . with $q_i$ the columns of the symmetric $Q$ matrix. . K = K_SquareExp @jit def logp(p, μ, x, f, ϵ): #...宿題 return gradlogp = jit(grad(logp)) @jit def gradlogp_SE(p, μ, x, f, ϵ): #...宿題 return . def hyperparams(init,μ,x,f,noise): opt0 = optimize.minimize(logp, init, args=(μ,x,f,noise), method=&#39;Nelder-Mead&#39;, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) opt1 = optimize.minimize(logp, opt0[&#39;x&#39;], args=(μ,x,f,noise), method=&#39;BFGS&#39;, jac = gradlogp, options={&#39;maxiter&#39;:100, &#39;disp&#39;:0}) return [opt0, opt1] . avg_samples = np.ones_like(x_samples)*2.0 noise_samples = np.ones_like(x_samples)*1.0e-6 opts = hyperparams(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples) for o in opts: p = np.exp(o[&#39;x&#39;]) print(f&#39; t loss = {o[&quot;fun&quot;]:12.6e}, niter = {o[&quot;nit&quot;]:5d}, Converged = {o[&quot;success&quot;]:6b} : {o[&quot;message&quot;]}&#39;) print(f&#39; t t η = {p[0]:6.3f}, l = {p[1]:6.3f}&#39;) ηopt,lopt = p . loss = -4.343454e+01, niter = 84, Converged = 1 : Optimization terminated successfully. η = 1.229, l = 0.116 loss = -4.343454e+01, niter = 3, Converged = 1 : Optimization terminated successfully. η = 1.229, l = 0.116 . Just to be sure, we can compare our hand-coded gradient with the automatic differentiated version provided by jax. . np.allclose(gradlogp(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples), gradlogp_SE(np.zeros(2), avg_samples, x_samples, f_samples, noise_samples)) . DeviceArray(True, dtype=bool) . Take home message : always use jax! . Predicting test values . Now, use the conditional GP distribution (with the optimal hyperparameters just obtained) to predict the values of the function for test points $x_ star in [-1.5, 1.5]$, with $ Delta x_ star = 0.015$, for $n_ star = 201$ points. . Recall the definition of the conditional GP begin{align} f_ star lvert f &amp; sim mathcal{N} left( mu_ star + K_{ star}^t K^{-1} delta f, K_{ star star} - K_{ star}^t K^{-1} K_{ star} right) K &amp;= K(X, X) K_ star &amp;= K(X, X_ star) K_{ star star} &amp;= K(X_ star, X_ star) end{align} where $X=(x_1,x_2, cdots, x_n)$ and $X_{ star} = (x_{1 star}, x_{2 star}, cdots, x_{n_ star})$, are the design matrices for the training and test points. . def GPpost(x_new, x_obs, f_obs, noise_obs, μ, K): #...宿題 return μpost, Kpost . x = np.linspace(-1.5, 1.5, num=201) μpost, Kpost = GPpost(x, x_samples, f_samples, noise_samples, lambda x: 2.0, lambda x1,x2: ηopt**2*K(x1,x2,lopt)) σpost = np.sqrt(np.diag(Kpost)) . finterp = interp1d(x_samples, f_samples, kind=&#39;cubic&#39;) xinterp = np.linspace(x_samples[0], x_samples[-1], num=100) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;, ls=&#39;--&#39;) ax.legend(fontsize=22) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) . (1.0, 3.0) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;) ax.legend(fontsize=22) . &lt;matplotlib.legend.Legend at 0x7fa193ecdd10&gt; . Predicting test values from noisy data . Now assume that you have noisy training data, with $ sigma = 0.04$. . key = random.PRNGKey(12345) key,subkey = random.split(key) σ = 0.04 f_noisy = f_samples + σ*random.normal(subkey, shape=f_samples.shape) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) #ax.plot(x_samples, f_samples, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mew=1, label=&#39;Training&#39;) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) ax.legend(fontsize=22) plt.show() . Optimize the hyperparameters $( eta, l)$ and make predictions for the test points in this case. . def logp(p, μ, x, f, noise): #...宿題 return . def hyperparams(init,μ,x,f,noise): #...宿題 return . opts = hyperparams(np.zeros(2), avg_samples, x_samples, f_samples, np.ones_like(x_samples)*σ**2) for o in opts: p = np.exp(o[&#39;x&#39;]) print(f&#39; t loss = {o[&quot;fun&quot;]:12.6e}, niter = {o[&quot;nit&quot;]:5d}, Converged = {o[&quot;success&quot;]:6b} : {o[&quot;message&quot;]}&#39;) print(f&#39; t t η = {p[0]:6.3f}, l = {p[1]:6.3f}&#39;) ηopt,lopt = p . loss = -3.405535e+01, niter = 95, Converged = 1 : Optimization terminated successfully. η = 0.215, l = 0.037 loss = -3.405535e+01, niter = 3, Converged = 1 : Optimization terminated successfully. η = 0.215, l = 0.037 . μpost, Kpost = GPpost(x, x_samples, f_noisy, np.ones_like(x_samples)*0.05**2, lambda x: 2.0, lambda x1,x2: ηopt**2*K(x1,x2,lopt)) σpost = np.sqrt(np.diag(Kpost)) . finterp = interp1d(x_samples, f_noisy, kind=&#39;cubic&#39;) . fig, ax = plt.subplots(figsize=(18,9)) ax.plot(x, hidden_function(x), lw=3, color=color[&#39;dark&#39;], label=&#39;Truth&#39;) ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mew=2, label=&#39;Training&#39;) #ax.plot(x_samples, f_noisy, ls=&#39;None&#39;, marker=&#39;o&#39;, ms=10, color=color[&#39;dark&#39;], label=&#39;Training&#39;) ax.plot(x, μpost, alpha=0.6, color=color[&#39;mid_highlight&#39;], label=&#39;GP&#39;) ax.fill_between(x, μpost-2*σpost, μpost+2*σpost, color=color[&#39;light&#39;]) ax.plot(xinterp, finterp(xinterp), color=color[&#39;superfine&#39;], label=&#39;Cubic Spline&#39;, ls=&#39;--&#39;) ax.legend(fontsize=22) ax.set_xlim(0, 1.0) ax.set_ylim(1,3) plt.show() .",
            "url": "https://johnjmolina.github.io/MLKyoto/gaussian%20processes/2020/07/03/Gaussian-Processes-Theory.html",
            "relUrl": "/gaussian%20processes/2020/07/03/Gaussian-Processes-Theory.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayesian Model Selection",
            "content": "Motivation . This is the third post, dealing with the problem of Model Selection and corresponding to Chapter 4 of Sivia and Skilling&#39;s book. The problem we wish to tackle is the following: Given a signal, how many peaks is there most evidence for, and what are the positions and amplitudes of those peaks. If our prior information included knowledge of the number of peaks, figuring out their positions and amplitudes would be a simple excercise in parameter estimation. However, here we are faced with the more challenging task of choosing between different models (i.e., the one with $M=2$, $M=3$,... number of peaks). . References . This topic is discussed in some detail in Sivia&#39;s book (Ch. 4, Least-Squares Extensions), as well as Jaynes&#39; (Ch. 20, Model Comparison). . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | . In addition, Sivia also has several papers covering this material, with an emphasis on analyzing spectral data . Molecular spectroscopy and Bayesian spectral analysis—how many lines are there? D. S. Sivia and C. J. Carlile, J. Chem. Phys. 96, 170 (1992) | Bayesian analysis of quasieleastic neutron scattering data D. S. Sivia, C. J. Carlile, W. S. Howells, and S. Konig, Physica B 182, 341 (1992) | A Bayesian approach to extracting structure-factor amplitudes from powder diffraction data D. S. Sivia and W. I. F. David, Acta Cryst. A 50, 703 (1994) | The Bayesian approach to reflectivity dataD. S. Sivia and J. R. P. Webster, Physica B 248, 327 (1998) | . Introduction . Let&#39;s say we have to decide between two models, $M=1$ and $M=2$, given some experimental data $D$. How to decide between them? We should look at the posterior ratio . begin{align} O_{21} &amp; equiv frac{P(M_2 lvert D, I)}{P(M_1 lvert D, I)} &amp;= frac{ frac{P(D lvert M_1, I) P(M_1 lvert I)}{P(D lvert I)}}{ frac{P(D lvert M_2, I) P(M_2 lvert I)}{P(D lvert I)}} &amp;= frac{P(D | M_2)}{P(D|M_1)} cdot frac{P(M_2)}{P(M_1)} &amp;= frac{Z_2}{Z_1} cdot frac{ Pi_2}{ Pi_1} end{align}where $Z_i = P(D|M_i,I)$ is the evidence for model $M_i$, and $ Pi_i = P(M_i lvert I)$ the prior. This ratio of likelihoods $Z_2/Z_1$ is also known as the &#39;Bayes Factor&#39;. . We can compute the likelihood $P(D|M_i, I)$ by marginalizing over the parameters $ Theta= Theta_i$ of model $M_i$. To simplify the notation we drop the index $i$, but it should be clear that all quantities are defined on a per/model basis . begin{align} Z equiv P(D | M, I) &amp;= int mathrm{d} Theta , P(D, Theta lvert M, I) &amp;= int mathrm{d} Theta , P(D lvert Theta, M, I) cdot P( Theta lvert M, I) &amp;= int mathrm{d} Theta , L( Theta_i) cdot Pi( Theta) end{align}with $L( Theta) = P(D| Theta, M, I)$ and $ Pi( Theta) = P( Theta lvert M, I)$ the likelihood of the data and prior for the parameters of model $M$, respectively. . Recall that the integrand that appears here is proportional to posterior for $ Theta$, which is the main quantity of interest in a parameter estimation problem begin{align} P( Theta lvert D, M, I) &amp;= frac{ overbrace{P(D lvert Theta, M, I)}^{ text{Likelihood} , L} overbrace{P( Theta lvert M, I)}^{ text{Prior} , Pi}}{ underbrace{P(D lvert M, I)}_{ textrm{Evidence} ,Z}} &amp;= frac{L( Theta) Pi( Theta)}{Z} end{align} Where the evidence integral was usually ignored since it is just a normalizing factor. However, we now see that for a Model Selection problem these $Z$ are the main objects of interest. . Evaluating this integral by brute force will only be possible for very simple models (low dimensions). However, following Jaynes (ch. 20), we can rewrite these factors into a more telling form. . Let $L^ star = L( Theta^ star) = max_{ Theta}{L}$, the evidence is now begin{align} Z &amp;= L^ star underbrace{ int text{d} Theta , frac{L( Theta)}{L^ star} cdot Pi( Theta)}_{ textrm{Ockham factor} ,W} = L^ star W end{align} where $W$ is the so-called &#39;Ockham factor&#39;. It is this term which will protect us from over-fitting. By definition, the prior $ Pi( Theta) = P( Theta | M)$ integrates to one, so we see that the factor of $L/L^ star$ is picking out the fraction of prior probability mass contained in the high-likelihood regions. Quoting from Jaynes: &quot;...the Ockham factor is the ratio by which the parameter space is contracted by the information in the data, which expresses how much the vagueness of our prior infomration deteriorates the performance of [the model], by placing prior probability outside its high-likelihood region&quot; . Finally, we can write the Bayes factor appearing in the posterior ratio as begin{align} O_{21} &amp;= underbrace{ frac{L_2^ star}{L_1^ star}}_{ textrm{Goodness of fit}} cdot underbrace{ frac{W_2^{ phantom{ star}}}{W_1^{ phantom{ star}}}}_{ textrm{Model robustness}} cdot underbrace{ frac{ Pi_2^{ phantom{ star}}}{ Pi_1^{ phantom{ star}}}}_{ textrm{Prior ratio}} end{align} . To see how this could favor one model over another, let&#39;s use the Laplace approximation to try and evaluate the integral in the Ockham factor. begin{align} W &amp;= int textrm{d} Theta e^{ mathcal{L}} Pi( Theta) mathcal{L}&amp;= ln{ frac{L( Theta)}{L^ star}} end{align} expanding $ mathcal{L}$ around the maximum, $ Theta^ star$, we have begin{align} mathcal{L}&amp; simeq mathcal{L}^ star + left. nabla mathcal{L} right lvert_{ Theta^ star} cdot ( Theta - Theta^ star) + frac{1}{2}( Theta- Theta^ star)^{t} cdot left. nabla nabla mathcal{L} right lvert_{ Theta^*} cdot( Theta- Theta^ star) &amp;= frac{1}{2}( Theta - Theta^ star)^t cdot left. nabla nabla mathcal{L} right lvert_{ Theta^ star}( Theta- Theta^ star) end{align} since $ mathcal{L}^ star = ln{L^ star/L^ star} = 0 = left. nabla mathcal{L} right lvert_{ Theta^ star}$ by definition. . Thus, we can write the Ockham factor as begin{align} W &amp;= int text{d} Theta , exp{ left[- frac{1}{2}( Theta- Theta^ star)^t cdot Sigma^{-1} cdot( Theta- Theta^ star) right]} Pi( Theta) Sigma &amp;= left( left.- nabla nabla mathcal{L} right lvert_{ Theta^ star} right)^{-1} end{align} . Now assume that $ Pi( Theta)$ is constant, $ Pi( Theta) = 1/ Omega$, with $ Omega$ the allowed volume in parameter space. begin{align} W &amp; simeq frac{ sqrt{(2 pi)^p det{ Sigma}}}{ Omega} end{align} with $p$ the number of parameters for this model, $ Theta = ( Theta_1, Theta_2, ldots, Theta_p)$. . Recall that $ Sigma$ is the covariance matrix, so that $ det Sigma$ is related to the volume of the parameter region that is consistent with the data. In this case, larger error bars are actually a good thing! They are telling us that the model allows for more values that can explain the data, and thus is more robust, and should be preferred. . How many lines are there? . Let our parameters be denoted as . begin{align} M &amp;: textrm{Number of Line Peaks} A_i &amp;: textrm{Amplitude of peak} ,i X_i &amp;: textrm{Position of peak} , i b_1 &amp;: textrm{Intercept for linear background signal} b_2 &amp;: textrm{Slope for linear background signal} D= {x_k,y_k } &amp;: textrm{Experimental data} end{align}With the following prior information . begin{align} I &amp;: textrm{Lines peaks are measured as gaussians of fixed width } W textrm{, subject to Gaussian noise of variance } sigma, textrm{ in the presence of a linear background signal}. end{align}The inference problem to solve is then, given a spectra, how many peaks are there (at what positions and with what magnitudes)? . The posterior probability we are interested in is $P(M lvert D I)$, i.e., the probability that the signal corresponds to an $M$-peak model. From Bayes theorem, we have the following . begin{align} P(M, D lvert I) &amp;= P(M lvert D,I)P(D lvert I) = P(D lvert M,I)P(M lvert I) P(M lvert D,I) &amp;= frac{P(D lvert M,I) P(M lvert I)}{P(D lvert I)} end{align}Now, if we assign uniform priors for $P(M lvert I) = 1/M_{ text{max}}$, with $M=1, ldots, M_{ text{max}}$, the posterior becomes proportional to the evidence begin{align} P(M lvert D, I) &amp; propto P(D lvert M, I) = Z_M end{align} where the normalization constant can be evaluated later, since begin{align} sum_{M=0}^{M_{ text{Max}}} P(M lvert D, I)&amp;= alpha sum_M P(D lvert M, I)= 1 alpha^{-1} equiv M_{ text{max}} P(D lvert I) &amp;= sum_M P(D lvert M, I) end{align} . To evaluate the evidence , we &quot;simply&quot; marginalize over the model parameters, . begin{align} Z_M &amp;= int textrm{d}b_1 int textrm{d} b_2 int textrm{d}A^{M} int textrm{d}X^M P(D, b_1, b_2, {A_i, X_i } lvert M, I) &amp;= int textrm{d}b_1 int textrm{d}b_2 int textrm{d}A^{M} int textrm{d}X^M P(D lvert b_1, b_2, {A_i,X_i }, M, I) P(b_1, b_2, {A_i,X_i } lvert M, I) end{align}The prior can be decomposed as begin{align} P(b_1, b_2, {A_i, X_i } lvert M, I) &amp;= P(b_1, b_2 lvert {A_i, X_i }, M, I) , P( {A_i, X_i } lvert M, I) &amp;= P(b_1, b_2 lvert I) , P( {A_i, X_i } lvert M, I) end{align} where we use the fact that the background signal parameters do not depend on the number of lines (or peak parameters). Thus, this prior $P(b_1,b_2 lvert I)$ can be absorbed into the normalization constant $ alpha$. . For the peak position and amplitude parameters we assume independent uniform priors, such that $X_{ textrm{min}} le X le X_{ textrm{max}}$ and $0 le A le A_{ textrm{max}}$ begin{align} P( {A_i,X_i } lvert M, I) &amp;= prod_{i} P(A_i,X_i lvert M,I) &amp;= prod_i P(A_i lvert M,I) P(X_i lvert M, I) &amp;= left(A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right)^{-M} end{align} . Therefore, the posterior becomes . begin{align} P(M lvert D, I) &amp; propto frac{1}{ left[A_{ text{max}} left(X_ text{max} - X_ text{min} right) right]^M} int textrm{d}b_1 int textrm{d}b_2 int text{d}A^M int text{d}X^M P(D lvert b_1, b_2, {A_i,X_i }, M, I) end{align} Our prior information says that the measurements are independent, subject to Gaussian noise of amplitude $ sigma_k$. Then, the Likelihood for the data, given the M-peak model and its parameters becomes . begin{align} P(D lvert b_1, b_2, {A_i, x_i }, M, I) &amp; propto P( {D_k } lvert b_1, b_2, {A_i, x_i }, M, I) &amp;= prod_k P(D_k lvert b_1, b_2, {A_i, x_i }, M, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{1}{ sqrt{2 pi}} left( frac{y_k - f(x_k; b_1, b_2, {A_i, X_i })}{ sigma_k} right)^2 right]} &amp; propto exp{ left(- frac{ chi^2}{2} right)} chi^2 &amp;= sum_k left( frac{y_k - f_k}{ sigma_k} right)^2 end{align}where the factors of $ sqrt{2 pi sigma_k^2}$ can be absorbed into the normalization constant since they don&#39;t depend on $M$, but only the number of data points $N$ (the $ sigma_k$ are not model parameters). . We therefore have the following for the posterior (evidence) . begin{align} P(M lvert D, I) propto frac{1}{ left[A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right]^M} int textrm{d}b_1 int textrm{d}b_2 int textrm{d}A^M int textrm{d}X^M exp{ left(- frac{ chi^2}{2} right)} end{align} A brute force approach to this $2M + 2$ dimensional integral is out of the question. However, we can proceed analytically by using the Laplace approximation, expanding around the minimum of $ chi^2$. Let $ Theta$ denote the model parameters, $ Theta = (b_1, b_2, {A_i }, {X_i })$, with $ Theta_0$ the minimum, such that . begin{align} chi^2&amp; simeq chi^2_0 + frac{1}{2} left( Theta - Theta_0 right)^t cdot left. nabla nabla chi^2 right lvert_{ Theta_0} cdot left( Theta - Theta_0 right) &amp;= chi^2_0 + left( Theta- Theta_0 right)^t cdot Sigma^{-1} cdot left( Theta- Theta_0 right) Sigma^{-1} &amp;= frac{1}{2} left. nabla nabla chi^2 right lvert_{ Theta_0} end{align} Putting this expression back into the evidence integral gives us . begin{align} P(M lvert D, I) &amp; propto frac{1}{ left[A_{ text{max}} left(X_{ text{max}} - X_{ text{min}} right) right]^M} e^{- chi^2_0/2} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot Sigma^{-1} cdot left( Theta- Theta_0 right) right]} &amp; propto frac{M!}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M} e^{- chi^2_0/2} sqrt{(2 pi)^{2M+2} det{ Sigma}} end{align}where the factor of $M!$ is needed to account for all possible permutations of the peaks (i.e., the labels we use are arbitrary and should not matter). . The determinant of the Hessian matrix can be simplified as follows, begin{align} det{ Sigma} &amp;= frac{1}{ det{ Sigma^{-1}}} = frac{1}{ det{ left( frac{1}{2} left. nabla nabla chi^2 right lvert_{ Theta} right)}} = frac{2^{2M+2}}{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}} end{align} from which we finally obtain begin{align} P(M lvert D, I) &amp; propto frac{M!}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M} e^{- chi^2_0/2} sqrt{ frac{(2 pi)^{2M+2} 2^{2M + 2}}{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}}} &amp;= frac{M! (4 pi)^{M+1}}{ left[A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) right]^M sqrt{ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)}}} e^{- chi^2_0/2} end{align} . We dropped all the proportionality constants that did not depend on $M$, but we can now recover them by enforcing the normalization condition, $ sum_k P(M lvert D, I) = 1$. . As always, it is more convenient to work with the logarithm of the posterior, . begin{align} ln{P(M lvert D, I)}&amp;= - frac{ chi_0^2}{2} + ln{M!} + M Big[ ln{4 pi} - ln big(A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) big) Big] - frac{1}{2} ln{ left[ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)} right]} + ln{4 pi} end{align}Thus, within the approximations we have made, the model selection problem can be effectively reduced to a series of minimization problems (one for each of the models). Due to the non-linearity, Sivia (Ch. 4.2.1) advocates for the following numerical algorithm . ($M=0$) Find the optimum values for the 2d problem (b_1, b_2) | | ($M=1$) Perform a 1d scan over the new peak position $x_1$, by minimizing with respect to $(b_1, b_2, A_1)$, using the optimum values for ($M=0$) as initial guess. For the initial $A_1$, we can use the minimum of signal as a guide. Take the $x_1$ value resulting in the lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, x_1)$, using the optimum values obtained at step (1.) as an initial guess. | | ($M=2$) Perform a 1d scan over the new peak position $x_2$, by minimizing with respect to the background and amplitude parameters, $(b_1, b_2, A_1, A_2)$. At each point in this scan, the positions of the peaks are fixed to ($x_1^{(M=1)}, x_2$), with the initial guess taken from the optimum value for $M=1$. Take the $x_2$ value resulting in the lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, A_2, x_1, x_2)$, using the optimum values obtained at step (1.) as an initial guess. $ vdots$ | | ($M=m$) Perform a 1d scan over the new peak position $x_m$, by minimizing with respect to the background and amplitude parameters, $(b_1, b_2, A_1, A_2, ldots A_m)$. At each point in this scan, the positions of the peaks are fixed to ($x_1^{(M=m-1)}, ldots, x_{m-1}^{(M=m-1)}, x_m$), with the initial guess taken from the optimum value for $M=m-1$. Take the $x_m$ value resulting int he lowest $ chi^2$ as the best guess. | Perform a full minimization $(b_1, b_2, A_1, ldots , A_m, x_1, ldots, x_m)$, using the optimum values obtained at step (1.) as an initial guess. | | . Additional hints and best practices: . Scale all parameters to be unity and dimensionless | Add small multiples of identity to the Hessian matrix: $ nabla nabla chi^2 longrightarrow nabla nabla chi^2 + epsilon I$ | . Numerical Example . import jax import jax.numpy as np import numpy as onp import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import pickle from numpy import random from scipy import integrate from scipy import optimize from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from functools import partial,reduce import matplotlib.patheffects as PathEffects mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def addtxt(ax, x, y, txt, fs=8, lw=3, clr=&#39;k&#39;, bclr=&#39;w&#39;, rot=0): &quot;&quot;&quot;Add text to figure axis&quot;&quot;&quot; return ax.text(x, y, txt, color=clr, ha=&#39;left&#39;, transform=ax.transAxes, rotation=rot, weight=&#39;bold&#39;, path_effects=[PathEffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs) def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) . Let&#39;s define our ideal peak and background signals . begin{align} f_{ mathrm{background}}(x; b_1, b_2) &amp;= b_1 + b_2 x f_{ text{peak}}(x; A, X) &amp;= A exp{ left(- frac{(x-X)^2}{2 w^2} right)} end{align}with $w=0.01$ the width of the peaks. Then, for an $M$-peak model, the ideal signal would be given as begin{align} f(x; b_1, b_2, {A_i, X_i }) &amp;= f_{ mathrm{background}}(x;b_1, b_2) + sum_{i=1}^M f_{ text{peak}}(x; A_i, X_i) end{align} . def ideal_bckg(b1,b2,x): &quot;&quot;&quot;Linear Backbround Signal&quot;&quot;&quot; return b1 + b2*x def ideal_peak(xi,ai,x): &quot;&quot;&quot;Ideal Gaussian Peak&quot;&quot;&quot; return ai*np.exp(-(x-xi)**2/(2*0.01**2)) def squash(b1,b2,pos,amp): &quot;&quot;&quot;Return signal parameters as single array&quot;&quot;&quot; n = len(pos) p = np.empty(2 + 2*n, dtype=pos.dtype) p = jax.ops.index_update(p, jax.ops.index[0],b1) p = jax.ops.index_update(p, jax.ops.index[1],b2) p = jax.ops.index_update(p, jax.ops.index[2::2],pos) p = jax.ops.index_update(p, jax.ops.index[3::2],amp) return p def unsquash(p): &quot;&quot;&quot;Return signal parameters independently&quot;&quot;&quot; return p[0], p[1], p[2::2], p[3::2] def ideal_signal0(p, x): &quot;&quot;&quot;Ideal (noiseless) signal&quot;&quot;&quot; b1,b2,xi,ai = unsquash(p) return ideal_bckg(b1,b2,x) + np.sum(ideal_peak(xi,ai,x), axis=-1) ideal_signal = jit(vmap(ideal_signal0, in_axes=(None, 0), out_axes=0)) . Let&#39;s generate some synthetic data for a five peak signal (roughly corresponding to Fig. 4.4 in Sivia&#39;s book) . peaks_xi = (np.array([11.5, 11.55, 12.15, 12.8, 13.5])-11.0)/3.0 peaks_ai = np.array([1.8, 7.0, 3.5, 3.8, 8.9])/10 peaks = np.vstack([peaks_xi, peaks_ai]).transpose() line_b1 = 0.3 line_b2 = 0.02 params = squash(line_b1, line_b2, peaks_xi, peaks_ai) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . fig, [ax,bx] = plt.subplots(figsize=(18,9),ncols=2) theta = np.linspace(0, 1., num=300) signal = ideal_signal(params, theta) ax.plot(theta, signal, color=color[&#39;mid&#39;]) ax.plot(theta, ideal_bckg(line_b1, line_b2, theta), color=color[&#39;mid_highlight&#39;], ls=&#39;:&#39;, lw=4) bx.plot(theta-0.5, ideal_peak(theta, 1, 0.5), color=color[&#39;mid&#39;]) for x0,a0 in zip(peaks_xi, peaks_ai): baseline = ideal_bckg(line_b1, line_b2, x0) ax.vlines(x0, baseline, baseline + a0, alpha=0.8, color=color[&#39;superfine&#39;]) ax.set_title(r&#39;Pure Signal&#39;) bx.set_title(r&#39;Ideal Peak&#39;) bx.set_xlim(-0.06, 0.06) ax.set_ylim(0.2, 1.3) fig.tight_layout() plt.show() . Now let&#39;s add some noise to this ideal signal, to see how our peak finding algorithm behaves. We generate 5 noisy datasets, with increasing amplitude of the signal noise. . def noisy_signal(μ, σ, n): &quot;&quot;&quot;Given ideal signal μ, return simulated measurement, averaged over n samples, with variance σ&quot;&quot;&quot; counts = onp.random.normal(μ, σ, size=(n,)+μ.shape) avg = onp.average(counts, axis=0) err = onp.std(counts, axis=0) return pd.DataFrame({&#39;avg&#39;:avg, &#39;err&#39;:err}) random.seed(12345) σs = np.linspace(0.02, 0.1, num=5) datas = [noisy_signal(signal, σ, 100) for σ in σs] . fig, axes = plt.subplots(figsize=(18,18), ncols=2, nrows=3, sharex=True, sharey=True) for ax,data,σ in zip(axes.flatten(), datas, σs): ax.errorbar(theta, data[&#39;avg&#39;], data[&#39;err&#39;], ls=&#39;None&#39;, lw=1.5, alpha=0.8, color=color[&#39;mid_highlight&#39;]) ax.plot(theta, data[&#39;avg&#39;] , marker=&#39;o&#39;, ms=6, mfc=&#39;None&#39;, mew=1.2, color=color[&#39;dark&#39;], ls=&#39;None&#39;) for ax,σ in zip(axes.flatten()[:-1], σs): for x0,a0 in zip(peaks_xi, peaks_ai): baseline = ideal_bckg(line_b1, line_b2, x0) ax.vlines(x0, baseline, baseline + a0, alpha=0.8, color=color[&#39;superfine&#39;]) addtxt(ax, 0.1, 0.9, f&#39;$ sigma = {σ:3.2f}$&#39;, fs=18) fig.tight_layout() . To find the optimum parameters, we minimize the $ chi^2$, begin{align} chi^2 &amp;= sum_k left( frac{y_k - f_k}{ sigma_k} right)^2 end{align} . However, keep in mind that the algorithm outlined above requires three seperate minimizations: . Background only begin{align} f_k = f_{ text{background}}(x_k; b_1, b_2) end{align} | Background and peak amplitudes begin{align} f_k = f(x_k; b_1, b_2, {A_i, underline{X_i} }) end{align} where the underline refers to parameters that are held fixed (i.e., the peak positions) | Full minimization begin{align} f_k = f(x_k; b_1, b_2, {A_i,X_i }) end{align} | . In practice we minimize the average deviations, i.e., $ chi^2/N$. So in the end, we should not forget to rescale both $ chi^2$ and the Hessian $ nabla nabla chi^2$. . Let&#39;s define the corresponding loss functions. Thanks to the fact that we are using jax, we get the gradients and the hessians for free (basically). This greatly improves the convergence of the minimization routines. . @jit def loss_bckg(p, x, y, dy): b1,b2 = p ymodel = ideal_bckg(b1, b2, x) return np.mean(((y-ymodel)/dy)**2) dloss_bckg = jit(grad(loss_bckg)) @jit def loss(p, x, y, dy): ymodel = ideal_signal(p, x) return np.mean(((y - ymodel)/dy)**2) dloss = jit(grad(loss)) hess_loss= hessian(loss) @jit def loss_fixpos(p, pos, x, y, dy): param = squash(p[0], p[1], pos, p[2:]) return loss(param, x, y, dy) dloss_fixpos = jit(grad(loss_fixpos)) . Thanks to the fact that we are using jax, we get the gradients and the hessians for free (basically). This greatly improves the convergence of the minimization routines. . The peak finding algorithm can then be written as follows . def findNewPeak(x,y,dy,old_guess): &quot;&quot;&quot;Determine number of positions of peaks in a given signal x : measured x data y : measured y data dy : measured y uncertainty &quot;&quot;&quot; ### 1D scan over x to locate peak candidate def fitBckAmp(init, peak_pos, x, y, dy): &quot;&quot;&quot;Optimize background and amplitude parameters given position of the peaks x : measured x data y : measured y data dy : measured y uncertainty peak_pos : fixed position of peaks init : inital guess for background (b, m) and peak amplitudes (ai) &quot;&quot;&quot; opt0 = optimize.minimize(loss_fixpos, init, args=(peak_pos, x, y, dy), method=&#39;BFGS&#39;, jac=dloss_fixpos, options={&#39;disp&#39;:0}) return opt0 old_b1, old_b2, old_pos, old_amp = unsquash(old_guess) old_n = len(old_pos) #### #### initial guess for background and amplitude parameters #### init0 = np.zeros(2 + old_n + 1) init0 = jax.ops.index_update(init0, jax.ops.index[:2], old_guess[:2]) # b_1,b_2 init0 = jax.ops.index_update(init0, jax.ops.index[2:2+old_n], old_amp) # a_i (i=1,n-1) init0 = jax.ops.index_update(init0, jax.ops.index[old_n], np.minimum(np.min(old_amp)*0.25, np.min(y))) #a_n # fixed position parameters pos0 = np.zeros(old_n + 1) pos0 = jax.ops.index_update(pos0, jax.ops.index[:old_n], old_pos) opt0 = [fitBckAmp(init0,jax.ops.index_update(pos0, jax.ops.index[old_n], x0),x,y,dy) for x0 in x] # scan over x, optimizing (b1,b2,ai) idx0 = np.argmin(np.array([dmy[&#39;fun&#39;] for dmy in opt0])) # best guess for new peak @ min(chi2) p0 = opt0[idx0][&#39;x&#39;] # optimized (b1,b2,ai) parameters ### ### Full optimization in 2n + 2 parameters space ### pos0 = jax.ops.index_update(pos0, jax.ops.index[old_n], x[idx0]) init = squash(p0[0], p0[1], pos0, p0[2:]) opt1 = optimize.minimize(loss, init, args=(x,y,dy), method=&#39;Nelder-Mead&#39;) opt = optimize.minimize(loss, opt1[&#39;x&#39;], args=(x,y,dy), method=&#39;BFGS&#39;, jac=dloss) opt[&#39;chi2&#39;] = opt[&#39;fun&#39;]*len(x) opt[&#39;H&#39;] = hess_loss(opt[&#39;x&#39;], x, y, dy)*len(x) opt[&#39;H_inv&#39;]= np.linalg.inv(opt[&#39;H&#39;]) #rescale approximate hessian given by bfgs opt[&#39;hess_inv&#39;] = opt[&#39;hess_inv&#39;]/len(x) opt[&#39;hess&#39;] = np.linalg.inv(opt[&#39;hess_inv&#39;]) return opt def fitBackground(x,y,dy): &quot;&quot;&quot; Fit background signal &quot;&quot;&quot; opt = optimize.minimize(loss_bckg, np.array([np.min(y), 0.0]), args=(x,y,dy), method=&#39;Nelder-Mead&#39;) return opt[&#39;x&#39;] def findPeaks(x,y,dy,Mmax): opts = [] oldguess = fitBackground(x,y,dy) print(&#39;*** Searching for Peaks ***&#39;) for M in range(1,Mmax+1): opt = findNewPeak(x,y,dy,oldguess) opts.append(opt) oldguess = opt[&#39;x&#39;] print(f&quot; t M = {M}: t χ2 = {opt[&#39;chi2&#39;]:8.3e}, niter = {opt[&#39;nit&#39;]:4d}, Converged = {opt[&#39;success&#39;]:3b} : {opt[&#39;message&#39;]}&quot;) return opts . opts = [findPeaks(theta, data[&#39;avg&#39;].values, data[&#39;err&#39;].values, 10) for data in datas] . *** Searching for Peaks *** M = 1: χ2 = 9.849e+03, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 3.237e+03, niter = 9, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 1.626e+03, niter = 12, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 1.466e+02, niter = 14, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.382e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.285e+00, niter = 19, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.214e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 3.159e+00, niter = 24, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 3.097e+00, niter = 25, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 3.034e+00, niter = 28, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 2.663e+03, niter = 7, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 8.824e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 4.246e+02, niter = 12, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 3.677e+01, niter = 15, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.118e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.051e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.006e+00, niter = 19, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.964e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.928e+00, niter = 20, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.898e+00, niter = 22, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 1.181e+03, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 4.212e+02, niter = 9, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 2.119e+02, niter = 11, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 2.007e+01, niter = 14, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 3.538e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 3.467e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 3.424e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 3.383e+00, niter = 18, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 3.343e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 3.304e+00, niter = 18, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 6.756e+02, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 2.144e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 1.084e+02, niter = 13, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 1.188e+01, niter = 12, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 2.853e+00, niter = 13, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 2.758e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 2.687e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.630e+00, niter = 14, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.577e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.539e+00, niter = 15, Converged = 1 : Optimization terminated successfully. *** Searching for Peaks *** M = 1: χ2 = 4.331e+02, niter = 6, Converged = 1 : Optimization terminated successfully. M = 2: χ2 = 1.453e+02, niter = 10, Converged = 1 : Optimization terminated successfully. M = 3: χ2 = 7.171e+01, niter = 10, Converged = 1 : Optimization terminated successfully. M = 4: χ2 = 8.578e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 5: χ2 = 2.993e+00, niter = 14, Converged = 1 : Optimization terminated successfully. M = 6: χ2 = 2.903e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 7: χ2 = 2.825e+00, niter = 16, Converged = 1 : Optimization terminated successfully. M = 8: χ2 = 2.757e+00, niter = 15, Converged = 1 : Optimization terminated successfully. M = 9: χ2 = 2.713e+00, niter = 17, Converged = 1 : Optimization terminated successfully. M = 10: χ2 = 2.679e+00, niter = 18, Converged = 1 : Optimization terminated successfully. . begin{align} ln{P(M lvert D, I)}&amp;= - frac{ chi_0^2}{2} + ln{M!} + M Big[ ln{4 pi} - ln big(A_{ text{max}} left(X_{ text{max}}- X_{ text{min}} right) big) Big] - frac{1}{2} ln{ left[ det{ left( left. nabla nabla chi^2 right lvert_{ Theta_0} right)} right]} + ln{4 pi} end{align} def dump(fname, obj): with open(fname, &#39;wb&#39;) as fp: pickle.dump(obj, fp, protocol=pickle.HIGHEST_PROTOCOL) def load(fname): with open(fname, &#39;rb&#39;) as fp: return pickle.load(fp) . def logposterior(vol, opt): &quot;&quot;&quot;Calculate log posterior probability vol: prior volume term (Amax (Xmax - Xmin)) opt: result of minimization procedure Returns: log(P(M|D,I)) = -0.5 χ^2 + log(M!) + M[log(4π) - log(vol)] - 0.5log(det( nabla nabla χ^2)) &quot;&quot;&quot; def logfact(n): &quot;&quot;&quot;Computes log(n!)&quot;&quot;&quot; return np.sum([np.log(i) for i in range(1,n+1)]) hess = opt[&#39;H&#39;] M = (len(opt[&#39;x&#39;]) - 2)//2 logp = -opt[&#39;chi2&#39;]/2 + logfact(M) + M*np.log(4*np.pi/vol) - 0.5*np.log(np.linalg.det(hess)) return logp def normalize(logp): &quot;&quot;&quot;Computed normalized posteriors&quot;&quot;&quot; α = reduce(np.logaddexp, logp) return logp - α def estimates(opt): &quot;&quot;&quot;Compute optimum position/amplitude and background parameters&quot;&quot;&quot; Θ = opt[&#39;x&#39;] Σ = opt[&#39;H_inv&#39;] sig = np.sqrt(2.0*np.diagonal(Σ)) b1,b2,amp,pos = unsquash(opt[&#39;x&#39;]) db1,db2,damp,dpos = unsquash(sig) return [b1,db1], [b2, db2], [amp, damp], [pos, dpos] . logps, bests = [], [] for opt, data in zip(opts, datas): y = data[&#39;avg&#39;].values arange = integrate.simps(y,x=theta) / integrate.simps(ideal_signal(squash(np.min(y), 0.0, np.array([0.5]), np.array([1.0])), theta), x=theta) xrange = np.max(theta) - np.min(theta) logp = normalize(np.array([logposterior(arange*xrange, opt_m) for opt_m in opt])) bests.append(np.argmax(logp)) # for each signal (corresponding to different σ) find the model M with the hight posterior probability logps.append(np.array([lp / np.log(10) for lp in logp])) # report log posteriors in log10 basis . fig, ax = plt.subplots(figsize=(12,6)) for lp,sig,c in zip(logps, σs, mpl.cm.viridis(np.linspace(0,1,len(σs)))): ax.plot(np.arange(1,11), lp, marker=&#39;o&#39;, color=c, label=f&#39;$ sigma = {sig:.2f}$&#39;, alpha=0.9) ax.set_xlim(3,9) ax.set_ylim(-12,1) ax.set_ylabel(r&#39;$ log_{10}{P left(M| D I right)}$&#39;, fontsize=22) ax.set_xlabel(r&#39;Number of peaks $M$&#39;, fontsize=22) ax.legend(fontsize=22) plt.show() . We see that for the three signals with the larger signal/noise ratio, we are able to correctly infer the number of peaks, i.e., the posterior $P(M=5 lvert D, I)$ maximum. For the two signals with the large noise amplitude, we have a flatter posterior, with $P(M=4 lvert D, I) gtrsim P(M=5 lvert D, I)$. . Now, let&#39;s plot the peak positions and amplitudes for the model $M$ with the highest posterior, for each of the five signals. . fig, ax = plt.subplots(figsize=(12,9)) print(f&#39;Background parameters:&#39;) colors = mpl.cm.viridis(np.linspace(0, 1, len(σs))) for i,opt in enumerate(opts): best,sig,c = bests[i], σs[i], colors[i] best_inter, best_slope, best_pos, best_amp = estimates(opt[best]) print(f&#39;σ = {sig:.2f} m = {best_slope[0]:.3f} +/- {best_slope[1]:.3f}, b = {best_inter[0]:.3f} +/- {best_inter[1]:.3f}&#39;) ax.errorbar(best_pos[0], best_amp[0], xerr=best_pos[1], yerr=best_amp[1], ls=&#39;none&#39;, marker=&#39;s&#39;, color=c, mew=3, mfc=&#39;None&#39;, alpha=0.6, label=f&#39;$ sigma = {sig:.2f}$&#39;) ax.plot(peaks_xi, peaks_ai, marker=&#39;x&#39;, ms=22, mew=3, ls=&#39;None&#39;, color=&#39;C2&#39;) ax.legend(fontsize=22) ax.set_xlabel(r&#39;Peak Position $x$&#39;, fontsize=22) ax.set_ylabel(r&#39;Amplitude&#39;, fontsize=22) plt.show() . Background parameters: σ = 0.02 m = 0.020 +/- 0.004, b = 0.300 +/- 0.002 σ = 0.04 m = 0.022 +/- 0.008, b = 0.299 +/- 0.005 σ = 0.06 m = 0.021 +/- 0.013, b = 0.299 +/- 0.007 σ = 0.08 m = 0.009 +/- 0.017, b = 0.308 +/- 0.010 σ = 0.10 m = 0.012 +/- 0.021, b = 0.306 +/- 0.012 . As expected, for the three signals with the smallest noise we are able to correctly infer not only the number of peaks, but their positions and amplitudes. For the signals with large noise, the two peaks at $x sim 0.2$ become indistinguishable, merging into one peak with a larger amplitude. .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/model%20selection/2020/06/19/Model-Selection-How-Many-Lines.html",
            "relUrl": "/data%20analysis/model%20selection/2020/06/19/Model-Selection-How-Many-Lines.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian Parameter Estimation with Outliers",
            "content": "Motivation / Disclaimer . This is the second in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . Having covered the basics of Bayesian Data Analysis, following Chapters 2 and 3 of the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling, we now consider a slightly more complicated case. . Now, we study how to perform parameter estimation in the presence of outliers. . References . This topic is discussed in some detail in Sivia&#39;s book (Ch. 8, Least-Squares Extensions), as well as Jaynes&#39; (Ch. 21, Outliers and Robustness). . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | . In addition, we also found the following paper to be very useful and in line with the discussions found in the books mentioned above . Data analysis recipes: Fitting a model to data, D. W. Hogg, J. Bovy, D. Lang, arXiv:1008.4686 (2010) | . Below we basically follow Jaynes, using the data of Hogg et al. . Intro . Let us consider that our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. Furthermore, assume that on prior information, we know that the results of any given measurement can be either good or bad. . When analyzing the data, in order to determine the parameters $ Theta$, we want to make sure that we use a robust method that is able to account for these outliers. As always with these parameter estimation problems, the quantity we are interested is the following posterior distribution begin{align} P( Gamma lvert D, I) &amp; propto P(D lvert Gamma ,I) P( Gamma lvert I) &amp;= L( Gamma) , Pi( Gamma) L( Gamma) &amp;= P(D, lvert Gamma ,I) Pi( Gamma) &amp;= P( Gamma lvert I) end{align} with $ Gamma=( Theta, Xi)$, where $ Theta$ is parametrizing the target function $f(X; Theta)$, and $ Xi$ refers to all the remaining parameters (e.g., those related to the goodness/badness of the points). . The &quot;Mixture&quot; or &quot;Two-Model&quot; Model . To evaluate the likelihood, we consider the case of good/bad points separately. If a point is good, it is assumed to be sampled from a distribution $P_{ text{Good}}=G$, whereas bad points are sampled from $P_{ text{Bad}}= B$. . Assuming Gaussian errors, we set begin{align} P(y_k lvert mathrm{Good}_k, Gamma, I) = G(y_k lvert Theta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 right) P(y_k lvert mathrm{Bad}_k, Gamma, I) = B(y_k lvert Theta, eta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 + eta^2 right) end{align} Good points will be normally distributed around the true values $f(X_k; Theta)$, with a standard deviation given by the measured error bars. Bad points, on the other hand, will be distributed around the true values with a much larger variance, given by $ sigma_k^2 + eta^2$. Here, $ eta^2$ is an additional parameter which defines the variance of the bad points. I think that whether or not this represents the true physical origin behind the outliers is of secondary importance (as we are not trying to make inferences about this process). What matters here is that we can build a probabilistic model that can explain the data, i.e., the fact that we can have good/bad points, and that the bad points can show very large deviations with respect to the true values. I think we would obtain roughly the same results if we uncoupled the Bad data from the model (as done in Jaynes treatment), begin{align} y_k lvert text{Bad} sim mathcal{N} left(c, eta^2 right) end{align} with $c$ an additional nuissance parameter to be learned from the data. . Assuming independent measurements, the full likelihood $D= {y_k }_{k=1}^N$ can be expressed as a product of likelihoods for each individual measurement, such that . begin{align} L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[P(y_k lvert Gamma, I) Big] &amp;= prod_k Big[P big(y_k,( mathrm{Good}_k text{ or } mathrm{Bad}_k) lvert Gamma, I big) Big] &amp;= prod_k Big[P big(y_k, mathrm{Good}_k lvert Gamma, I big) + P big(y_k, mathrm{Bad}_k lvert Gamma, I big) Big] &amp;= prod_k Big[P big( mathrm{Good}_k lvert Gamma, I big) P(y_k lvert mathrm{Good}_k, Gamma, I) + P big( mathrm{Bad}_k lvert Gamma, I big) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] end{align} Where we have simply used marginalization over the Good/Bad state of the points. Now, let us assume that the probability of observing a given sequence of good/bad points is invariant under permutations. What matters is just the number of good/bad points, not the order in which they were obtained. Let $u$, which we don&#39;t know, be the probability that any given point is good, Jaynes calls this the &quot;purity&quot; of the data. The likelihood is then given by begin{align} P( mathrm{Good}_k lvert Gamma, I)&amp;= u L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[uP(y_k lvert mathrm{Good}_k, Gamma, I) + (1-u) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . The posterior for $ Theta$, which are the parameters we are actually interested in, is obtained by marginalizing over $u$ and $ eta$ begin{align} P( Theta lvert D, I)&amp;= iint mathrm{d}u mathrm{d} eta , P( Theta, u, eta lvert D, I) &amp;= frac{ iint mathrm{d}u mathrm{d} eta , L( Theta, u, eta) Pi( Theta, u, eta)}{ iiint mathrm{d} Theta mathrm{d}u mathrm{d} eta ,L( Theta, u, eta) Pi( Theta, u, eta)} end{align} where we have explicitly included the normalization constant in the definition. . We can use this model, with hyper-parameters $u$ and $ eta$, to compute the posterior for $ Theta$, and thus find the &quot;best-fit&quot; to the data, in the presence of outliers. . Digression on the two-model model . Let us look in a bit more detail into what exactly we are calculating here. For ease of reading (to avoid having to write $P$ all over the place), we will again follow Jaynes and rewrite the prior $ Pi( Gamma)$ as begin{align} Pi( Theta, u, eta)&amp;= P( Theta, u, eta lvert I) &amp;= P( Theta lvert I) P(u, eta lvert Theta, I) &amp;= Pi( Theta) H(u, eta lvert Theta, I) end{align} . where $ Pi( Theta)=P( Theta lvert I)$ and $H(u, eta lvert Theta, I)=P(u, eta lvert Theta, I)$. . Now, defining the &quot;pseudo-likelihood&quot; $ bar{L}$ as begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) end{align} . The marginalized posterior for $ Theta$ becomes begin{align} P( Theta lvert D, I) &amp;= frac{ bar{L}( Theta) Pi( Theta)}{ int mathrm{d} Theta bar{L}( Theta) Pi( Theta)} end{align} . To understand what this pseudo-likelihood is giving us, let&#39;s first expand the definition of $L( Theta, u, eta)$ in terms of the good and bad likelihoods $G$ and $B$. We obtain the following begin{align} L( Theta, u, eta) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u)B(y_k lvert Theta, eta) Big] &amp;= u^N prod_i G(y_i lvert Theta) +u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) prod_{i ne j} G(y_i lvert Theta) &amp; quad + u^{N-2}(1-u)^{2} sum_{j&lt;k} B(y_j lvert Theta, eta) B(y_k lvert Theta, eta) prod_{i ne j,k} G(y_i lvert Theta) &amp; quad vdots &amp; quad + (1-u)^{N} prod_{j}^N B(y_j lvert Theta, eta) &amp;=u^N L( Theta) &amp; quad+ u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) L_j( Theta) &amp; quad+ u^{N-2}(1-u)^2 sum_{j&lt;k}^N B(y_j lvert Theta, eta)B(y_k lvert Theta, eta) L_{jk}( Theta) &amp; quad vdots &amp; quad+ (1-u)^N B(y_1 lvert Theta, eta)B(y_2 lvert Theta, eta) cdots B(y_N lvert Theta, eta) &amp;= L^{(0)}( Theta, u, eta)+ L^{(1)}( Theta, u, eta) + L^{(2)}( Theta, u, eta) + ldots + L^{(N)}( Theta, u, eta) end{align} . Here, we use $L( Theta) = prod_i G(y_i lvert Theta)$ to denote the likelihood of the pure model, i.e., one without outliers. Likewise $L_j( Theta)$ is the likelihood of observing $ {y_{i ne j} }$ from the pure model, $L_{jk}$ that of observing $ {y_{i ne j,k} }$, etc. . Plugging this expression into the definition of $ bar{L}$, begin{align*} bar{L} &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , left[L^{(0)}( Theta, u, eta) + L^{(1)}( Theta, u, eta) + cdots + L^{(N)}( Theta, u, eta) right] H(u, eta lvert Theta, I) end{align*} we would obtain the following for the first term begin{align} iint mathrm{d}u mathrm{d} eta L^{(0)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^N L( Theta) H(u, eta lvert Theta, I) &amp;= L( Theta) int mathrm{d}u , u^N int mathrm{d} eta , H(u, eta lvert Theta,I) &amp;= L( Theta) underbrace{ int mathrm{d}u , u^N H(u lvert Theta, I)}_{ text{Probability that all points are good}} end{align} where in the second to last step we marginilize $h(u, eta lvert Theta, I)$ over $ eta$. Notice that the integral in the last line evaluates to the probability all points are good, regardless of $u$ and conditioned on $ Theta$ and $I$, begin{align*} P( text{All points good} lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good}, u lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good} lvert u, Theta, I) P(u lvert Theta, I) &amp;= int mathrm{d}u , u^N P(u lvert Theta, I) = int mathrm{d}u , u^N H(u lvert Theta, I) end{align*} . We would obtain something similar for the second term . begin{align} iint mathrm{d}u mathrm{d} eta , L^{(1)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^{N-1}(1-u) B(y_j lvert Theta, eta) L_j( Theta) H(u, eta lvert Theta, I) &amp;= L_j( Theta) underbrace{ int B(y_j lvert Theta, I) underbrace{ mathrm{d} eta int mathrm{d}u , u^{N-1}(1-u) H(u, eta lvert Theta, I)}_{ textrm{Probability that any given point } y_j textrm{ is bad,} atop textrm{all others are good, and } eta textrm{ lies in } ( eta, eta+ mathrm{d} eta) textrm{, conditioned on } Theta textrm{ and } I.}}_{ textrm{Probability that the } j textrm{-th point is bad, with value } y_j, atop textrm{and all others are good, conditioned on } Theta textrm{ and } I.} end{align} Thus, we see that the pseudo-likelihood can be expressed as . begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= big( textrm{Probability all data is good} big) times big( textrm{Likelihood for all data} big) &amp; ,+ sum_j big( textrm{Probability only } x_j textrm{ is bad} big) times big( textrm{Likelihood without } x_j big) &amp; ,+ sum_{j&lt;k} big( textrm{Probability only } x_j textrm{ and } x_k textrm{ are bad} big) times big( textrm{Likelihood without } x_j textrm{ or } x_k big) &amp; ; vdots &amp; , + sum_j big( textrm{Probability only } x_j textrm{ is good} big) times big( textrm{Likelihood with only } x_j big) &amp; , + big( textrm{Probability all data is bad} big) end{align}which is just a weighted-average of the &quot;pure&quot; likelihoods $L( Theta)$, over all possible combinations of good/bad points, weighted over the corresponding prior probabilities. Note that the sums here are over non-repeating indices. . The Exponential Model . Above, we saw that we could express the likelihood as a sum of likelihoods for all possible combinations of good/bad points. Then, the pseudo-likelihood was obtained as an average over the hyper-parameters $u$ and $ eta$. . Consider the contribution to the likelihood form the term with exactly $M$ good points ($m=N-M$ bad points) begin{align} L^{(m)}( Theta, u, eta)&amp;= sum_{j_1 &lt; j_2 &lt; cdots&lt; j_m}u^{M}(1-u)^{m} L_{j_1, ldots,j_m}( Theta) B(y_{j_1} lvert Theta, eta) ldots B(y_{j_m} lvert Theta, eta) &amp;= sum_{j_1&lt;j_2&lt; cdots&lt; j_m} u^{M}(1-u)^{m} Big( prod_{i ne j_1, ldots,j_m} G(y_i lvert Theta) Big) Big(B(y_{j_1} lvert Theta, eta) cdots B(y_{j_m} lvert Theta, eta) Big) end{align} . Up to now we haven&#39;t really cared about which point are good/bad. However, we could just as well introduce additional hyperparameters $q_i$ to keep track of this information. Let begin{align} q_i = begin{cases} 1 &amp; textrm{point } i textrm{ is good} 0 &amp; textrm{point } i textrm{ is bad} end{cases} end{align} . Such that $q_{j_1} = q_{j_2}= cdots = q_{j_m}= 0$, $q_{i ne j_1, j_2, cdots j_m} = 1$, and $M= sum_i q_i$. The likelihood term we are considering then becomes begin{align} L^{(m)}( Theta, u, eta)&amp;= underbrace{ sum_{j_1&lt;j_2&lt; cdots&lt;j_m} sum_{q_1,q_2, ldots q_N} delta_{q_{j_1}, cdots,q_{j_m}}^{0} delta_{i ne j_1, cdots j_m}^{1}}_{ sum_{ {q_j }_ text{m bad}}} underbrace{ phantom{ prod_i^N}u^M(1-u)^m quad}_{P( {q_k } lvert Gamma, I)} underbrace{ prod_i^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{= P(D lvert Gamma, {q_k },I)} end{align} . allowing us to write down the full likelihood as begin{align} L( Theta, u, eta) &amp;= sum_{ {q_j }_{m=0}} u^{N-m}(1-u)^{m} prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp;+ sum_{ {q_j }_{m=1}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^q_i times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp; vdots &amp;+ sum_{ {q_j }_{m=N}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) L( Theta, u, eta) &amp;= sum_{q_1,q_2, ldots q_n} underbrace{ phantom{ Pi_i}u^{N-m} (1-u)^m quad}_{= P( {q_i } lvert Theta, u, eta, I)} underbrace{ prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{P(D lvert {q_i }, Theta, u, eta) = L( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots,q_N}P( {q_i } lvert Theta, u, eta, I) times P(D lvert {q_i }, Theta, u, eta) end{align} . Recall the definition of the posterior begin{align} P( Theta, u, eta lvert D, I)&amp; propto L( Theta, u, eta) Pi( Theta, u, eta) &amp;= sum_{q_1, ldots q_N} L( {q_i }, Theta, u, eta) underbrace{P( {q_i } lvert Theta, u, eta, I) P( Theta, u, eta lvert I)}_{= P( {q_i }, Theta, u, eta lvert I) equiv Pi( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots q_N} L( {q_i }, theta, u, eta) Pi( {q_i }, Theta, u, eta) &amp; propto sum_{q_1, ldots q_N} P( Theta, u, eta, {q_i } lvert D, I) end{align} Thus, we see that the posterior for the &quot;mixture&quot; model is nothing but the marginalized posterior (over $q_i$) of the &quot;exponential&quot; model, which considers the goodness/badness of each of the points explicitly (with a global purity value $u$). . Within the exponential model, we then have begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) end{align} . As always, it&#39;s more convenient to work with the logarithm of the likelihood begin{align} ln{L} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} ln{G(y_i lvert Theta)} + sum_{ text{bad}} ln{B(y_i lvert Theta, eta)} &amp;= sum_{ text{good}} ln{ left[ frac{1}{ sqrt{2 pi sigma_i^2}} exp left( frac{-(y_i - f(X_i; Theta))^2}{2 sigma_i^2} right) right]} + sum_{ text{bad}} ln{ left[ frac{1}{ sqrt{2 pi( sigma_i^2+ eta^2)}} exp{ left( frac{-(y_i-f(X_i; Theta))^2}{2( sigma_i^2+ eta^2)} right)} right]} &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . Numerical Example (Mixture Model) . We will use the data given in the paper by Hogg, Bovy and Lang. While they consider the case of correlated x/y noise, here we will simply consider the case of noise in $y$. . #import numpy as np #### If using windows... import jax import jax.numpy as np import numpy as onp import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano as th import theano.tensor as tt from scipy.optimize import minimize ### remove all jax stuff on windows... from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from matplotlib.collections import EllipseCollection from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib.collections as clt . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_sig2= quantiles_sig[1:-1] quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, *,nbins,scatter, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2, nbins) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) def colorbar(axes, mappable, *, loc=&quot;right&quot;, size=&quot;5%&quot;, pad=.1): &quot;&quot;&quot;Add colorbar to axes&quot;&quot;&quot; divider = make_axes_locatable(axes) cax = divider.append_axes(loc, size=size, pad=0.1) cb = plt.colorbar(mappable, cax=cax) return cb . # data taken from Hogg et al. x = np.array([201.,244, 47,287,203, 58,210,202,198,158,165,201,157,131,166,160,186,125,218,146], dtype=np.float64) y = np.array([592.,401,583,402,495,173,479,504,510,416,393,442,317,311,400,337,423,334,533,344], dtype=np.float64) σx = np.array([ 9, 14, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 5, 6, 6, 5, 9, 8, 6, 5], dtype=np.float64) σy = np.array([ 61, 25, 38, 15, 21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22], dtype=np.float64) ρxy= np.array([-0.84, 0.31, 0.64, -0.27, -0.33, 0.67, -0.02, -0.05, -0.84, -0.69, -.30, -0.46, -0.03, 0.50, 0.73, -0.52, 0.90, 0.40, -0.78, -0.56], dtype=np.float64) i_sort = np.argsort(x) x_sorted = x[i_sort] def computeErrors(): # Compute full sigma matrix Σ = onp.zeros((len(y), 2, 2)) Σ[:,0,0] = σx**2 Σ[:,0,1] = Σ[:,1,0] = σx*σy*ρxy Σ[:,1,1] = σy**2 Σinv = np.array([np.linalg.inv(s) for s in Σ]) # Diagonalize n = len(Σ) λ,η,θ = onp.zeros((n,2)), onp.zeros((n,2,2)), onp.zeros(n) for i, σ in enumerate(Σ): w, v = onp.linalg.eig(σ) # unordered eigenvalues and eigenvectors w, v = onp.real_if_close(w), onp.real_if_close(np.transpose(v)) w, v = zip(*sorted(zip(w,v), reverse=True)) # descending order λ[i,:] = onp.array(w) η[i,:,:]= onp.array(v) θ[i] = onp.arctan2(v[0][1], v[0][0])*180/np.pi # CCW angle with respect to x-axis of major axis (eigen-vector) return np.array(λ),np.array(η),np.array(θ),np.array(Σ) λ,η,θ,Σ = computeErrors() . The full covariance matrix for the data given by Hogg et al. can be computed as begin{align*} Sigma^2 &amp;= begin{pmatrix} sigma_x^2 &amp; rho_{xy} sigma_x sigma_y rho_{xy} sigma_x sigma_y &amp; sigma_y^2 end{pmatrix} end{align*} with the values of $ sigma_x$, $ sigma_y$, and $ rho_{xy}$ given in the paper. . Let&#39;s plot the full data, using ellipses to represent the $1 sigma$ contours. . fig,ax = plt.subplots(figsize=(16,9)) ax.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;]) ec = EllipseCollection(2*np.sqrt(λ[:,0]), 2*np.sqrt(λ[:,1]), θ, units=&#39;xy&#39;, offsets=np.column_stack((x,y)), transOffset=ax.transData, edgecolor=color[&#39;dark&#39;], facecolor=&#39;None&#39;) ax.add_collection(ec); ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . Ignoring the correlated erros, we consider only the case of measurement errors in $y$, given by $ sigma_y$. The data now looks like this . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x,y,σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . (Naive) Least-Squares . Before trying our Bayesian solution, let us consider a naive (least-squares) fit to the data... . def func(x,θ): return θ[0]*x + θ[1] @jit def chi2(θ): return np.sum((y-func(x,θ))**2/σy**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 fit = minimize(chi2, np.ones(2), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) fit_ihess = np.linalg.inv(hess_chi2(fit[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 289.963723 Iterations: 13 Function evaluations: 15 Gradient evaluations: 15 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(50,300) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange,fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.legend() plt.show() . This is clearly not what we want. . Least-Squares extension . Now let&#39;s consider the (marinalized) mixture model, assuming uniform priors, all we need is to maximize the likelihood begin{align} L( Gamma) &amp;= P(D lvert Gamma, I) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . def func(x,θ): return θ[0]*x + θ[1] def normal(x,μ,σ2): return 1.0/np.sqrt(2*np.pi*σ2)*np.exp(-(x-μ)**2/(2*σ2)) @jit def loss(Γ): # = ln L θ,u,η = Γ[:2],Γ[2],np.exp(Γ[3]) # Γ = (Θ, u, ln(η)) ymodel = func(x,θ) loss_g = normal(y, ymodel, σy**2) loss_b = normal(y, ymodel, σy**2 + η**2) return -np.sum(np.log(u*loss_g + (1-u)*loss_b)) grad_loss = jit(grad(loss)) # nabla ln(L) hess_loss = hessian(loss) # nabla nabla ln(L) bounds = ((None,None), (None,None), (0,1), (None,None)) opt3 = minimize(loss, np.array([1.0, 1.0, 0.5, 1.0]), method=&#39;L-BFGS-B&#39;, bounds=bounds, options={&#39;maxiter&#39;:500}) opt2 = minimize(loss, opt3[&#39;x&#39;], method=&#39;TNC&#39;, bounds=bounds, jac=grad_loss, options={&#39;maxiter&#39;:200, &#39;disp&#39;:1}) opt = minimize(loss, opt2[&#39;x&#39;], method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 print(opt3[&#39;x&#39;]) print(opt2[&#39;x&#39;]) print(opt[&#39;x&#39;]) . Optimization terminated successfully. Current function value: 110.170025 Iterations: 1 Function evaluations: 3 Gradient evaluations: 3 [ 2.2374954 35.27694601 0.80797022 5.63764122] [ 2.23749529 35.27693403 0.80796973 5.63763495] [ 2.2374953 35.27693402 0.80796886 5.63763506] . Recall that within the Laplace approximation, the (posterior) average is given by the optimum, and the variance estimates are obtained from the diagonal of the inverse Hessian matrix (evaluated at the optimum value). . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot;μ_{l} = {avg:.3e}; σ_{l} = {sig:.3e} t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e}&quot;) . μ_m = 2.237e+00; σ_m = 1.100e-01 2.018e+00 &lt; m &lt; 2.457e+00 μ_b = 3.528e+01; σ_b = 1.860e+01 -1.930e+00 &lt; b &lt; 7.248e+01 μ_u = 8.080e-01; σ_u = 1.032e-01 6.015e-01 &lt; u &lt; 1.014e+00 μ_logη = 5.638e+00; σ_logη = 3.922e-01 4.853e+00 &lt; logη &lt; 6.422e+00 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(x_sorted[0],x_sorted[-1]) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.set_ylim(-100,800) ax.legend() plt.show() . This is much better, and all we had to do was introduce two additional hyper-parameters, the data purity $u$, and the bad variance $ eta$. . MC solution . Now let&#39;s see how an MC simulation for this mixture model performs . begin{align} P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} Pi( Gamma) &amp;= P(m,b u, eta lvert I) end{align}We will assume that the priors are independent of each other. For simplicity, we will also assume uniform priors for all parameters except $ eta$, for which we use Jeffreys&#39; prior begin{align} P(m,b,u, eta lvert I)&amp;= P(m lvert I) times P(b lvert I) times P(u lvert I) times P( eta lvert I) P(m lvert I)&amp;: textrm{Uniform} P(b lvert I)&amp;: textrm{Uniform} P( eta lvert I)&amp; propto frac{1}{ eta} end{align} . Finally, we note that PYMC requires us to specify log-likelihood $ ln{L( Gamma)}$ for our problem . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u): r2 = (y_obs - y_g)**2 G = tt.exp(-r2/(2*σ_obs**2)) / (tt.sqrt(2*onp.pi*σ_obs**2)) B = tt.exp(-r2/(2*(σ_obs**2 + η**2))) / (tt.sqrt(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(tt.log(u*G + (1-u)*B)) with pm.Model() as model_mix: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u}) # make sure there are no divergences with initial point for RV in model_mix.basic_RVs: print(RV.name, RV.logp(model_mix.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 likelihood -162.7700867258116 . with model_mix: trace_mix = pm.sample(10000, tune=20000, progressbar=True, random_seed = 1123581321) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logη, u, b, m] INFO:pymc3:NUTS: [logη, u, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 60000/60000 [01:03&lt;00:00, 950.70draws/s] The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. . pm.traceplot(trace_mix) plt.show(); . pm.plot_posterior(trace_mix, var_names=[&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_mix, var_names=[&#39;u&#39;, &#39;logη&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo,hi = pm.stats.hpd(trace_mix[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (Laplace)&quot;) print(f&quot; t t {lo:.3e} &lt; {l} &lt; {hi:.3e} t (MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (MC) -1.930e+00 &lt; b &lt; 7.248e+01 (Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (MC) 6.015e-01 &lt; u &lt; 1.014e+00 (Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (MC) . We see that the Laplace approximation gave us a pretty good estimate of the reliability! . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_mix[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Mixture Model MC&#39;) ax.legend(loc=4) plt.show() . Here we show our predictions for $y$, obtained from the posterior distribution given by the MC simulation, i.e., begin{align} langle y(x) rangle &amp;= int textrm{d} Theta f(x; Theta) p( Theta lvert D, I) end{align} where the shaded regions are showing the $ pm sigma$ and $ pm 2 sigma$ intervals. Note that this could also have been computed within the Laplace approximation. The benefit of the MC is that such averages are trivial to compute from the trace of the simulation. Again, we see excellent agreement between the MC predictions and the fit to the mixture-model. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace_mix[&#39;b&#39;], trace_mix[&#39;m&#39;], scatter=True, nbins=40, colors=[color[&#39;mid&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) ax.set_xlim(-100,100) ax.set_ylim(1.5, 3.0) plt.show() . Numerical Example (Exponential Model) . MC solution . We are now ready to try the MC solution to the full exponential model. Recall that in this case begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) ln{L( Gamma)} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . We use the same priors as above for $ Theta$, $u$, $ eta$, and use a Bernoulli distribution for the $ {q_k }$, such that . begin{align} Pi( {q_k }, u) &amp;= P( {q_k }, u lvert I) &amp;= P( {q_k } lvert u, I) P(u lvert I) &amp; propto P( {q_k } lvert u, I) &amp;= prod_k P(q_k lvert u, I) &amp;= prod_k u^{q_k} times left(1-u right)^{1-q_k} &amp;= u^M (1-u)^{N-M} end{align}with $M= sum_k q_k$ . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u, q): r2 = (y_obs - y_g)**2 G = -0.5*q*(r2/σ_obs**2 + tt.log(2*onp.pi*σ_obs**2)) B = -0.5*(1-q)*(r2/(σ_obs**2 + η**2) + tt.log(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(G + B) with pm.Model() as model_exp: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) q = pm.Bernoulli(&#39;q&#39;, p=u, shape=x.shape) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u, &#39;q&#39;:q}) # make sure there are no divergences with initial point for RV in model_exp.basic_RVs: print(RV.name, RV.logp(model_exp.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 q -13.862943611198906 likelihood -148.95080195551284 . with model_exp: trace_exp = pm.sample(20000, tune=20000, progressbar=True, random_seed = 1123581321) . Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) CompoundStep INFO:pymc3:CompoundStep &gt;NUTS: [logη, u, b, m] INFO:pymc3:&gt;NUTS: [logη, u, b, m] &gt;BinaryGibbsMetropolis: [q] INFO:pymc3:&gt;BinaryGibbsMetropolis: [q] Sampling 2 chains, 0 divergences: 100%|██████████| 80000/80000 [02:13&lt;00:00, 600.12draws/s] The number of effective samples is smaller than 10% for some parameters. WARNING:pymc3:The number of effective samples is smaller than 10% for some parameters. . pm.traceplot(trace_exp, var_names=[&#39;m&#39;, &#39;b&#39;,&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_exp, [&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;q&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo0,hi0 = pm.stats.hpd(trace_mix[l]) lo,hi = pm.stats.hpd(trace_exp[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (mixture model - Laplace)&quot;) print(f&quot; t t {lo0:.3e} &lt; {l} &lt; {hi0:.3e} t (mixture model - MC)&quot;) print(f&quot; t t {lo :.3e} &lt; {l} &lt; {hi :.3e} t (exponential model - MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (mixture model - Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (mixture model - MC) 2.002e+00 &lt; m &lt; 2.469e+00 (exponential model - MC) -1.930e+00 &lt; b &lt; 7.248e+01 (mixture model - Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (mixture model - MC) -4.059e+00 &lt; b &lt; 7.498e+01 (exponential model - MC) 6.015e-01 &lt; u &lt; 1.014e+00 (mixture model - Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (mixture model - MC) 5.566e-01 &lt; u &lt; 9.479e-01 (exponential model - MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (mixture model - Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (mixture model - MC) 4.961e+00 &lt; logη &lt; 6.652e+00 (exponential model - MC) . As expected, the marginalized distributions for the model parameters $m$, $b$, $u$, and $ eta$ are essentially the same, regardless of whether we look at the mixture model or the exponential model. . The Good, the Bad, and the Maybes . The benefit of using the exponential model, with additional hyper-parameters for the good/bad state of each point, is that we can make inferences about the state of the points. Here, we are again following Jayne&#39;s (see Chapter 4, Elementary Hypothesis Testing). The odds for a given hypothesis, in this case, the odds for point $i$ to be a good point is defined as . begin{align} O(q_i = 1 | D, I) &amp; equiv frac{P(q_i = 1 | D, I)}{P(q_i = 0 | D, I)} end{align}where $P(q lvert D, I)$ is given by marginalization of the full posterior begin{align} P(q_i lvert D, I) &amp;= int mathrm{d} Theta mathrm{d}u mathrm{d} eta , sum_{ {q_j }_{j ne i}} P( Theta, u, eta, {q_j } lvert D, I) end{align} . Since the points generated from the MC simulation are drawn in proportion to the full posterior, the marginalized distribution for $q_i$ is obtained by simply taking the trace over all other parameters. That is, we can approximate $P(q_i lvert D, I)$ from the histogram of the $q_i$ values themselves. . It&#39;s more convenient to look at this quantity in decibels, begin{align} e(q_i = 1 | D, I) &amp; equiv 10 log_{10}{O(q_i = 1 | D I)} end{align} Jayne&#39;s calls $e$ the evidence, but this conflicts with the (somewhat widespread) use of evidence to denote the normalization constant (partition function) in the definition of the posterior. We&#39;ll just call it $e$. . belief = pd.DataFrame({&#39;P_good&#39;:onp.array([1/2, 2/3, 4/5, 10/11, 0.954, 100/101, 0.999, 0.9999])}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.500000 | 1.00000 | 0.000000 | . 1 0.666667 | 2.00000 | 3.010300 | . 2 0.800000 | 4.00000 | 6.020600 | . 3 0.909091 | 10.00000 | 10.000000 | . 4 0.954000 | 20.73913 | 13.167905 | . 5 0.990099 | 100.00000 | 20.000000 | . 6 0.999000 | 999.00000 | 29.995655 | . 7 0.999900 | 9999.00000 | 39.999566 | . This provides three scales for measuring our degrees of belief in $q_i$: $p(q_i =1)$, $O(q_i = 1)$, and $e(q_i = 1)$. It turns out we have a better intuition for $e$, with $3$db corresponding to a factor of 2 (in the odds), $10$db to a factor of 10, and $13$db to a probability of $95%$ (i.e. the $2 sigma$ criteria). . Let&#39;s calculate these quantities for each of the data points. The probability that a given $q_i$ is good is simply the fraction of points (in the parameter space) where $q_i = 1$, regardless of the values of the other $q_j$ (or the other parameter values). . fig, ax = plt.subplots() ax.plot(trace_exp[&#39;q&#39;][::100,0], marker=&#39;o&#39;, ls=&#39;None&#39;, ms=4, color=color[&#39;dark&#39;]); ax.set_xlabel(&#39;Trace&#39;) ax.set_ylabel(r&#39;$q_0$&#39;) plt.show() . This is what the trace for $q_0$ looks like (note that we only show every 100 points) . belief = pd.DataFrame({&#39;P_good&#39;:onp.average(trace_exp[&#39;q&#39;], axis=0)}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.778600 | 3.516712 | 5.461368 | . 1 0.000400 | 0.000400 | -33.977663 | . 2 0.000000 | 0.000000 | -inf | . 3 0.000350 | 0.000350 | -34.557799 | . 4 0.966475 | 28.828486 | 14.598218 | . 5 0.945725 | 17.424689 | 12.411650 | . 6 0.943750 | 16.777778 | 12.247344 | . 7 0.944525 | 17.026138 | 12.311161 | . 8 0.934400 | 14.243902 | 11.536290 | . 9 0.904800 | 9.504202 | 9.779156 | . 10 0.970875 | 33.334764 | 15.228974 | . 11 0.875375 | 7.024072 | 8.465890 | . 12 0.873675 | 6.916089 | 8.398606 | . 13 0.952650 | 20.119324 | 13.036134 | . 14 0.955125 | 21.284123 | 13.280558 | . 15 0.841175 | 5.296238 | 7.239675 | . 16 0.938275 | 15.200891 | 11.818690 | . 17 0.954950 | 21.197558 | 13.262858 | . 18 0.963750 | 26.586207 | 14.246564 | . 19 0.959125 | 23.464832 | 13.704174 | . Notice that there are only three negative evidence values, corresponding to the three obvious outlier. This means that all other points are more likely to be good than bad ($p&gt;1/2$). If we want, we can now label our points to show our belief in their good/bad state. Note, that this is purely a cosmetic procedure, our estimate for the model parameters, $m$ and $b$, is obtained by averaging over all possible configurations of the system, consistent with our measured data and prior information. . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;superfine&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, alpha=0.6, mec=&#39;k&#39;, mew=1.5, lw=1.5) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_exp[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Exponential Model MC&#39;) im = ax.scatter(x,y,c=np.clip(belief[&#39;e_good&#39;].values, a_min=-10, a_max=10), cmap=&#39;coolwarm&#39;) ax.legend(loc=4) cb = colorbar(ax,im) cb.set_label(r&#39;$e(q_i = 1)$&#39;) plt.show() . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "relUrl": "/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bayesian Parameter Estimation",
            "content": "Motivation / Disclaimer . This is the first in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . We start with the basics of Bayesian Data Analysis, following the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling. Dr. Sivia is a research scientist at the Rutherford Appleton Lab and Prof. Skilling was at the DAMPT (Cambridge) before becoming a founding director of the Maximum Entropy Data Consultants group. The book is written by/for physicist and includes advanced topics (such as Maximum Entropy and Nested Sampling) that we hope to cover in future posts. . References . While mainly following Sivia&#39;s book, we have also found the following references extremely helpful . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | Bayesian Data Analysis, Third Edition. A. Gelman, J. B. Carlin, H. Stern et al., Chapman &amp; Hall/CRC Texts in Statistical Science (2013) | &quot;Frequentism and Bayesianism&quot; Blog-Post series (I, II, III, IV, IV) by Jake VanderPlas | Michael Betancourt&#39;s writings on probability theory. | . Preliminaries . Basic Rules . The probability $P(X lvert I)$ expresses our belief that $X$ is true given prior information $I$. The two basic rules of probability theory are just the sum and product rules, $$ begin{align} P(X lvert I) + P( overline{X} lvert I) &amp;= 1 &amp; textrm{(Sum Rule)} label{e:sum_rule} P(X,Y lvert I) &amp;= P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Product Rule)} notag &amp;=P(Y lvert X, I) times P(X lvert I) label{e:product_rule} end{align}$$ where $ overline{X}$ denotes the negation of $X$. See Sivia or Jaynes for a derivation, originally due to Cox, which shows that these are the only rules that will guarantee consistency. . From the product rule, one can easily derive Bayes&#39; Theorem $$ begin{align*} overbrace{P(X lvert Y, I)}^{ small{ mathrm{posterior}}} &amp;= frac{ overbrace{P(Y lvert X,I)}^{ small{ mathrm{likelihood}}} times overbrace{P(X lvert I)}^{ small{ mathrm{prior}}}}{ underbrace{P(Y lvert I)}_{ small{ mathrm{evidence}}}}&amp; textrm{(Bayes&#39; Theorem)} label{e:bayes} end{align*}$$ which states that the &quot;posterior&quot; is proportional to the &quot;likelihood&quot; times the &quot;prior&quot;, with the &quot;evidence&quot; a normalization constant. Posterior, likelihood and prior are the standard terms for these quantities, but the &quot;evidence&quot; label seems not to be widespread (we are following Sivia&#39;s nomenclature here). Note that there is nothing special about these labels, one&#39;s prior can be another&#39;s posterior, and your posterior today can be your prior tomorrow. Basically, the posterior is what we want to calculate, but can&#39;t (at least not directly), the prior is what we start with, and the likelihood is something we can easily calculate. . The sum rule can be extended to the to a series of mutually exclusive and exhaustive set of propositions $ {Y_k }$, such that $ sum_k P(Y_k lvert I) = 1$, and in the continuum limit we obtain the following marginalization property . $$ begin{align*} P(X lvert I) &amp;= int textrm{d}Y P(X, Y lvert I) = int textrm{d}Y P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Marginalization)} label{e:marginalization} end{align*}$$This is incredibly useful when there are unknown quantities that are required to compute the likelihood, but which are not really of interest to us. These so-called nuisance parameters can then be introduced and integrated out. We are being a bit careless here, and intermixing probabilities with probability densities. Strictly speaking, if $Y$ is a real random variable we should say that the probability for $Y$ to be within the range $y le Y le y+ Delta y$ is . $$ begin{align*} P(y le Y le y + Delta y)= rho(Y=y) Delta t end{align*}$$with $ rho$ the probability density function. However, following Sivia we will use the same symbol for both quantities, as it should be apparent from the context which one we are referring to. . Bayesian Data Analysis . It is easier to recognize how this Bayesian framework fits within a Data Analysis problem if we rewrite Bayes theorem in the form . $$ begin{align*} P( textrm{Hypothesis} lvert textrm{Data}, I) &amp;= frac{P( textrm{Data} lvert textrm{Hypothesis}, I) times P( textrm{Hypothesis} lvert I)}{P( textrm{Data}, I)} label{e:bayes_hypothesis} end{align*}$$or as . $$ begin{align*} P( Theta lvert D, I) &amp;= frac{P(D lvert Theta, I) times P( Theta lvert I)}{P(D lvert I)} label{e:bayes_theta} &amp; propto P(D lvert Theta, I) times P( Theta lvert I) end{align*}$$where $ Theta$ denotes the parameters of our model, $D$ our measured experimental/simulation data. The questions we are looking to answer, which these posteriors allow us to formulate are the following: &quot;What does my data say about my hypothesis&quot; or &quot;What does my data say about the model parameters&quot;? For what follows, we will focus exclusively on the parameter estimation problem. . One shot or sequential analysis? . Assume that our experimental data consists of a series of $N$ measurements, $D = {D_k }_{k=1}^N$. Our prior information $I$ specifies the model we believe explains this data, and we want to infer the parameters of this model. . Thus, what we want is the posterior of $ Theta$ given $D$, which gives $$ begin{align*} P( Theta lvert D,I) &amp; propto P( {D_k }_{k=1}^N lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert D_1, Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=3}^N lvert Theta, I) times P(D_2 lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp; vdots &amp;= left( Pi_{k=1}^N P(D_k lvert Theta, I) right) times P( Theta lvert I) end{align*}$$ . In step $2$, we are using the product rule to peel off one of the $D_i$, then in step $3$, we assume that (given the prior and the model parameters), knowledge of $D_1$ does not give us any extra information regarding the probabilities of observing $ {D_k }_{k ge 2}$, which means that they are independent. This procedure can be repeated until the original (one-shot) likelihood, which quantifies the probability of obtaining the entire set of data, is written as a product of individual likelihoods. From this, it is easy to see that we can analyze the data in one-shot, or sequentially, as the measurements arrive. The two routes are equivalent. . To see this, consider the expression obtained after peeling off m elements from the original likelihood. The posterior is given as $$ begin{align*} P( Theta lvert D,I)&amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) big( Pi_{j=1}^{m} P(D_j lvert Theta, I) big) P( Theta lvert I) &amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) times P( Theta lvert {D_j }_{j=1}^m, I) end{align*}$$ where the second and third terms in the rhs of the first equation are nothing but the likelihood and the prior after obtaining the first $m$ measurements. Thus, we can analyze the first $m$ data points, obtaining a posterior $P( Theta lvert {D_j }_{j=1}^m, I)$, and then use this as a prior when analyzing the remaining $N-m$ points. . What we can never ever do, however, is to use the posterior $P( Theta lvert {D_j }_{j=1}^m, I)$ to re-analyze the $ {D_j }_{j=1}^m$ measurements! . Laplace&#39;s Approximation, Best Estimates and Error Bars . Everything we can say about our parameters, given the Data and prior information, is contained in the posterior $P( Theta lvert D, I)$. However, this quantity can be quite complicated to deal with, particularly when the number of parameters exceeds 3. Therefore, it can be useful to develop approximations to this posterior. In particular, if the distribution is simple enough we can try to approximate it with a Gaussian distribution. Then, all that are needed are the first two moments, i.e., the average and the variance. The average give the peak of the distribution (the most likely point) and the variance an estimate of its spread, and thus of the reliability of our estimate. . This approximation is commonly referred to as Laplace&#39;s approximation, and amounts to a Taylor expansion of the logarithm of the posterior around the maximum (truncated to second order). Consider the general multi-dimensional case with $p$ parameters, $ Theta = ( Theta^{1}, Theta^{2}, ldots, Theta^{p})$, and let $ Theta_0 = text{argmax}{P( Theta lvert D, I)}$ . $$ begin{align*} L &amp;= ln{P( Theta lvert D, I)} = ln P &amp;= L lvert_{ Theta_0} + nabla L lvert_{ Theta_0} cdot( Theta - Theta_0) + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) + mathcal{O} big(( Theta- Theta_0)^3 big) &amp; simeq L lvert_{ Theta_0} + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) end{align*}$$where $ nabla= nabla_{ Theta}$, $A^t$ denotes the transpose of $A$, $( cdot)$ a matrix-matrix multiplication, and $ nabla nabla L$ is the (Hessian) matrix of second derivatives. Notice that the first-order term vanishes, since $ nabla L lvert_{ Theta_0} = 0$ by definition. . We can then re-exponentiate this expression to obtain an approximation to our original posterior distribution $$ begin{align*} P( Theta lvert D,I) propto exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot left(- nabla nabla L lvert_{ Theta_0} right) cdot left( Theta- Theta_0 right) right]} end{align*}$$ . Multi-variate Gaussians . Recall the definition of a multi-variate Gaussian distribution for a random $d$-dimensional vector $A$, expressed as $A sim mathcal{N}( mu, Sigma)$ $$ begin{align*} mathcal{N}(A lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^d det{ Sigma}}} exp{ left[- frac{1}{2} left(A- mu right)^t cdot Sigma^{-1} cdot left(A- mu right) right]} left langle A right rangle &amp;= mu left langle(A^i - mu^i)(A^j - mu^j) right rangle &amp;= Sigma^{ij} int text{d}A mathcal{N}(A lvert mu, Sigma) &amp;=1 end{align*}$$ where $ langle cdot rangle$ denotes an average over $P_{ mathcal{N}}$. . Such distributions have many interesting properties. In particular, if A is partitioned into $B$ and $C$, $A= begin{pmatrix}B C end{pmatrix}$, the marginalization over $B$ ($C$), results in another multi-variate Gaussian distribution for $C$ ($B$) $$ begin{align*} P(B lvert mu, Sigma) &amp;= int textrm{d}C mathcal{N} left( left. begin{pmatrix}B C end{pmatrix} right lvert begin{pmatrix} mu_B mu_C end{pmatrix}, begin{pmatrix} Sigma_{BB} &amp; Sigma_{BC} Sigma_{CB}&amp; Sigma_{CC} end{pmatrix} right) &amp;= frac{1}{ sqrt{(2 pi)^{d_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} left(B - mu_B right)^t Sigma_{BB}^{-1} left(B - mu_B right) right]} &amp; equiv mathcal{N}( mu_{B}, Sigma_{BB}) end{align*}$$ This can be easily proved by completing the squares in the exponent of the joint distribution. . Thus, since each component of $A$ is itself described by a Gaussian distribution, the best estimate is given by $$ begin{align*} A &amp;= mu pm sigma sigma &amp;= sqrt{ text{diag}{ Sigma}} end{align*}$$ . Finally, we see that the Laplace approximation of the posterior distribution for $ Theta$ is nothing but a multi-variate Gaussian, with average and covariance matrix given by $$ begin{align*} mu&amp; rightarrow Theta_0 Sigma&amp; rightarrow (- nabla nabla L lvert_{ Theta_0})^{-1} end{align*}$$ . Bayesian Parameter Estimation . Fitting with (known) noise . Let&#39;s see how this Bayesian formalism can be applied in practice. Our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. . The posterior for $ Theta$, given the Data $D$ and the prior information $I$, is then $$ begin{align*} P( Theta lvert D, I) &amp; propto P(D lvert Theta, I) times P( Theta lvert I) &amp; propto Pi_k P(y_k lvert Theta, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{ big(y_k - f(X_k; Theta) big)^2}{2 sigma_k^2} right]} end{align*}$$ where we have assumed that : . The prior $P( Theta lvert I)$ is a constant | The $y_k$ are independent of each-other (on knowing $ Theta$ and $I$), such that $P(y_i,y_j lvert Theta, I) = P(y_i lvert Theta, I) P(y_j lvert Theta, I)$ for $i ne j$. | The measurement error, and thus the likelihood, is Gaussian with variance $ sigma_k^2$ | . Let&#39;s see what the best-estimate would be in this case. . In practice, it will be more convenient to work with the logarithm of the probabilities, than with the probabilities themselves. Since $ ln$ is a monotonically increasing function, the maximum of $P$ will coincide with the maximum of $ ln{P}$. The log posterior is given by . $$ begin{align*} L &amp;= textrm{constant} - sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma_k^2} &amp;= textrm{constant} - frac{1}{2} chi^2 end{align*}$$where $ chi^2= sum_k(y_k - f_k)^2/ sigma_k^2$ is the sum of the squared residuals, weighted by the inverse of the error, and all terms not depending on $ Theta$ have been absorbed into the normalization constant. . The best estimate $ Theta_0$, which maximizes the posterior, is found by setting the gradient of $L$ to zero, . $$ begin{align*} nabla L = - frac{1}{2} nabla chi^2 = 0 Longrightarrow nabla chi^2 = 0 end{align*}$$which coincides with setting the gradient of $ chi^2$ to zero. Thus, in this case, maximizing the posterior is equivalent to a least-squares fit! . Finally, our reliability estimate is obtained by computing the matrix of second derivatives, evaluated at the optimal value $ Theta_0$ $$ begin{align*} Sigma_{ Theta}^{-1}=- nabla nabla L lvert_{ Theta_0} = frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0} end{align*}$$ . Approximating our posterior as a Gaussian, our best estimate for the model parameters would be $$ begin{align*} Theta = Theta_0 pm sigma_{ Theta} ; qquad left( sigma_{ Theta}= sqrt{ text{diag}( Sigma_{ Theta})} right) end{align*}$$ . Fitting with uknown noise . What happens in the case where don&#39;t have an estimate of the error bars? How would we derive the best-estimate in this case? What quantity should we minimize? . The solution is simply, use marginalization and average over all possible values of $ sigma$ (assumed to be the same for all points) $$ begin{align} P( Theta lvert D, I) &amp;= int textrm{d} sigma , P( Theta, sigma lvert D, I) &amp; propto int textrm{d} sigma , P(D lvert Theta, sigma, I) times P( Theta, sigma lvert I) end{align}$$ where the last step uses Bayes&#39; to write the integrand as the likelihood and the prior. Notice that this is the average of $P(D lvert Theta, sigma, I)$ over $P( Theta, sigma lvert I)$. . Under the same simplifying assumptions as above, i.e., independent measurements, uniform prior for $ Theta$, and Gaussian noise, we can again decompose the likelihood of $D$ into a product of likelihoods for the individual measurements $$ begin{align} P( Theta lvert D, I) &amp; propto int_0^ infty textrm{d} sigma left( Pi_k frac{1}{ sqrt{2 pi} sigma} exp{ left[- frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} right]} right) times frac{1}{ sigma} &amp;= (2 pi)^{-N/2} int_0^ infty frac{ textrm{d} sigma}{ sigma} sigma^{-N} exp{ left[- frac{1}{2 sigma^2} sum_k left(y_k - f(X_k; Theta) right)^2 right]} end{align}$$ . Here we have used Jeffreys&#39; prior for $ sigma$, $P( sigma lvert I) propto 1/ sigma$, as it is a scale-parameter. Depending on your prior information (e.g., do you know the order of magnitude?), there may be more appropriate priors, but we leave that for a future discussion. See Sivia and or Jaynes for a more in depth review of such issues. In practice, if $N$ is large enough this will not make a big difference, choosing Jeffreys prior or a constant prior for $ sigma$ will give essentially the same predictions. . The integral over $ sigma$ can be done analytically through the following variable transformation, with $S = sum_k (y_k - f_k)^2$ $$ begin{align} t &amp;= frac{ sum_k left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} = frac{S}{2 sigma^2} frac{ textrm{d} sigma}{ sigma} &amp;= - frac{ textrm{d}t}{2t} end{align}$$ . The posterior is then $$ begin{align} P( Theta lvert D, I) &amp; propto (2 pi)^{-N/2} int_0^ infty frac{ textrm{d}t}{2t} left( frac{2t}{S} right)^{N/2} e^{-t} &amp;= frac{ pi^{-N/2}}{2} underbrace{ left( int_0^ infty frac{ textrm{d}t}{t} t^{N/2} e^{-t} right)}_{= Gamma(N/2)}S^{-N/2} end{align}$$ . where the quantity in parenthesis is independent of $ Theta$ and equal to the Gamma function of $N/2$. . As before, let&#39;s look at the log-posterior . $$ begin{align} L &amp;= textrm{constant} - frac{N}{2} ln{S} end{align}$$The best-estimate is again that which maximizes $L$ $$ begin{align} nabla L &amp;= - frac{N}{2} frac{ nabla S}{S}= 0 Longrightarrow nabla S = 0 end{align}$$ which in this case corresponds to minimizing $ ln{S}$ or just $S$. So far it seems that the procedure is the same, minimize the sum of the squared residuals (weighted by the magnitude of the error, if known). The differences come when we consider the reliability estimate. . The Hessian is now $$ begin{align} nabla nabla L &amp;= - frac{N}{2} left[ frac{ nabla nabla S}{S} - frac{ left( nabla S right) left( nabla S right)}{S^2} right] end{align}$$ . which reduces to the following when evaluated at the optimum point $ Theta_0$ (since $ nabla S lvert_{ Theta_0} equiv 0$) $$ begin{align} nabla nabla L lvert_{ Theta_0} &amp;= - frac{N}{2} frac{ nabla nabla S lvert_{ Theta_0}}{S_0} = - frac{1}{2} left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} end{align}$$ . Within the Laplace approximation, the covariance matrix for the posterior distribution of $ Theta$ is then given by $$ begin{align} Sigma_ Theta^{-1} &amp;= frac{1}{2} left( left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} right) end{align}$$ . This expression is very similar to the one we obtained above, in the case where the measurement error $ sigma$ was known, $ Sigma_ theta^{-1}=- frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0}$. Here, we have replaced $ sigma_k^2$ with an estimate derived from the data $S_0/N$ . $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{ sigma_k^2} &amp; longrightarrow frac{S}{S_0/N} = frac{N}{S_0} sum_k left(y_k - f(X_k; Theta) right)^2 sigma_k^2 &amp; longrightarrow frac{S_0}{N} = frac{1}{N} sum_{k} left(y_k - f(X_k; Theta_0) right)^2 end{align}$$However, $S_0/N$ is not necessarily the best estimate for the amplitude of the noise, as we will see below. . ... about the noise . What does the data say about the noise, i.e., what is $P( sigma lvert D, I)$? . To be able to compute this quantity, we again make use of marginalization, treating the model parameters as nuisance parameters and integrating over them. $$ begin{align} P( sigma lvert D, I) &amp;= int textrm{d} Theta P( sigma, Theta lvert D, I) &amp; propto int textrm{d} Theta P(D lvert sigma, Theta, I) P( sigma, Theta lvert I) end{align}$$ . Assuming $ sigma$ and $ Theta$ are independent, $P( sigma, Theta lvert I)= P( sigma lvert I) P( Theta lvert I)$, and taking a uniform prior for $ Theta$ and Jeffreys prior for $ sigma$, we have $$ begin{align} P( sigma lvert D, I) propto sigma^{-(N+1)} int textrm{d} Theta exp{ left[- frac{S( Theta)}{2 sigma^2} right]} end{align}$$ where $S$ is still the sum of squared residuals $S = sum_k (y_k - f(X_k; Theta))$. We get one factor of $1/ sigma$ from the normalization constant of the likelihood for each data measurement, and one from the prior. . Without having to assume anything regarding the form of the model function $f(X)$, we can proceed by again using the Laplace approximation, now expanding $S( Theta)$ around the minimum $ Theta_0$ (which corresponds to the maximum of the posterior $P( Theta lvert D, I)$) $$ begin{align} S simeq S_0 + nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) + frac{1}{2} left( Theta - Theta_0 right)^t cdot nabla nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) end{align}$$ . The posterior $P( sigma lvert D, I)$ is then $$ begin{align} P( sigma lvert D, I)&amp; propto sigma^{-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot frac{ nabla nabla S lvert_{ Theta_0}}{2 sigma^2} cdot left( Theta- Theta_0 right) right]} &amp; propto sigma^{p-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} end{align}$$ where the $p$-dimensional integral over $ Theta$ gives us a factor of $ sqrt{(2 pi)^p det{ Sigma}_{ sigma}}$, with $ Sigma_ sigma^{-1} = - frac{1}{2 sigma^2} nabla nabla S lvert_{ Theta_0}$. . To find the best-estimate for $ sigma$ we maximize the log posterior $$ begin{align} L &amp;= ln{P( sigma lvert D, I)} = textrm{const} + left(p - N - 1 right) ln{ sigma} - frac{S_0}{2 sigma^2} frac{ textrm{d}L}{ textrm{d} sigma} &amp;= (p-N-1) frac{1}{ sigma} + frac{S_0}{ sigma^3} frac{ textrm{d}^2L}{ textrm{d}^2 sigma} &amp;= (N+1-p) frac{1}{ sigma^2} - frac{3 S_0}{ sigma^4} end{align}$$ . Setting $ text{d} L / text{d} sigma lvert_{ sigma_0} = 0$, we get $$ begin{align} sigma_0^2 = frac{S_0}{N+1-p} end{align}$$ . with the reliability estimates given by the inverse of the Hessian $$ begin{align} left. frac{ textrm{d}^2L}{ textrm{d}^2 sigma} right lvert_{ sigma_0} &amp;= frac{(N+1-p) sigma_0^2 - 3S_0}{ sigma_0^4} &amp;= frac{-2(N+1-p)}{ sigma_0^2} end{align}$$ . we get $$ begin{align} sigma = sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align}$$ . Example : Fitting a Straigh-Line (known measurement error) . Now, let&#39;s see how all this works out in practice. For convenience, we will use the jax library and take advantage of its automatic differentiation capabilities, which allow us to calculate the gradients and Hessians exactly (to machine precision). If you are on Windows this will not work, in that case remove all the jax commands. . Within the Laplace approximation, we can work out the best estimates for all our quantities by simply maximizing the appropriate posterior. However, to compare with the full solution, we will also make use of the PYMC3 library, which allows us to perform Hamiltonian Monte-Carlo simulations and generate samples which converge to those of the posterior. For the minimiztion, we make use of the scipy library. . Please install the following packages to run the code below . numpy | matplotlib | pymc3 | theano | scipy | jax | . #import numpy as np #### If using windows... import jax.numpy as np import numpy as onp import numpy.random as random import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano import theano.tensor as tt from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, scatter=False, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . We define a linear and quadratic function of a scalar variable $x$, which takes as first argument a list of parameters $ Theta$. We then define a routine to create noisy versions a function. . def linear_func(θ, x): return θ[0] + θ[1]*x # y = mx + b def quadratic_func(θ, x): return θ[0] + θ[1]*x + θ[2]*x**2 # y = a x^2 + bx + c def make_noisy_func(f, σ): &quot;&quot;&quot;Returns function that computes noisy measurements for y ~ N(f, σ^2), such that y will be normally distributed around f, with variance σ^2&quot;&quot;&quot; return lambda θ,x : f(θ,x) + σ*random.randn(len(x)) . Now let&#39;s simulate measuring using a noisy linear function (intercept $b=2$, slope $m=1$). We discretize $x$ over $N_p=10$ points evenly spaced in the range $[0,2]$, and simulate $N=8$ draws. Computing the average and variance over these $N$ sets of measurements gives us our best estimate and error bar for the value of $f$ at $x_i$. . random.seed(12345) nparams,func = 2, linear_func #nparams,func = 3, quadratic_func _θ0,σ0 = np.array([2.0, 1.0, 0.6]), 0.25 # exact parameter values θ0 = _θ0[:nparams] measurement = make_noisy_func(func, σ0) npoints,ndraws= 10,8 x_sample = np.linspace(0, 2.0, num=npoints) y_sample = np.array([measurement(θ0,x_sample) for i in range(ndraws)]) y_avg, y_err = np.average(y_sample, axis=0), np.std(y_sample, axis=0) fig, ax = plt.subplots() for yi in y_sample: ax.plot(x_sample, yi, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, label=&#39;Estimate&#39;, color=color[&#39;dark&#39;]) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . The symbols represent our &quot;measurement&quot;, 8 for each of the 10 $x$ values we are sampling. The average and variance of these measurements would give your experimental estimate (shown here with the error bar plot). . The &quot;Least-Squares&quot; route . Let&#39;s first attempt a least-squares minimization. . Recall the definition of $ chi^2$ $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(x_k; Theta) right)}{ sigma_k^2} end{align}$$ . For simple functions, it is trivial to work out the first and second derivatives of this quantity, but jax does all the work for us! As shown above, the best-estimate for $ Theta$ is given by minimizing $ chi^2$, and the reliability estimates are given by (twice) the inverse of the Hessian $$ begin{align*} Theta &amp;= Theta_0 pm sqrt{ text{diag}( Sigma_{ Theta})} Sigma_{ Theta}&amp;= 2 left( nabla nabla chi^2 lvert_{ Theta_0} right)^{-1} end{align*}$$ . For a the simple linear model we are considering, we can easily work out the analytical solution . $$ begin{align} partial_m chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right) x_k}{ sigma_k^2} qquad &amp; partial_b chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right)}{ sigma_k^2} nabla chi^2 &amp;= begin{pmatrix} partial_m chi^2 partial_b chi^2 end{pmatrix} = begin{pmatrix} sum w_k x_k^2 &amp; sum w_k x_k sum w_k x_k &amp; sum w_k end{pmatrix} begin{pmatrix} m b end{pmatrix} - begin{pmatrix} sum w_k x_k y_k sum w_k y_k end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} begin{pmatrix}m b end{pmatrix} - begin{pmatrix}p q end{pmatrix} end{align}$$ with $ alpha = sum w_k x_k^2$, $ beta = sum_k w_k$, $ gamma= sum_k w_k x_k$, $w_k = 2/ sigma_k^2$, $p= sum_k w_k x_k y_k$ and $q= sum_k w_k y_k$. The optimum solution is then . $$ begin{align*} nabla chi^2 &amp;= 0 begin{pmatrix}m b end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix}^{-1} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta- gamma^2} begin{pmatrix} beta p - gamma q alpha q - gamma p end{pmatrix} end{align*}$$Finally, the covariance matrix $ Sigma_{ Theta} = 2 ( nabla nabla chi^2)^{-1}$ is just . $$ begin{align*} nabla nabla chi^2 &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} Sigma_{ Theta} &amp;= 2 frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} end{align*}$$ @jit def chi2(θ): return np.sum((y_avg-func(θ,x_sample))**2/y_err**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 def exact_sol(xk,yk,dyk): wk = 2/dyk**2 α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_avg, y_err) . opt = minimize(chi2, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_chi2(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 1.118355 Iterations: 5 Function evaluations: 7 Gradient evaluations: 7 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.2f}]&#39;) . |θ - Θ_exact|_inf = 2.442491e-15 |H - H_exact|_inf = 3.469447e-18 |H_min - H_exact|_inf = 1.040834e-17 1.69 &lt; θ^1 (b) &lt; 2.24 [μ = 1.961, σ = 0.14] 0.79 &lt; θ^2 (m) &lt; 1.24 [μ = 1.018, σ = 0.11] . The exact values used to generate the data were $ Theta^1 = b = 2$ and $ Theta^2 = m = 1$, which is very close to the optimal value obtained here. Furthermore, we see that the approximate of the Hessian returned by the minimization routine coincides with the exact result (given by jax). We use $ pm 2 sigma$ interval as a measure of the error bounds. . The full Bayesian route . Now let&#39;s see how to get the &quot;full&quot; solution, not just a point-estimate. For this we use PYMC3 to generate samples from the posterior. To compare with our previous analysis, we will assume a uniform prior for both $m$ and $b$. . with pm.Model() as model: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) # true function y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) # measured data (accounting for noise) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=y_err, observed=y_avg) # generate samples from the prior (before looking at the data) prior = pm.sample_prior_predictive(samples=1000, random_seed = 123456) # generate samples from the posterior trace = pm.sample(5000, tune = 20000, progressbar=True) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [b, m] INFO:pymc3:NUTS: [b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 50000/50000 [00:20&lt;00:00, 2415.40draws/s] The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. . First, let us try to get an idea for how good/bad our priors are. We can do this by looking at the prior predictive distribution. This simply takes parameters values sampled from the prior, and pushes them through the model, generating corresponding samples for $y$. If we plot $y-b = mx$ we can clearly see that this prior is heavily skewed towards lines with high slopes. Is this what we want? Probably not, but this will depend on our prior information. . fig, ax = plt.subplots() for y_i,b_i in zip(prior[&#39;y&#39;], prior[&#39;b&#39;]): ax.plot(x_sample, y_i-b_i, alpha=0.2, color=&#39;k&#39;) ax.set_xlabel(r&#39;x&#39;) ax.set_ylabel(r&#39;y&#39;) ax.set_ylim(0,2) ax.set_xlim(0,2) plt.show() . PYMC3 comes with many useful post-processing functions to visualize our data. Let&#39;s look at a few of them. . pm.traceplot(trace) plt.show(); . Above, we see the distribution of the model parameters, with different different lines corresponding to different chains or independent runs of the MC simulation. At first glance, there doesn&#39;t seem to be any obvious divergence and we get nice Gaussian-like distributions for all model parameters (this includes the values of the function at the measured points). . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace, var_names=[&#39;m&#39;, &#39;b&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.show(); . &lt;Figure size 504x504 with 0 Axes&gt; . Here we have the $94 %$ credible intervals, corresponding to the $ mu pm 2 sigma$ interval in the case of a Gaussian distribution. The vertical lines indicate the real values that were used to generate the data. . pm.plot_posterior(trace, var_names=[&#39;m&#39;, &#39;b&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fff4d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fc8a10&gt;], dtype=object) . Here we have the posterior distribution for our two model parameters. Notice that the $94 %$ HPD (Highest Posterior Density Interval) corresponds exactly to that obtained using the Laplace approximation. This should not be surprising, when we see the posterior. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace[&#39;y&#39;], quantiles_sig) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, marker=&#39;o&#39;, color=color[&#39;superfine&#39;], alpha=0.5, label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], alpha=1, label=&#39;exact&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.legend() plt.show() . The shaded red regions show the $ sigma$, $2 sigma$ and $3 sigma $ regions $y$ obtained from the posterior samples of y. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace[&#39;b&#39;], trace[&#39;m&#39;], scatter=True, colors=[color[&#39;mid&#39;], color[&#39;light_highlight&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) plt.show() . Here we see the trace generated by the MC simulation, where the distribution of points approximates the the posterior distribution. Also drawn are the $ pm sigma$ and $ pm 2 sigma$ contour levels, which should contain $68 %$ and $95 %$ of the points, respectively. . We can redo the analysis using a &quot;better&quot; prior, e.g., assigning a uniform prior on the angles of the lines instead of their slope, but the results will not vary significantly (unless the number of data points is reduced). . Example : Fitting a straight line (with unknown measurement error) . The &quot;Least-Squares&quot; route . Now let&#39;s analyze the case where the measurement error is not known. As shown above, instead of minimizing $ chi^2$, we should maximize $L=- frac{N}{2} ln{S}$, with $S= sum_k (y_k -f_k)^2$, which coincides with minimizing $S$. Thus, the form of the solution is the same as before, we simply set $w_k=2 , ( sigma_k=1)$, as all the points are given equal weight. Recall that the covariance matrix is $ Sigma_ Theta^{-1} = frac{1}{2} nabla nabla frac{S}{S_0/N} lvert_{ Theta_0}$. . @jit def loss(θ): return np.sum((y_sample[0]-func(θ,x_sample))**2) grad_loss = jit(grad(loss)) hess_loss = hessian(loss) def exact_sol(xk,yk): wk = 2.0*np.ones_like(yk) α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_sample[0]) opt = minimize(loss, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) . Optimization terminated successfully. Current function value: 0.323150 Iterations: 4 Function evaluations: 6 Gradient evaluations: 6 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2*loss(opt[&#39;x&#39;])/npoints) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.3f}]&#39;) . |θ - Θ_exact|_inf = 2.220446e-16 |H - H_exact|_inf = 2.775558e-17 |H_min - H_exact|_inf = 4.163336e-17 1.77 &lt; θ^1 (b) &lt; 2.20 [μ = 1.984, σ = 0.106] 0.96 &lt; θ^2 (m) &lt; 1.32 [μ = 1.140, σ = 0.089] . Within the Gaussian approximation, we can also use this optimum solution to give an estimate of the unknown measurement error . $$ begin{align*} sigma_0^2 &amp;= frac{S_0}{N+1-p} sigma &amp;= sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align*}$$ σa = np.sqrt(loss(opt[&#39;x&#39;])/(npoints + 1 - nparams)) dσa= σa / np.sqrt(2*(npoints + 1 - nparams)) print(f&#39;{σa - 2*dσa:.2f} &lt; σ &lt; {σa + 2*dσa:.2f} [μ = {σa : .3f}, σ = {dσa : .3f}]&#39;) . 0.10 &lt; σ &lt; 0.28 [μ = 0.189, σ = 0.045] . Which is in good agreement with the exact value used to generate the noisy data ($ sigma = 0.25$). . fig, ax = plt.subplots() ax.plot(x_sample, y_sample[0], marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.plot(x_sample, func(opt[&#39;x&#39;],x_sample), ls=&#39;-&#39;, color=color[&#39;dark&#39;], alpha=1, label=&#39;Estimate&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . with pm.Model() as model_sig: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) logσ = pm.Uniform(&#39;logσ&#39;, lower=-6, upper=6) σ = pm.Deterministic(&#39;σ&#39;, tt.exp(logσ)) y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=σ, observed=y_sample[0]) prior_sig = pm.sample_prior_predictive(samples=1000, random_seed = 123456) trace_sig = pm.sample(5000, tune = 50000, progressbar=True) #post_sig = pm.sample_posterior_predictive(trace_uni, model=model_sig, random_seed=4938483) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logσ, b, m] INFO:pymc3:NUTS: [logσ, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 110000/110000 [01:08&lt;00:00, 1606.14draws/s] . plt.figure(figsize=(12,12)) pm.traceplot(trace_sig) plt.show(); . &lt;Figure size 864x864 with 0 Axes&gt; . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.axvline(σ0, 0, 1, c=&#39;C1&#39;) plt.show() . &lt;Figure size 504x504 with 0 Axes&gt; . pm.plot_posterior(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); . Notice how the resulting distribution for $ sigma$ is no longer symmetric. In this case we expect the Gaussian approximation will start to break down. However, the error bounds derived above, $0.1&lt; sigma &lt; 0.28$ are still close to the &quot;real&quot; values provided by the MC trace. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace_sig[&#39;y&#39;], quantiles_sig, label=&#39;prediction&#39;) ax.plot(x_sample, y_sample[0], ls=&#39;None&#39;, marker=&#39;o&#39;, mfc=&#39;None&#39;, mew=2, color=color[&#39;dark&#39;], label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], label=&#39;exact&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fe5629e4c90&gt; . Finally, here is the list of packages in my environment. . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "relUrl": "/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://johnjmolina.github.io/MLKyoto/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johnjmolina.github.io/MLKyoto/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://johnjmolina.github.io/MLKyoto/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://johnjmolina.github.io/MLKyoto/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}