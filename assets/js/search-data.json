{
  
    
        "post0": {
            "title": "Bayesian Parameter Estimation",
            "content": "Motivation / Disclaimer . This is the second in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . Having covered the basics of Bayesian Data Analysis, following Chapters 2 and 3 of the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling, we now consider a slightly more complicated case. . Now, we study how to perform parameter estimation in the presence of outliers. . References . This topic is discussed in some detail in Sivia&#39;s book (Ch. 8, Least-Squares Extensions), as well as Jaynes&#39; (Ch. 21, Outliers and Robustness). . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | . In addition, we also found the following paper to be very useful and in line with the discussions found in the books mentioned above . Data analysis recipes: Fitting a model to data, D. W. Hogg, J. Bovy, D. Lang, arXiv:1008.4686 (2010) | . Below we basically follow Jaynes, using the data of Hogg et al. . Intro . Let us consider that our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. Furthermore, assume that on prior information, we know that the results of any given measurement can be either good or bad. . When analyzing the data, in order to determine the parameters $ Theta$, we want to make sure that we use a robust method that is able to account for these outliers. As always with these parameter estimation problems, the quantity we are interested is the following posterior distribution begin{align} P( Gamma lvert D, I) &amp; propto P(D, lvert Gamma ,I) P( Gamma lvert I) &amp;= L( Gamma) , Pi( Gamma) L( Gamma) &amp;= P(D, lvert Gamma ,I) Pi( Gamma) &amp;= P( Gamma lvert I) end{align} with $ Gamma=( Theta, Xi)$, where $ Theta$ is parametrizing the target function $f(X; Theta)$, and $ Xi$ refers to all the remaining parameters (e.g., those related to the goodness/badness of the points). . The &quot;Mixture&quot; or &quot;Two-Model&quot; Model . To evaluate the likelihood, we consider the case of good/bad points separately. If a point is good, it is assumed to be sampled from a distribution $P_{ text{Good}}=G$, whereas bad points are sampled from $P_{ text{Bad}}= B$. . Assuming Gaussian errors, we set begin{align} P(y_k lvert mathrm{Good}_k, Gamma, I) = G(y_k lvert Theta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 right) P(y_k lvert mathrm{Bad}_k, Gamma, I) = B(y_k lvert Theta, eta) &amp; sim mathcal{N} left(f(X_k; Theta), sigma_k^2 + eta^2 right) end{align} Good points will be normally distributed around the true values $f(X_k; Theta)$, with a standard deviation given by the measured error bars. Bad points, on the other hand, will be distributed around the true values with a much larger variance, given by $ sigma_k^2 + eta^2$. Here, $ eta^2$ is an additional parameter which defines the variance of the bad points. I think that whether or not this represents the true physical origin behind the outliers is of secondary importance (as we are not trying to make inferences about this process). What matters here is that we can build a probabilistic model that can explain the data, i.e., the fact that we can have good/bad points, and that the bad points can show very large deviations with respect to the true values. I think we would obtain roughly the same results if we uncoupled the Bad data from the model (as done in Jaynes treatment), begin{align} y_k lvert text{Bad} sim mathcal{N} left(c, eta^2 right) end{align} with $c$ an additional nuissance parameter to be learned from the data. . Assuming independent measurements, the full likelihood $D= {y_k }_{k=1}^N$ can be expressed as a product of likelihoods for each individual measurement, such that . begin{align} L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[P(y_k lvert Gamma, I) Big] &amp;= prod_k Big[P big(y_k,( mathrm{Good}_k text{ or } mathrm{Bad}_k) lvert Gamma, I big) Big] &amp;= prod_k Big[P big(y_k, mathrm{Good}_k lvert Gamma, I big) + P big(y_k, mathrm{Bad}_k lvert Gamma, I big) Big] &amp;= prod_k Big[P big( mathrm{Good}_k lvert Gamma, I big) P(y_k lvert mathrm{Good}_k, Gamma, I) + P big( mathrm{Bad}_k lvert Gamma, I big) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] end{align} Where we have simply used marginalization over the Good/Bad state of the points. Now, let us assume that the probability of observing a given sequence of good/bad points is invariant under permutations. What matters is just the number of good/bad points, not the order in which they were obtained. Let $u$, which we don&#39;t know, be the probability that any given point is good, Jaynes calls this the &quot;purity&quot; of the data. The likelihood is then given by begin{align} P( mathrm{Good}_k lvert Gamma, I)&amp;= u L( Gamma) = P(D lvert Gamma, I) &amp;= prod_k Big[uP(y_k lvert mathrm{Good}_k, Gamma, I) + (1-u) P(y_k lvert mathrm{Bad}_k, Gamma, I) Big] &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . The posterior for $ Theta$, which are the parameters we are actually interested in, is obtained by marginalizing over $u$ and $ eta$ begin{align} P( Theta lvert D, I)&amp;= iint mathrm{d}u mathrm{d} eta , P( Theta, u, eta lvert D, I) &amp;= frac{ iint mathrm{d}u mathrm{d} eta , L( Theta, u, eta) Pi( Theta, u, eta)}{ iiint mathrm{d} Theta mathrm{d}u mathrm{d} eta ,L( Theta, u, eta) Pi( Theta, u, eta)} end{align} where we have explicitly included the normalization constant in the definition. . We can use this model, with hyper-parameters $u$ and $ eta$, to compute the posterior for $ Theta$, and thus find the &quot;best-fit&quot; to the data, in the presence of outliers. . Digression on the two-model model . Let us look in a bit more detail into what exactly we are calculating here. For ease of reading (to avoid having to write $P$ all over the place), we will again follow Jaynes and rewrite the prior $ Pi( Gamma)$ as begin{align} Pi( Theta, u, eta)&amp;= P( Theta, u, eta lvert I) &amp;= P( Theta lvert I) P(u, eta lvert Theta, I) &amp;= Pi( Theta) H(u, eta lvert Theta, I) end{align} . where $ Pi( Theta)=P( Theta lvert I)$ and $H(u, eta lvert Theta, I)=P(u, eta lvert Theta, I)$. . Now, defining the &quot;pseudo-likelihood&quot; $ bar{L}$ as begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) end{align} . The marginalized posterior for $ Theta$ becomes begin{align} P( Theta lvert D, I) &amp;= frac{ bar{L}( Theta) Pi( Theta)}{ int mathrm{d} Theta bar{L}( Theta) Pi( Theta)} end{align} . To understand what this pseudo-likelihood is giving us, let&#39;s first expand the definition of $L( Theta, u, eta)$ in terms of the good and bad likelihoods $G$ and $B$. We obtain the following begin{align} L( Theta, u, eta) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u)B(y_k lvert Theta, eta) Big] &amp;= u^N prod_i G(y_i lvert Theta) +u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) prod_{i ne j} G(y_i lvert Theta) &amp; quad + u^{N-2}(1-u)^{2} sum_{j&lt;k} B(y_j lvert Theta, eta) B(y_k lvert Theta, eta) prod_{i ne j,k} G(y_i lvert Theta) &amp; quad vdots &amp; quad + (1-u)^{N} prod_{j}^N B(y_j lvert Theta, eta) &amp;=u^N L( Theta) &amp; quad+ u^{N-1}(1-u) sum_{j=1}^N B(y_j lvert Theta, eta) L_j( Theta) &amp; quad+ u^{N-2}(1-u)^2 sum_{j&lt;k}^N B(y_j lvert Theta, eta)B(y_k lvert Theta, eta) L_{jk}( Theta) &amp; quad vdots &amp; quad+ (1-u)^N B(y_1 lvert Theta, eta)B(y_2 lvert Theta, eta) cdots B(y_N lvert Theta, eta) &amp;= L^{(0)}( Theta, u, eta)+ L^{(1)}( Theta, u, eta) + L^{(2)}( Theta, u, eta) + ldots + L^{(N)}( Theta, u, eta) end{align} . Here, we use $L( Theta) = prod_i G(y_i lvert Theta)$ to denote the likelihood of the pure model, i.e., one without outliers. Likewise $L_j( Theta)$ is the likelihood of observing $ {y_{i ne j} }$ from the pure model, $L_{jk}$ that of observing $ {y_{i ne j,k} }$, etc. . Plugging this expression into the definition of $ bar{L}$, begin{align*} bar{L} &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , left[L^{(0)}( Theta, u, eta) + L^{(1)}( Theta, u, eta) + cdots + L^{(N)}( Theta, u, eta) right] H(u, eta lvert Theta, I) end{align*} we would obtain the following for the first term begin{align} iint mathrm{d}u mathrm{d} eta L^{(0)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^N L( Theta) H(u, eta lvert Theta, I) &amp;= L( Theta) int mathrm{d}u , u^n int mathrm{d} eta , H(u, eta lvert Theta,I) &amp;= L( Theta) underbrace{ int mathrm{d}u , u^n H(u lvert Theta, I)}_{ text{Probability that all points are good}} end{align} where in the second to last step we marginilize $h(u, eta lvert Theta, I)$ over $ eta$. Notice that the integral in the last line evaluates to the probability all points are good, regardless of $u$ and conditioned on $ Theta$ and $I$, begin{align*} P( text{All points good} lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good}, u lvert Theta, I) &amp;= int mathrm{d}u , P( text{All points good} lvert u, Theta, I) P(u lvert Theta, I) &amp;= int mathrm{d}u , u^N P(u lvert Theta, I) = int mathrm{d}u , u^N H(u lvert Theta, I) end{align*} . We would obtain something similar for the second term begin{align} iint mathrm{d}u mathrm{d} eta , L^{(1)}( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= iint mathrm{d}u mathrm{d} eta , u^{N-1}(1-u) B(y_j lvert Theta, eta) L_j( Theta) H(u, eta lvert Theta, I) &amp;= L_j( Theta) underbrace{ int B(y_j lvert Theta, I) underbrace{ mathrm{d} eta int mathrm{d}u , u^{N-1}(1-u) H(u, eta lvert Theta, I)}_{ text{Probability that any given point ($y_j$) is bad,} atop text{all others are good, and $ eta$ lies in $( eta, eta+ mathrm{d} eta)$, conditioned on $ Theta$ and $I$.}}}_{ text{Probability that $j$-th point is bad, with value $y_j$,} atop text{and all others are good, conditined on $ Theta$ and $I$.}} end{align} . Thus, we see that the pseudo-likelihood can be expressed as begin{align} bar{L}( Theta) &amp;= iint mathrm{d}u mathrm{d} eta L( Theta, u, eta) H(u, eta lvert Theta, I) &amp;= big( textrm{Probability all data is good} big) times big( textrm{Likelihood for all data} big) &amp; ,+ sum_j big( textrm{Probability only $x_j$ is bad} big) times big( textrm{Likelihood without $x_j$} big) &amp; ,+ sum_{j&lt;k} big( textrm{Probability only $x_j$ and $x_k$ are bad} big) times big( textrm{Likelihood without $x_j$ or $x_k$} big) &amp; ; vdots &amp; , + sum_j big( textrm{Probability only $x_j$ is good} big) times big( textrm{Likelihood with only $x_j$} big) &amp; , + big( textrm{Probability all data is bad} big) end{align} which is just a weighted-average of the &quot;pure&quot; likelihoods $L( Theta)$, over all possible combinations of good/bad points, weighted over the corresponding prior probabilities. Note that the sums here are over non-repeating indices. . The Exponential model . Above, we saw that we could express the likelihood as a sum of likelihoods for all possible combinations of good/bad points. Then, the pseudo-likelihood was obtained as an average over the hyper-parameters $u$ and $ eta$. . Consider the contribution to the likelihood form the term with exactly $M$ good points ($m=N-M$ bad points) begin{align} L^{(m)}( Theta, u, eta)&amp;= sum_{j_1 &lt; j_2 &lt; cdots&lt; j_m}u^{M}(1-u)^{m} L_{j_1, ldots,j_m}( Theta) B(y_{j_1} lvert Theta, eta) ldots B(y_{j_m} lvert Theta, eta) &amp;= sum_{j_1&lt;j_2&lt; cdots&lt; j_m} u^{M}(1-u)^{m} Big( prod_{i ne j_1, ldots,j_m} G(y_i lvert Theta) Big) Big(B(y_{j_1} lvert Theta, eta) cdots B(y_{j_m} lvert Theta, eta) Big) end{align} . Up to now we haven&#39;t really cared about which point are good/bad. However, we could just as well introduce additional hyperparameters $q_i$ to keep track of this information. Let begin{align} q_i = begin{cases} 1 &amp; text{point $i$ is good} 0 &amp; text{point $i$ is bad} end{cases} end{align} . Such that $q_{j_1} = q_{j_2}= cdots = q_{j_m}= 0$, $q_{i ne j_1, j_2, cdots j_m} = 1$, and $M= sum_i q_i$. The likelihood term we are considering then becomes begin{align} L^{(m)}( Theta, u, eta)&amp;= underbrace{ sum_{j_1&lt;j_2&lt; cdots&lt;j_m} sum_{q_1,q_2, ldots q_N} delta_{q_{j_1}, cdots,q_{j_m}}^{0} delta_{i ne j_1, cdots j_m}^{1}}_{ sum_{ {q_j }_ text{m bad}}} underbrace{ phantom{ prod_i^N}u^M(1-u)^m quad}_{P( {q_k } lvert Gamma, I)} underbrace{ prod_i^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{= P(D lvert Gamma, {q_k },I)} end{align} . allowing us to write down the full likelihood as begin{align} L( Theta, u, eta) &amp;= sum_{ {q_j }_{m=0}} u^{N-m}(1-u)^{m} prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp;+ sum_{ {q_j }_{m=1}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^q_i times B(y_i lvert Theta, eta)^{1-q_i} Big) &amp; vdots &amp;+ sum_{ {q_j }_{m=N}} u^{N-m}(1-u)^m prod_{i=1}^N Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) L( Theta, u, eta) &amp;= sum_{q_1,q_2, ldots q_n} underbrace{ phantom{ Pi_i}u^{N-m} (1-u)^m quad}_{= P( {q_i } lvert Theta, u, eta, I)} underbrace{ prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big)}_{P(D lvert {q_i }, Theta, u, eta) = L( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots,q_N}P( {q_i } lvert Theta, u, eta, I) times P(D lvert {q_i }, Theta, u, eta) end{align} . Recall the definition of the posterior begin{align} P( Theta, u, eta lvert D, I)&amp; propto L( Theta, u, eta) Pi( Theta, u, eta) &amp;= sum_{q_1, ldots q_N} L( {q_i }, Theta, u, eta) underbrace{P( {q_i } lvert Theta, u, eta, I) P( Theta, u, eta lvert I)}_{= P( {q_i }, Theta, u, eta lvert I) equiv Pi( {q_i }, Theta, u, eta)} &amp;= sum_{q_1, ldots q_N} L( {q_i }, theta, u, eta) Pi( {q_i }, Theta, u, eta) &amp; propto sum_{q_1, ldots q_N} P( Theta, u, eta, {q_i } lvert D, I) end{align} Thus, we see that the posterior for the &quot;mixture&quot; model is nothing but the marginalized posterior (over $q_i$) of the &quot;exponential&quot; model, which considers the goodness/badness of each of the points explicitly (with a global purity value $u$). . Within the exponential model, we then have begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) end{align} . As always, it&#39;s more convenient to work with the logarithm of the likelihood begin{align} ln{L} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} ln{G(y_i lvert Theta)} + sum_{ text{bad}} ln{B(y_i lvert Theta, eta)} &amp;= sum_{ text{good}} ln{ left[ frac{1}{ sqrt{2 pi sigma_i^2}} exp left( frac{-(y_i - f(X_i; Theta))^2}{2 sigma_i^2} right) right]} + sum_{ text{bad}} ln{ left[ frac{1}{ sqrt{2 pi( sigma_i^2+ eta^2)}} exp{ left( frac{-(y_i-f(X_i; Theta))^2}{2( sigma_i^2+ eta^2)} right)} right]} &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . Numerical Example (Mixture Model) . We will use the data given in the paper by Hogg, Bovy and Lang. While they consider the case of correlated x/y noise, here we will simply consider the case of noise in $y$. . #import numpy as np #### If using windows... import jax import jax.numpy as np import numpy as onp import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano as th import theano.tensor as tt from scipy.optimize import minimize ### remove all jax stuff on windows... from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) from matplotlib.collections import EllipseCollection from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib.collections as clt . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_sig2= quantiles_sig[1:-1] quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, *,nbins,scatter, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2, nbins) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) def colorbar(axes, mappable, *, loc=&quot;right&quot;, size=&quot;5%&quot;, pad=.1): &quot;&quot;&quot;Add colorbar to axes&quot;&quot;&quot; divider = make_axes_locatable(axes) cax = divider.append_axes(loc, size=size, pad=0.1) cb = plt.colorbar(mappable, cax=cax) return cb . # data taken from Hogg et al. x = np.array([201.,244, 47,287,203, 58,210,202,198,158,165,201,157,131,166,160,186,125,218,146], dtype=np.float64) y = np.array([592.,401,583,402,495,173,479,504,510,416,393,442,317,311,400,337,423,334,533,344], dtype=np.float64) σx = np.array([ 9, 14, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 5, 6, 6, 5, 9, 8, 6, 5], dtype=np.float64) σy = np.array([ 61, 25, 38, 15, 21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22], dtype=np.float64) ρxy= np.array([-0.84, 0.31, 0.64, -0.27, -0.33, 0.67, -0.02, -0.05, -0.84, -0.69, -.30, -0.46, -0.03, 0.50, 0.73, -0.52, 0.90, 0.40, -0.78, -0.56], dtype=np.float64) i_sort = np.argsort(x) x_sorted = x[i_sort] def computeErrors(): # Compute full sigma matrix Σ = onp.zeros((len(y), 2, 2)) Σ[:,0,0] = σx**2 Σ[:,0,1] = Σ[:,1,0] = σx*σy*ρxy Σ[:,1,1] = σy**2 Σinv = np.array([np.linalg.inv(s) for s in Σ]) # Diagonalize n = len(Σ) λ,η,θ = onp.zeros((n,2)), onp.zeros((n,2,2)), onp.zeros(n) for i, σ in enumerate(Σ): w, v = onp.linalg.eig(σ) # unordered eigenvalues and eigenvectors w, v = onp.real_if_close(w), onp.real_if_close(np.transpose(v)) w, v = zip(*sorted(zip(w,v), reverse=True)) # descending order λ[i,:] = onp.array(w) η[i,:,:]= onp.array(v) θ[i] = onp.arctan2(v[0][1], v[0][0])*180/np.pi # CCW angle with respect to x-axis of major axis (eigen-vector) return np.array(λ),np.array(η),np.array(θ),np.array(Σ) λ,η,θ,Σ = computeErrors() . The full covariance matrix for the data given by Hogg et al. can be computed as begin{align*} Sigma^2 &amp;= begin{pmatrix} sigma_x^2 &amp; rho_{xy} sigma_x sigma_y rho_{xy} sigma_x sigma_y &amp; sigma_y^2 end{pmatrix} end{align*} with the values of $ sigma_x$, $ sigma_y$, and $ rho_{xy}$ given in the paper. . Let&#39;s plot the full data, using ellipses to represent the $1 sigma$ contours. . fig,ax = plt.subplots(figsize=(16,9)) ax.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;]) ec = EllipseCollection(2*np.sqrt(λ[:,0]), 2*np.sqrt(λ[:,1]), θ, units=&#39;xy&#39;, offsets=np.column_stack((x,y)), transOffset=ax.transData, edgecolor=color[&#39;dark&#39;], facecolor=&#39;None&#39;) ax.add_collection(ec); ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . Ignoring the correlated erros, we consider only the case of measurement errors in $y$, given by $ sigma_y$. The data now looks like this . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x,y,σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plt.show() . (Naive) Least-Squares . Before trying our Bayesian solution, let us consider a naive (least-squares) fit to the data... . def func(x,θ): return θ[0]*x + θ[1] @jit def chi2(θ): return np.sum((y-func(x,θ))**2/σy**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 fit = minimize(chi2, np.ones(2), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) fit_ihess = np.linalg.inv(hess_chi2(fit[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 289.963723 Iterations: 13 Function evaluations: 15 Gradient evaluations: 15 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(50,300) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange,fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.legend() plt.show() . This is clearly not what we want. . Least-Squares extension . Now let&#39;s consider the (marinalized) mixture model, assuming uniform priors, all we need is to maximize the likelihood begin{align} L( Gamma) &amp;= P(D lvert Gamma, I) &amp;= prod_k Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big] ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} end{align} where $ Gamma = ( Theta, u, eta)$ begin{align} Theta &amp;: textrm{model function parameters} u &amp;: textrm{data purity, i.e., probability that given point is good} eta &amp;: textrm{variance for outliers} end{align} . def func(x,θ): return θ[0]*x + θ[1] def normal(x,μ,σ2): return 1.0/np.sqrt(2*np.pi*σ2)*np.exp(-(x-μ)**2/(2*σ2)) @jit def loss(Γ): # = ln L θ,u,η = Γ[:2],Γ[2],np.exp(Γ[3]) # Γ = (Θ, u, ln(η)) ymodel = func(x,θ) loss_g = normal(y, ymodel, σy**2) loss_b = normal(y, ymodel, σy**2 + η**2) return -np.sum(np.log(u*loss_g + (1-u)*loss_b)) grad_loss = jit(grad(loss)) # nabla ln(L) hess_loss = hessian(loss) # nabla nabla ln(L) bounds = ((None,None), (None,None), (0,1), (None,None)) opt3 = minimize(loss, np.array([1.0, 1.0, 0.5, 1.0]), method=&#39;L-BFGS-B&#39;, bounds=bounds, options={&#39;maxiter&#39;:500}) opt2 = minimize(loss, opt3[&#39;x&#39;], method=&#39;TNC&#39;, bounds=bounds, jac=grad_loss, options={&#39;maxiter&#39;:200, &#39;disp&#39;:1}) opt = minimize(loss, opt2[&#39;x&#39;], method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 print(opt3[&#39;x&#39;]) print(opt2[&#39;x&#39;]) print(opt[&#39;x&#39;]) . Optimization terminated successfully. Current function value: 110.170025 Iterations: 1 Function evaluations: 3 Gradient evaluations: 3 [ 2.2374954 35.27694601 0.80797022 5.63764122] [ 2.23749529 35.27693403 0.80796973 5.63763495] [ 2.2374953 35.27693402 0.80796886 5.63763506] . Recall that within the Laplace approximation, the (posterior) average is given by the optimum, and the variance estimates are obtained from the diagonal of the inverse Hessian matrix (evaluated at the optimum value). . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot;μ_{l} = {avg:.3e}; σ_{l} = {sig:.3e} t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e}&quot;) . μ_m = 2.237e+00; σ_m = 1.100e-01 2.018e+00 &lt; m &lt; 2.457e+00 μ_b = 3.528e+01; σ_b = 1.860e+01 -1.930e+00 &lt; b &lt; 7.248e+01 μ_u = 8.080e-01; σ_u = 1.032e-01 6.015e-01 &lt; u &lt; 1.014e+00 μ_logη = 5.638e+00; σ_logη = 3.922e-01 4.853e+00 &lt; logη &lt; 6.422e+00 . fig,ax = plt.subplots(figsize=(16,9)) xrange = np.linspace(x_sorted[0],x_sorted[-1]) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) ax.set_ylim(-100,800) ax.legend() plt.show() . This is much better, and all we had to do was introduce two additional hyper-parameters, the data purity $u$, and the bad variance $ eta$. . MC solution . Now let&#39;s see how an MC simulation for this mixture model performs . begin{align} P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) ln{L( Gamma)} &amp;= sum_k ln{ Big[u G(y_k lvert Theta) + (1-u) B(y_k lvert Theta, eta) Big]} Pi( Gamma) &amp;= P(m,b u, eta lvert I) end{align}We will assume that the priors are independent of each other. For simplicity, we will also assume uniform priors for all parameters except $ eta$, for which we use Jeffreys&#39; prior begin{align} P(m,b,u, eta lvert I)&amp;= P(m lvert I) times P(b lvert I) times P(u lvert I) times P( eta lvert I) P(m lvert I)&amp;: textrm{Uniform} P(b lvert I)&amp;: textrm{Uniform} P( eta lvert I)&amp; propto frac{1}{ eta} end{align} . Finally, we note that PYMC requires us to specify log-likelihood $ ln{L( Gamma)}$ for our problem . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u): r2 = (y_obs - y_g)**2 G = tt.exp(-r2/(2*σ_obs**2)) / (tt.sqrt(2*onp.pi*σ_obs**2)) B = tt.exp(-r2/(2*(σ_obs**2 + η**2))) / (tt.sqrt(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(tt.log(u*G + (1-u)*B)) with pm.Model() as model_mix: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u}) # make sure there are no divergences with initial point for RV in model_mix.basic_RVs: print(RV.name, RV.logp(model_mix.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 likelihood -162.7700867258116 . with model_mix: trace_mix = pm.sample(10000, tune=20000, progressbar=True, random_seed = 1123581321) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logη, u, b, m] INFO:pymc3:NUTS: [logη, u, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 60000/60000 [01:03&lt;00:00, 950.70draws/s] The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.6970200991002982, but should be close to 0.8. Try to increase the number of tuning steps. . pm.traceplot(trace_mix) plt.show(); . pm.plot_posterior(trace_mix, var_names=[&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_mix, var_names=[&#39;u&#39;, &#39;logη&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo,hi = pm.stats.hpd(trace_mix[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (Laplace)&quot;) print(f&quot; t t {lo:.3e} &lt; {l} &lt; {hi:.3e} t (MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (MC) -1.930e+00 &lt; b &lt; 7.248e+01 (Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (MC) 6.015e-01 &lt; u &lt; 1.014e+00 (Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (MC) . We see that the Laplace approximation gave us a pretty good estimate of the reliability! . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;dark&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, mec=color[&#39;dark&#39;], mew=2) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_mix[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Mixture Model MC&#39;) ax.legend(loc=4) plt.show() . Here we show our predictions for $y$, obtained from the posterior distribution given by the MC simulation, i.e., begin{align} langle y(x) rangle &amp;= int textrm{d} Theta f(x; Theta) p( Theta lvert D, I) end{align} where the shaded regions are showing the $ pm sigma$ and $ pm 2 sigma$ intervals. Note that this could also have been computed within the Laplace approximation. The benefit of the MC is that such averages are trivial to compute from the trace of the simulation. Again, we see excellent agreement between the MC predictions and the fit to the mixture-model. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace_mix[&#39;b&#39;], trace_mix[&#39;m&#39;], scatter=True, nbins=40, colors=[color[&#39;mid&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) ax.set_xlim(-100,100) ax.set_ylim(1.5, 3.0) plt.show() . Numerical Example (Exponential Model) . MC solution . We are now ready to try the MC solution to the full exponential model. Recall that in this case begin{align} Gamma &amp;= ( Theta, u, eta, {q_i }) P( Gamma lvert D, I) &amp; propto L( Gamma) Pi( Gamma) L( Gamma)&amp;= prod_i Big(G(y_i lvert Theta)^{q_i} times B(y_i lvert Theta, eta)^{1-q_i} Big) ln{L( Gamma)} &amp;= sum_i Big[q_i ln{G(y_i lvert Theta)} + (1-q_i) ln{B(y_i lvert Theta, eta)} Big] &amp;= sum_{ text{good}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 sigma_i^2} - frac{1}{2} ln{2 pi sigma_i^2} right] + sum_{ text{bad}} left[- frac{ left(y_i - f(X_i; Theta) right)^2}{2 left( sigma_i^2+ eta^2 right)} - frac{1}{2} ln{2 pi( sigma_i^2+ eta^2)} right] end{align} . We use the same priors as above for $ Theta$, $u$, $ eta$, and use a Bernoulli distribution for the $ {q_k }$, such that . begin{align} Pi( {q_k }, u) &amp;= P( {q_k }, u lvert I) &amp;= P( {q_k } lvert u, I) P(u lvert I) &amp; propto P( {q_k } lvert u, I) &amp;= prod_k P(q_k lvert u, I) &amp;= prod_k u^{q_k} times left(1-u right)^{1-q_k} &amp;= u^M (1-u)^{N-M} end{align}with $M= sum_k q_k$ . # https://docs.pymc.io/advanced_theano.html # y_in = m x_i + b def logp(y_obs, σ_obs, y_g, η, u, q): r2 = (y_obs - y_g)**2 G = -0.5*q*(r2/σ_obs**2 + tt.log(2*onp.pi*σ_obs**2)) B = -0.5*(1-q)*(r2/(σ_obs**2 + η**2) + tt.log(2*onp.pi*(σ_obs**2+η**2))) return tt.sum(G + B) with pm.Model() as model_exp: # observed data as Theano shared variables ~ global data x_obs = th.shared(onp.array(x), name=&#39;x_obs&#39;) y_obs = th.shared(onp.array(y), name=&#39;y_obs&#39;) σ_obs = th.shared(onp.array(σy), name=&#39;σ_obs&#39;) # priors for unkown line model parameters (m,b) m = pm.Uniform(&#39;m&#39;, lower=0, upper = 10, testval=1) b = pm.Uniform(&#39;b&#39;, lower=-800, upper = 800, testval=0) y_g = pm.Deterministic(&#39;y_g&#39;, m*x_obs + b) # priors for outlier parameters (u, η) u = pm.Uniform(&#39;u&#39;, lower=0, upper=1, testval=0.5) logη = pm.Uniform(&#39;logη&#39;, lower=-10, upper=10, testval=5) η = pm.Deterministic(&#39;η&#39;, tt.exp(logη)) q = pm.Bernoulli(&#39;q&#39;, p=u, shape=x.shape) likelihood = pm.DensityDist(&#39;likelihood&#39;, logp, observed={&#39;y_obs&#39;: y_obs,&#39;σ_obs&#39;:σ_obs,&#39;y_g&#39;:y_g, &#39;η&#39;:η, &#39;u&#39;:u, &#39;q&#39;:q}) # make sure there are no divergences with initial point for RV in model_exp.basic_RVs: print(RV.name, RV.logp(model_exp.test_point)) . m_interval__ -2.4079456086518722 b_interval__ -1.3862943611198906 u_interval__ -1.3862943611198906 logη_interval__ -1.6739764335716716 q -13.862943611198906 likelihood -148.95080195551284 . with model_exp: trace_exp = pm.sample(20000, tune=20000, progressbar=True, random_seed = 1123581321) . Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) CompoundStep INFO:pymc3:CompoundStep &gt;NUTS: [logη, u, b, m] INFO:pymc3:&gt;NUTS: [logη, u, b, m] &gt;BinaryGibbsMetropolis: [q] INFO:pymc3:&gt;BinaryGibbsMetropolis: [q] Sampling 2 chains, 0 divergences: 100%|██████████| 80000/80000 [02:13&lt;00:00, 600.12draws/s] The number of effective samples is smaller than 10% for some parameters. WARNING:pymc3:The number of effective samples is smaller than 10% for some parameters. . pm.traceplot(trace_exp, var_names=[&#39;m&#39;, &#39;b&#39;,&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;m&#39;, &#39;b&#39;]); . pm.plot_posterior(trace_exp, [&#39;u&#39;, &#39;logη&#39;]); . pm.plot_posterior(trace_exp, [&#39;q&#39;]); . for i,l in enumerate([&#39;m&#39;, &#39;b&#39;, &#39;u&#39;, &#39;logη&#39;]): lo0,hi0 = pm.stats.hpd(trace_mix[l]) lo,hi = pm.stats.hpd(trace_exp[l]) avg,sig = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i][i]) print(f&quot; t t {avg-2*sig:.3e} &lt; {l} &lt; {avg+2*sig:.3e} t (mixture model - Laplace)&quot;) print(f&quot; t t {lo0:.3e} &lt; {l} &lt; {hi0:.3e} t (mixture model - MC)&quot;) print(f&quot; t t {lo :.3e} &lt; {l} &lt; {hi :.3e} t (exponential model - MC)&quot;) print() . 2.018e+00 &lt; m &lt; 2.457e+00 (mixture model - Laplace) 2.011e+00 &lt; m &lt; 2.466e+00 (mixture model - MC) 2.002e+00 &lt; m &lt; 2.469e+00 (exponential model - MC) -1.930e+00 &lt; b &lt; 7.248e+01 (mixture model - Laplace) -4.207e+00 &lt; b &lt; 7.274e+01 (mixture model - MC) -4.059e+00 &lt; b &lt; 7.498e+01 (exponential model - MC) 6.015e-01 &lt; u &lt; 1.014e+00 (mixture model - Laplace) 5.701e-01 &lt; u &lt; 9.501e-01 (mixture model - MC) 5.566e-01 &lt; u &lt; 9.479e-01 (exponential model - MC) 4.853e+00 &lt; logη &lt; 6.422e+00 (mixture model - Laplace) 4.973e+00 &lt; logη &lt; 6.631e+00 (mixture model - MC) 4.961e+00 &lt; logη &lt; 6.652e+00 (exponential model - MC) . As expected, the marginalized distributions for the model parameters $m$, $b$, $u$, and $ eta$ are essentially the same, regardless of whether we look at the mixture model or the exponential model. . The Good and the Bad . The benefit of using the exponential model, with additional hyper-parameters for the good/bad state of each point, is that we can make inferences about the state of the points. Here, we are again following Jayne&#39;s (see Chapter 4, Elementary Hypothesis Testing). The odds for a given hypothesis, in this case, the odds for point $i$ to be a good point is defined as . begin{align} O(q_i = 1 | D, I) &amp; equiv frac{P(q_i = 1 | D, I)}{P(q_i = 0 | D, I)} end{align}where $P(q lvert D, I)$ is given by marginalization of the full posterior begin{align} P(q_i lvert D, I) &amp;= int mathrm{d} Theta mathrm{d}u mathrm{d} eta , sum_{ {q_j }_{j ne i}} P( Theta, u, eta, {q_j } lvert D, I) end{align} . Since the points generated from the MC simulation are drawn in proportion to the full posterior, the marginalized distribution for $q_i$ is obtained by simply taking the trace over all other parameters. That is, we can approximate $P(q_i lvert D, I)$ from the histogram of the $q_i$ values themselves. . It&#39;s more convenient to look at this quantity in decibels, begin{align} e(q_i = 1 | D, I) &amp; equiv 10 log_{10}{O(q_i = 1 | D I)} end{align} Jayne&#39;s calls $e$ the evidence, but this conflicts with the (somewhat widespread) use of evidence to denote the normalization constant (partition function) in the definition of the posterior. We&#39;ll just call it $e$. . belief = pd.DataFrame({&#39;P_good&#39;:onp.array([1/2, 2/3, 4/5, 10/11, 0.954, 100/101, 0.999, 0.9999])}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.500000 | 1.00000 | 0.000000 | . 1 0.666667 | 2.00000 | 3.010300 | . 2 0.800000 | 4.00000 | 6.020600 | . 3 0.909091 | 10.00000 | 10.000000 | . 4 0.954000 | 20.73913 | 13.167905 | . 5 0.990099 | 100.00000 | 20.000000 | . 6 0.999000 | 999.00000 | 29.995655 | . 7 0.999900 | 9999.00000 | 39.999566 | . This provides three scales for measuring our degrees of belief in $q_i$: $p(q_i =1)$, $O(q_i = 1)$, and $e(q_i = 1)$. It turns out we have a better intuition for $e$, with $3$db corresponding to a factor of 2 (in the odds), $10$db to a factor of 10, and $13$db to a probability of $95%$ (i.e. the $2 sigma$ criteria). . Let&#39;s calculate these quantities for each of the data points. The probability that a given $q_i$ is good is simply the fraction of points (in the parameter space) where $q_i = 1$, regardless of the values of the other $q_j$ (or the other parameter values). . fig, ax = plt.subplots() ax.plot(trace_exp[&#39;q&#39;][::100,0], marker=&#39;o&#39;, ls=&#39;None&#39;, ms=4, color=color[&#39;dark&#39;]); ax.set_xlabel(&#39;Trace&#39;) ax.set_ylabel(r&#39;$q_0$&#39;) plt.show() . This is what the trace for $q_0$ looks like (note that we only show every 100 points) . belief = pd.DataFrame({&#39;P_good&#39;:onp.average(trace_exp[&#39;q&#39;], axis=0)}) belief[&#39;O_good&#39;] = belief[&#39;P_good&#39;] / (1.0 - belief[&#39;P_good&#39;]) belief[&#39;e_good&#39;] = 10*onp.log10(belief[&#39;O_good&#39;]) belief . P_good O_good e_good . 0 0.778600 | 3.516712 | 5.461368 | . 1 0.000400 | 0.000400 | -33.977663 | . 2 0.000000 | 0.000000 | -inf | . 3 0.000350 | 0.000350 | -34.557799 | . 4 0.966475 | 28.828486 | 14.598218 | . 5 0.945725 | 17.424689 | 12.411650 | . 6 0.943750 | 16.777778 | 12.247344 | . 7 0.944525 | 17.026138 | 12.311161 | . 8 0.934400 | 14.243902 | 11.536290 | . 9 0.904800 | 9.504202 | 9.779156 | . 10 0.970875 | 33.334764 | 15.228974 | . 11 0.875375 | 7.024072 | 8.465890 | . 12 0.873675 | 6.916089 | 8.398606 | . 13 0.952650 | 20.119324 | 13.036134 | . 14 0.955125 | 21.284123 | 13.280558 | . 15 0.841175 | 5.296238 | 7.239675 | . 16 0.938275 | 15.200891 | 11.818690 | . 17 0.954950 | 21.197558 | 13.262858 | . 18 0.963750 | 26.586207 | 14.246564 | . 19 0.959125 | 23.464832 | 13.704174 | . Notice that there are only three negative evidence values, corresponding to the three obvious outlier. This means that all other points are more likely to be good than bad ($p&gt;1/2$). If we want, we can now label our points to show our belief in their good/bad state. Note, that this is purely a cosmetic procedure, our estimate for the model parameters, $m$ and $b$, is obtained by averaging over all possible configurations of the system, consistent with our measured data and prior information. . fig,ax = plt.subplots(figsize=(16,9)) ax.errorbar(x, y, σy, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;superfine&#39;], label=&#39;data&#39;, mfc=&#39;None&#39;, alpha=0.6, mec=&#39;k&#39;, mew=1.5, lw=1.5) ax.plot(xrange,func(xrange, fit[&#39;x&#39;]), color=color[&#39;mid&#39;], label=&#39;Least-Squares Fit&#39;, ls=&#39;:&#39;) ax.plot(xrange,func(xrange, opt[&#39;x&#39;][:2]), color=color[&#39;dark&#39;], label=&#39;Mixture Model Fit&#39;, ls=&#39;--&#39;) ax.set_xlabel(r&#39;$x$&#39;); ax.set_ylabel(r&#39;$y$&#39;) plot_quantiles(ax, x[i_sort], trace_exp[&#39;y_g&#39;][:,i_sort], quantiles_sig2, label=&#39;Exponential Model MC&#39;) im = ax.scatter(x,y,c=np.clip(belief[&#39;e_good&#39;].values, a_min=-10, a_max=10), cmap=&#39;coolwarm&#39;) ax.legend(loc=4) cb = colorbar(ax,im) cb.set_label(r&#39;$e(q_i = 1)$&#39;) plt.show() . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "relUrl": "/data%20analysis/parameter%20estimation/outliers/2020/06/01/Parameter-Estimation-With-Outliers.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayesian Parameter Estimation",
            "content": "Motivation / Disclaimer . This is the first in what I hope will be a long series of posts on Data Analysis, Probabilistic Programming and Machine Learning. We have recently become interested in incorporating such techniques into our more traditional Physics simulations and for this, we started a Seminar/Reading club with colleagues in our University. I plan to posts all of our study guides here. These posts are only intended as an easy way to store and retrieve our notes, nothing more...so expect brevity and don&#39;t be too disappointed if you find any glaring mistakes and/or omissions (but please let me know if you do). . We start with the basics of Bayesian Data Analysis, following the excellent tutorial book &quot;Data Analysis : A Bayesian Tutorial&quot;, written by Dr. Devinder S. Sivia, with contributions by Prof. John Skilling. Dr. Sivia is a research scientist at the Rutherford Appleton Lab and Prof. Skilling was at the DAMPT (Cambridge) before becoming a founding director of the Maximum Entropy Data Consultants group. The book is written by/for physicist and includes advanced topics (such as Maximum Entropy and Nested Sampling) that we hope to cover in future posts. . References . While mainly following Sivia&#39;s book, we have also found the following references extremely helpful . Data Analysis : A Bayesian Tutorial, second edition. D.S. Sivia with J. Skilling, Oxford, Oxford University Press (2006) | Probability Theory: The Logic of Science. E. T. Jaynes, Cambridge, Cambridge Unviversity Press (2003) | Bayesian Data Analysis, Third Edition. A. Gelman, J. B. Carlin, H. Stern et al., Chapman &amp; Hall/CRC Texts in Statistical Science (2013) | &quot;Frequentism and Bayesianism&quot; Blog-Post series (I, II, III, IV, IV) by Jake VanderPlas | Michael Betancourt&#39;s writings on probability theory. | . Preliminaries . Basic Rules . The probability $P(X lvert I)$ expresses our belief that $X$ is true given prior information $I$. The two basic rules of probability theory are just the sum and product rules, $$ begin{align} P(X lvert I) + P( overline{X} lvert I) &amp;= 1 &amp; textrm{(Sum Rule)} label{e:sum_rule} P(X,Y lvert I) &amp;= P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Product Rule)} notag &amp;=P(Y lvert X, I) times P(X lvert I) label{e:product_rule} end{align}$$ where $ overline{X}$ denotes the negation of $X$. See Sivia or Jaynes for a derivation, originally due to Cox, which shows that these are the only rules that will guarantee consistency. . From the product rule, one can easily derive Bayes&#39; Theorem $$ begin{align*} overbrace{P(X lvert Y, I)}^{ small{ mathrm{posterior}}} &amp;= frac{ overbrace{P(Y lvert X,I)}^{ small{ mathrm{likelihood}}} times overbrace{P(X lvert I)}^{ small{ mathrm{prior}}}}{ underbrace{P(Y lvert I)}_{ small{ mathrm{evidence}}}}&amp; textrm{(Bayes&#39; Theorem)} label{e:bayes} end{align*}$$ which states that the &quot;posterior&quot; is proportional to the &quot;likelihood&quot; times the &quot;prior&quot;, with the &quot;evidence&quot; a normalization constant. Posterior, likelihood and prior are the standard terms for these quantities, but the &quot;evidence&quot; label seems not to be widespread (we are following Sivia&#39;s nomenclature here). Note that there is nothing special about these labels, one&#39;s prior can be another&#39;s posterior, and your posterior today can be your prior tomorrow. Basically, the posterior is what we want to calculate, but can&#39;t (at least not directly), the prior is what we start with, and the likelihood is something we can easily calculate. . The sum rule can be extended to the to a series of mutually exclusive and exhaustive set of propositions $ {Y_k }$, such that $ sum_k P(Y_k lvert I) = 1$, and in the continuum limit we obtain the following marginalization property . $$ begin{align*} P(X lvert I) &amp;= int textrm{d}Y P(X, Y lvert I) = int textrm{d}Y P(X lvert Y, I) times P(Y lvert I) &amp; textrm{(Marginalization)} label{e:marginalization} end{align*}$$This is incredibly useful when there are unknown quantities that are required to compute the likelihood, but which are not really of interest to us. These so-called nuisance parameters can then be introduced and integrated out. We are being a bit careless here, and intermixing probabilities with probability densities. Strictly speaking, if $Y$ is a real random variable we should say that the probability for $Y$ to be within the range $y le Y le y+ Delta y$ is . $$ begin{align*} P(y le Y le y + Delta y)= rho(Y=y) Delta t end{align*}$$with $ rho$ the probability density function. However, following Sivia we will use the same symbol for both quantities, as it should be apparent from the context which one we are referring to. . Bayesian Data Analysis . It is easier to recognize how this Bayesian framework fits within a Data Analysis problem if we rewrite Bayes theorem in the form . $$ begin{align*} P( textrm{Hypothesis} lvert textrm{Data}, I) &amp;= frac{P( textrm{Data} lvert textrm{Hypothesis}, I) times P( textrm{Hypothesis} lvert I)}{P( textrm{Data}, I)} label{e:bayes_hypothesis} end{align*}$$or as . $$ begin{align*} P( Theta lvert D, I) &amp;= frac{P(D lvert Theta, I) times P( Theta lvert I)}{P(D lvert I)} label{e:bayes_theta} &amp; propto P(D lvert Theta, I) times P( Theta lvert I) end{align*}$$where $ Theta$ denotes the parameters of our model, $D$ our measured experimental/simulation data. The questions we are looking to answer, which these posteriors allow us to formulate are the following: &quot;What does my data say about my hypothesis&quot; or &quot;What does my data say about the model parameters&quot;? For what follows, we will focus exclusively on the parameter estimation problem. . One shot or sequential analysis? . Assume that our experimental data consists of a series of $N$ measurements, $D = {D_k }_{k=1}^N$. Our prior information $I$ specifies the model we believe explains this data, and we want to infer the parameters of this model. . Thus, what we want is the posterior of $ Theta$ given $D$, which gives $$ begin{align*} P( Theta lvert D,I) &amp; propto P( {D_k }_{k=1}^N lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert D_1, Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=2}^N lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp;= P( {D_k }_{k=3}^N lvert Theta, I) times P(D_2 lvert Theta, I) times P(D_1 lvert Theta, I) times P( Theta lvert I) &amp; vdots &amp;= left( Pi_{k=1}^N P(D_k lvert Theta, I) right) times P( Theta lvert I) end{align*}$$ . In step $2$, we are using the product rule to peel off one of the $D_i$, then in step $3$, we assume that (given the prior and the model parameters), knowledge of $D_1$ does not give us any extra information regarding the probabilities of observing $ {D_k }_{k ge 2}$, which means that they are independent. This procedure can be repeated until the original (one-shot) likelihood, which quantifies the probability of obtaining the entire set of data, is written as a product of individual likelihoods. From this, it is easy to see that we can analyze the data in one-shot, or sequentially, as the measurements arrive. The two routes are equivalent. . To see this, consider the expression obtained after peeling off m elements from the original likelihood. The posterior is given as $$ begin{align*} P( Theta lvert D,I)&amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) big( Pi_{j=1}^{m} P(D_j lvert Theta, I) big) P( Theta lvert I) &amp; propto big( Pi_{k=m+1}^{N} P(D_k lvert Theta, I) big) times P( Theta lvert {D_j }_{j=1}^m, I) end{align*}$$ where the second and third terms in the rhs of the first equation are nothing but the likelihood and the prior after obtaining the first $m$ measurements. Thus, we can analyze the first $m$ data points, obtaining a posterior $P( Theta lvert {D_j }_{j=1}^m, I)$, and then use this as a prior when analyzing the remaining $N-m$ points. . What we can never ever do, however, is to use the posterior $P( Theta lvert {D_j }_{j=1}^m, I)$ to re-analyze the $ {D_j }_{j=1}^m$ measurements! . Laplace&#39;s Approximation, Best Estimates and Error Bars . Everything we can say about our parameters, given the Data and prior information, is contained in the posterior $P( Theta lvert D, I)$. However, this quantity can be quite complicated to deal with, particularly when the number of parameters exceeds 3. Therefore, it can be useful to develop approximations to this posterior. In particular, if the distribution is simple enough we can try to approximate it with a Gaussian distribution. Then, all that are needed are the first two moments, i.e., the average and the variance. The average give the peak of the distribution (the most likely point) and the variance an estimate of its spread, and thus of the reliability of our estimate. . This approximation is commonly referred to as Laplace&#39;s approximation, and amounts to a Taylor expansion of the logarithm of the posterior around the maximum (truncated to second order). Consider the general multi-dimensional case with $p$ parameters, $ Theta = ( Theta^{1}, Theta^{2}, ldots, Theta^{p})$, and let $ Theta_0 = text{argmax}{P( Theta lvert D, I)}$ . $$ begin{align*} L &amp;= ln{P( Theta lvert D, I)} = ln P &amp;= L lvert_{ Theta_0} + nabla L lvert_{ Theta_0} cdot( Theta - Theta_0) + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) + mathcal{O} big(( Theta- Theta_0)^3 big) &amp; simeq L lvert_{ Theta_0} + frac{1}{2} ( Theta- Theta_0)^{t} cdot nabla nabla L lvert_{ Theta_0} cdot( Theta- Theta_0) end{align*}$$where $ nabla= nabla_{ Theta}$, $A^t$ denotes the transpose of $A$, $( cdot)$ a matrix-matrix multiplication, and $ nabla nabla L$ is the (Hessian) matrix of second derivatives. Notice that the first-order term vanishes, since $ nabla L lvert_{ Theta_0} = 0$ by definition. . We can then re-exponentiate this expression to obtain an approximation to our original posterior distribution $$ begin{align*} P( Theta lvert D,I) propto exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot left(- nabla nabla L lvert_{ Theta_0} right) cdot left( Theta- Theta_0 right) right]} end{align*}$$ . Multi-variate Gaussians . Recall the definition of a multi-variate Gaussian distribution for a random $d$-dimensional vector $A$, expressed as $A sim mathcal{N}( mu, Sigma)$ $$ begin{align*} mathcal{N}(A lvert mu, Sigma) &amp;= frac{1}{ sqrt{(2 pi)^d det{ Sigma}}} exp{ left[- frac{1}{2} left(A- mu right)^t cdot Sigma^{-1} cdot left(A- mu right) right]} left langle A right rangle &amp;= mu left langle(A^i - mu^i)(A^j - mu^j) right rangle &amp;= Sigma^{ij} int text{d}A mathcal{N}(A lvert mu, Sigma) &amp;=1 end{align*}$$ where $ langle cdot rangle$ denotes an average over $P_{ mathcal{N}}$. . Such distributions have many interesting properties. In particular, if A is partitioned into $B$ and $C$, $A= begin{pmatrix}B C end{pmatrix}$, the marginalization over $B$ ($C$), results in another multi-variate Gaussian distribution for $C$ ($B$) $$ begin{align*} P(B lvert mu, Sigma) &amp;= int textrm{d}C mathcal{N} left( left. begin{pmatrix}B C end{pmatrix} right lvert begin{pmatrix} mu_B mu_C end{pmatrix}, begin{pmatrix} Sigma_{BB} &amp; Sigma_{BC} Sigma_{CB}&amp; Sigma_{CC} end{pmatrix} right) &amp;= frac{1}{ sqrt{(2 pi)^{d_B} det{ Sigma_{BB}}}} exp{ left[- frac{1}{2} left(B - mu_B right)^t Sigma_{BB}^{-1} left(B - mu_B right) right]} &amp; equiv mathcal{N}( mu_{B}, Sigma_{BB}) end{align*}$$ This can be easily proved by completing the squares in the exponent of the joint distribution. . Thus, since each component of $A$ is itself described by a Gaussian distribution, the best estimate is given by $$ begin{align*} A &amp;= mu pm sigma sigma &amp;= sqrt{ text{diag}{ Sigma}} end{align*}$$ . Finally, we see that the Laplace approximation of the posterior distribution for $ Theta$ is nothing but a multi-variate Gaussian, with average and covariance matrix given by $$ begin{align*} mu&amp; rightarrow Theta_0 Sigma&amp; rightarrow (- nabla nabla L lvert_{ Theta_0})^{-1} end{align*}$$ . Bayesian Parameter Estimation . Fitting with (known) noise . Let&#39;s see how this Bayesian formalism can be applied in practice. Our data consists of a series of noisy measurements $D= {y_k }_{k=1}^N$, which on prior information $I$ are assumed to be generated by a model $y = f(X; Theta)$, parametrized by $p$ parameters $ Theta = ( Theta^1, ldots, Theta^p)$. We consider the case where $X=(X^1, ldots, X^d)$ is a d-dimensional vector, but $y$ is assumed to be a scalar. . The posterior for $ Theta$, given the Data $D$ and the prior information $I$, is then $$ begin{align*} P( Theta lvert D, I) &amp; propto P(D lvert Theta, I) times P( Theta lvert I) &amp; propto Pi_k P(y_k lvert Theta, I) &amp;= Pi_k frac{1}{ sqrt{2 pi} sigma_k} exp{ left[- frac{ big(y_k - f(X_k; Theta) big)^2}{2 sigma_k^2} right]} end{align*}$$ where we have assumed that : . The prior $P( Theta lvert I)$ is a constant | The $y_k$ are independent of each-other (on knowing $ Theta$ and $I$), such that $P(y_i,y_j lvert Theta, I) = P(y_i lvert Theta, I) P(y_j lvert Theta, I)$ for $i ne j$. | The measurement error, and thus the likelihood, is Gaussian with variance $ sigma_k^2$ | . Let&#39;s see what the best-estimate would be in this case. . In practice, it will be more convenient to work with the logarithm of the probabilities, than with the probabilities themselves. Since $ ln$ is a monotonically increasing function, the maximum of $P$ will coincide with the maximum of $ ln{P}$. The log posterior is given by . $$ begin{align*} L &amp;= textrm{constant} - sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma_k^2} &amp;= textrm{constant} - frac{1}{2} chi^2 end{align*}$$where $ chi^2= sum_k(y_k - f_k)^2/ sigma_k^2$ is the sum of the squared residuals, weighted by the inverse of the error, and all terms not depending on $ Theta$ have been absorbed into the normalization constant. . The best estimate $ Theta_0$, which maximizes the posterior, is found by setting the gradient of $L$ to zero, . $$ begin{align*} nabla L = - frac{1}{2} nabla chi^2 = 0 Longrightarrow nabla chi^2 = 0 end{align*}$$which coincides with setting the gradient of $ chi^2$ to zero. Thus, in this case, maximizing the posterior is equivalent to a least-squares fit! . Finally, our reliability estimate is obtained by computing the matrix of second derivatives, evaluated at the optimal value $ Theta_0$ $$ begin{align*} Sigma_{ Theta}^{-1}=- nabla nabla L lvert_{ Theta_0} = frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0} end{align*}$$ . Approximating our posterior as a Gaussian, our best estimate for the model parameters would be $$ begin{align*} Theta = Theta_0 pm sigma_{ Theta} ; qquad left( sigma_{ Theta}= sqrt{ text{diag}( Sigma_{ Theta})} right) end{align*}$$ . Fitting with uknown noise . What happens in the case where don&#39;t have an estimate of the error bars? How would we derive the best-estimate in this case? What quantity should we minimize? . The solution is simply, use marginalization and average over all possible values of $ sigma$ (assumed to be the same for all points) $$ begin{align} P( Theta lvert D, I) &amp;= int textrm{d} sigma , P( Theta, sigma lvert D, I) &amp; propto int textrm{d} sigma , P(D lvert Theta, sigma, I) times P( Theta, sigma lvert I) end{align}$$ where the last step uses Bayes&#39; to write the integrand as the likelihood and the prior. Notice that this is the average of $P(D lvert Theta, sigma, I)$ over $P( Theta, sigma lvert I)$. . Under the same simplifying assumptions as above, i.e., independent measurements, uniform prior for $ Theta$, and Gaussian noise, we can again decompose the likelihood of $D$ into a product of likelihoods for the individual measurements $$ begin{align} P( Theta lvert D, I) &amp; propto int_0^ infty textrm{d} sigma left( Pi_k frac{1}{ sqrt{2 pi} sigma} exp{ left[- frac{ left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} right]} right) times frac{1}{ sigma} &amp;= (2 pi)^{-N/2} int_0^ infty frac{ textrm{d} sigma}{ sigma} sigma^{-N} exp{ left[- frac{1}{2 sigma^2} sum_k left(y_k - f(X_k; Theta) right)^2 right]} end{align}$$ . Here we have used Jeffreys&#39; prior for $ sigma$, $P( sigma lvert I) propto 1/ sigma$, as it is a scale-parameter. Depending on your prior information (e.g., do you know the order of magnitude?), there may be more appropriate priors, but we leave that for a future discussion. See Sivia and or Jaynes for a more in depth review of such issues. In practice, if $N$ is large enough this will not make a big difference, choosing Jeffreys prior or a constant prior for $ sigma$ will give essentially the same predictions. . The integral over $ sigma$ can be done analytically through the following variable transformation, with $S = sum_k (y_k - f_k)^2$ $$ begin{align} t &amp;= frac{ sum_k left(y_k - f(X_k; Theta) right)^2}{2 sigma^2} = frac{S}{2 sigma^2} frac{ textrm{d} sigma}{ sigma} &amp;= - frac{ textrm{d}t}{2t} end{align}$$ . The posterior is then $$ begin{align} P( Theta lvert D, I) &amp; propto (2 pi)^{-N/2} int_0^ infty frac{ textrm{d}t}{2t} left( frac{2t}{S} right)^{N/2} e^{-t} &amp;= frac{ pi^{-N/2}}{2} underbrace{ left( int_0^ infty frac{ textrm{d}t}{t} t^{N/2} e^{-t} right)}_{= Gamma(N/2)}S^{-N/2} end{align}$$ . where the quantity in parenthesis is independent of $ Theta$ and equal to the Gamma function of $N/2$. . As before, let&#39;s look at the log-posterior . $$ begin{align} L &amp;= textrm{constant} - frac{N}{2} ln{S} end{align}$$The best-estimate is again that which maximizes $L$ $$ begin{align} nabla L &amp;= - frac{N}{2} frac{ nabla S}{S}= 0 Longrightarrow nabla S = 0 end{align}$$ which in this case corresponds to minimizing $ ln{S}$ or just $S$. So far it seems that the procedure is the same, minimize the sum of the squared residuals (weighted by the magnitude of the error, if known). The differences come when we consider the reliability estimate. . The Hessian is now $$ begin{align} nabla nabla L &amp;= - frac{N}{2} left[ frac{ nabla nabla S}{S} - frac{ left( nabla S right) left( nabla S right)}{S^2} right] end{align}$$ . which reduces to the following when evaluated at the optimum point $ Theta_0$ (since $ nabla S lvert_{ Theta_0} equiv 0$) $$ begin{align} nabla nabla L lvert_{ Theta_0} &amp;= - frac{N}{2} frac{ nabla nabla S lvert_{ Theta_0}}{S_0} = - frac{1}{2} left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} end{align}$$ . Within the Laplace approximation, the covariance matrix for the posterior distribution of $ Theta$ is then given by $$ begin{align} Sigma_ Theta^{-1} &amp;= frac{1}{2} left( left. nabla nabla left( frac{S}{S_0/N} right) right lvert_{ Theta_0} right) end{align}$$ . This expression is very similar to the one we obtained above, in the case where the measurement error $ sigma$ was known, $ Sigma_ theta^{-1}=- frac{1}{2} nabla nabla chi^2 lvert_{ Theta_0}$. Here, we have replaced $ sigma_k^2$ with an estimate derived from the data $S_0/N$ . $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(X_k; Theta) right)^2}{ sigma_k^2} &amp; longrightarrow frac{S}{S_0/N} = frac{N}{S_0} sum_k left(y_k - f(X_k; Theta) right)^2 sigma_k^2 &amp; longrightarrow frac{S_0}{N} = frac{1}{N} sum_{k} left(y_k - f(X_k; Theta_0) right)^2 end{align}$$However, $S_0/N$ is not necessarily the best estimate for the amplitude of the noise, as we will see below. . ... about the noise . What does the data say about the noise, i.e., what is $P( sigma lvert D, I)$? . To be able to compute this quantity, we again make use of marginalization, treating the model parameters as nuisance parameters and integrating over them. $$ begin{align} P( sigma lvert D, I) &amp;= int textrm{d} Theta P( sigma, Theta lvert D, I) &amp; propto int textrm{d} Theta P(D lvert sigma, Theta, I) P( sigma, Theta lvert I) end{align}$$ . Assuming $ sigma$ and $ Theta$ are independent, $P( sigma, Theta lvert I)= P( sigma lvert I) P( Theta lvert I)$, and taking a uniform prior for $ Theta$ and Jeffreys prior for $ sigma$, we have $$ begin{align} P( sigma lvert D, I) propto sigma^{-(N+1)} int textrm{d} Theta exp{ left[- frac{S( Theta)}{2 sigma^2} right]} end{align}$$ where $S$ is still the sum of squared residuals $S = sum_k (y_k - f(X_k; Theta))$. We get one factor of $1/ sigma$ from the normalization constant of the likelihood for each data measurement, and one from the prior. . Without having to assume anything regarding the form of the model function $f(X)$, we can proceed by again using the Laplace approximation, now expanding $S( Theta)$ around the minimum $ Theta_0$ (which corresponds to the maximum of the posterior $P( Theta lvert D, I)$) $$ begin{align} S simeq S_0 + nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) + frac{1}{2} left( Theta - Theta_0 right)^t cdot nabla nabla S lvert_{ Theta_0} cdot left( Theta- Theta_0 right) end{align}$$ . The posterior $P( sigma lvert D, I)$ is then $$ begin{align} P( sigma lvert D, I)&amp; propto sigma^{-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} int textrm{d} Theta exp{ left[- frac{1}{2} left( Theta- Theta_0 right)^t cdot frac{ nabla nabla S lvert_{ Theta_0}}{2 sigma^2} cdot left( Theta- Theta_0 right) right]} &amp; propto sigma^{p-(N+1)} exp{ left[- frac{S_0}{2 sigma^2} right]} end{align}$$ where the $p$-dimensional integral over $ Theta$ gives us a factor of $ sqrt{(2 pi)^p det{ Sigma}_{ sigma}}$, with $ Sigma_ sigma^{-1} = - frac{1}{2 sigma^2} nabla nabla S lvert_{ Theta_0}$. . To find the best-estimate for $ sigma$ we maximize the log posterior $$ begin{align} L &amp;= ln{P( sigma lvert D, I)} = textrm{const} + left(p - N - 1 right) ln{ sigma} - frac{S_0}{2 sigma^2} frac{ textrm{d}L}{ textrm{d} sigma} &amp;= (p-N-1) frac{1}{ sigma} + frac{S_0}{ sigma^3} frac{ textrm{d}^2L}{ textrm{d}^2 sigma} &amp;= (N+1-p) frac{1}{ sigma^2} - frac{3 S_0}{ sigma^4} end{align}$$ . Setting $ text{d} L / text{d} sigma lvert_{ sigma_0} = 0$, we get $$ begin{align} sigma_0^2 = frac{S_0}{N+1-p} end{align}$$ . with the reliability estimates given by the inverse of the Hessian $$ begin{align} left. frac{ textrm{d}^2L}{ textrm{d}^2 sigma} right lvert_{ sigma_0} &amp;= frac{(N+1-p) sigma_0^2 - 3S_0}{ sigma_0^4} &amp;= frac{-2(N+1-p)}{ sigma_0^2} end{align}$$ . we get $$ begin{align} sigma = sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align}$$ . Example : Fitting a Straigh-Line (known measurement error) . Now, let&#39;s see how all this works out in practice. For convenience, we will use the jax library and take advantage of its automatic differentiation capabilities, which allow us to calculate the gradients and Hessians exactly (to machine precision). If you are on Windows this will not work, in that case remove all the jax commands. . Within the Laplace approximation, we can work out the best estimates for all our quantities by simply maximizing the appropriate posterior. However, to compare with the full solution, we will also make use of the PYMC3 library, which allows us to perform Hamiltonian Monte-Carlo simulations and generate samples which converge to those of the posterior. For the minimiztion, we make use of the scipy library. . Please install the following packages to run the code below . numpy | matplotlib | pymc3 | theano | scipy | jax | . #import numpy as np #### If using windows... import jax.numpy as np import numpy as onp import numpy.random as random import matplotlib as mpl import matplotlib.pyplot as plt import pymc3 as pm import theano import theano.tensor as tt from scipy.optimize import minimize from jax import grad, jit, vmap, jacfwd, jacrev from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) . mpl.style.use([&#39;seaborn-poster&#39;, &#39;seaborn-muted&#39;]) #betanalphas&#39; colormap colors = [&quot;#DCBCBC&quot;,&quot;#C79999&quot;,&quot;#B97C7C&quot;,&quot;#A25050&quot;,&quot;#8F2727&quot;, &quot;#7C0000&quot;,&quot;#DCBCBC20&quot;, &quot;#8F272720&quot;,&quot;#00000060&quot;] color = {i[0]:i[1] for i in zip([&#39;light&#39;,&#39;light_highlight&#39;,&#39;mid&#39;,&#39;mid_highlight&#39;,&#39;dark&#39;,&#39;dark_highlight&#39;,&#39;light_trans&#39;,&#39;dark_trans&#39;,&#39;superfine&#39;],colors)} def hessian(f): &quot;&quot;&quot;Returns a function which computes the Hessian of a function f if f(x) gives the values of the function at x, and J = hessian(f) J(x) gives the Hessian at x&quot;&quot;&quot; return jit(jacfwd(jacrev(f))) quantiles_sig = np.array([.0014,.0228,.1587,0.5, 0.8413,.9772,.9986]) # ( mu +/- 3σ, mu +/- 2σ, mu +/- σ) quantiles_dec = np.arange(0.1, 1.0, 0.1) # [0.1, ..., 0.9] -&gt; (80%, 60%, 40%, 20%) credible interval def plot_quantiles(ax, xdata, ydata,quantiles,colors=colors,**kwargs): &quot;&quot;&quot;Plot quantiles of data as a function of x Note : q-th quantile of &#39;data&#39; is the value &#39;q&#39; away from the minimum to the maximum in a sorted copy of &#39;data&#39;&quot;&quot;&quot; quantiles = np.quantile(ydata,quantiles, axis=0) for i,c in zip(range(len(quantiles)//2), colors): ax.fill_between(xdata, quantiles[i,:], quantiles[-(i+1),:], color=c) ax.plot(xdata, quantiles[len(quantiles)//2], color=colors[-1], lw=4, **kwargs) # Auxiliary routines to plot 2D MCMC data # adapted from http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ def compute_sigma_level(trace1, trace2, nbins=20): &quot;&quot;&quot;From a set of traces, bin by number of standard deviations&quot;&quot;&quot; L, xbins, ybins = onp.histogram2d(trace1, trace2, nbins) L[L == 0] = 1E-16 logL = np.log(L) shape = L.shape L = L.ravel() # obtain the indices to sort and unsort the flattened array i_sort = np.argsort(L)[::-1] i_unsort = np.argsort(i_sort) L_cumsum = L[i_sort].cumsum() L_cumsum /= L_cumsum[-1] xbins = 0.5 * (xbins[1:] + xbins[:-1]) ybins = 0.5 * (ybins[1:] + ybins[:-1]) return xbins, ybins, L_cumsum[i_unsort].reshape(shape) def plot_MCMC_trace(ax, trace1, trace2, scatter=False, **kwargs): &quot;&quot;&quot;Plot traces and contours&quot;&quot;&quot; xbins, ybins, sigma = compute_sigma_level(trace1, trace2) ax.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955], **kwargs) if scatter: ax.plot(trace1, trace2, &#39;,k&#39;, alpha=0.4) . /opt/anaconda3/envs/ML/lib/python3.7/site-packages/jax/lib/xla_bridge.py:123: UserWarning: No GPU/TPU found, falling back to CPU. warnings.warn(&#39;No GPU/TPU found, falling back to CPU.&#39;) . We define a linear and quadratic function of a scalar variable $x$, which takes as first argument a list of parameters $ Theta$. We then define a routine to create noisy versions a function. . def linear_func(θ, x): return θ[0] + θ[1]*x # y = mx + b def quadratic_func(θ, x): return θ[0] + θ[1]*x + θ[2]*x**2 # y = a x^2 + bx + c def make_noisy_func(f, σ): &quot;&quot;&quot;Returns function that computes noisy measurements for y ~ N(f, σ^2), such that y will be normally distributed around f, with variance σ^2&quot;&quot;&quot; return lambda θ,x : f(θ,x) + σ*random.randn(len(x)) . Now let&#39;s simulate measuring using a noisy linear function (intercept $b=2$, slope $m=1$). We discretize $x$ over $N_p=10$ points evenly spaced in the range $[0,2]$, and simulate $N=8$ draws. Computing the average and variance over these $N$ sets of measurements gives us our best estimate and error bar for the value of $f$ at $x_i$. . random.seed(12345) nparams,func = 2, linear_func #nparams,func = 3, quadratic_func _θ0,σ0 = np.array([2.0, 1.0, 0.6]), 0.25 # exact parameter values θ0 = _θ0[:nparams] measurement = make_noisy_func(func, σ0) npoints,ndraws= 10,8 x_sample = np.linspace(0, 2.0, num=npoints) y_sample = np.array([measurement(θ0,x_sample) for i in range(ndraws)]) y_avg, y_err = np.average(y_sample, axis=0), np.std(y_sample, axis=0) fig, ax = plt.subplots() for yi in y_sample: ax.plot(x_sample, yi, marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, label=&#39;Estimate&#39;, color=color[&#39;dark&#39;]) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . The symbols represent our &quot;measurement&quot;, 8 for each of the 10 $x$ values we are sampling. The average and variance of these measurements would give your experimental estimate (shown here with the error bar plot). . The &quot;Least-Squares&quot; route . Let&#39;s first attempt a least-squares minimization. . Recall the definition of $ chi^2$ $$ begin{align} chi^2 = sum_k frac{ left(y_k - f(x_k; Theta) right)}{ sigma_k^2} end{align}$$ . For simple functions, it is trivial to work out the first and second derivatives of this quantity, but jax does all the work for us! As shown above, the best-estimate for $ Theta$ is given by minimizing $ chi^2$, and the reliability estimates are given by (twice) the inverse of the Hessian $$ begin{align*} Theta &amp;= Theta_0 pm sqrt{ text{diag}( Sigma_{ Theta})} Sigma_{ Theta}&amp;= 2 left( nabla nabla chi^2 lvert_{ Theta_0} right)^{-1} end{align*}$$ . For a the simple linear model we are considering, we can easily work out the analytical solution . $$ begin{align} partial_m chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right) x_k}{ sigma_k^2} qquad &amp; partial_b chi^2 &amp;= sum_k frac{2 left(m x_k + b - y_k right)}{ sigma_k^2} nabla chi^2 &amp;= begin{pmatrix} partial_m chi^2 partial_b chi^2 end{pmatrix} = begin{pmatrix} sum w_k x_k^2 &amp; sum w_k x_k sum w_k x_k &amp; sum w_k end{pmatrix} begin{pmatrix} m b end{pmatrix} - begin{pmatrix} sum w_k x_k y_k sum w_k y_k end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} begin{pmatrix}m b end{pmatrix} - begin{pmatrix}p q end{pmatrix} end{align}$$ with $ alpha = sum w_k x_k^2$, $ beta = sum_k w_k$, $ gamma= sum_k w_k x_k$, $w_k = 2/ sigma_k^2$, $p= sum_k w_k x_k y_k$ and $q= sum_k w_k y_k$. The optimum solution is then . $$ begin{align*} nabla chi^2 &amp;= 0 begin{pmatrix}m b end{pmatrix} &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix}^{-1} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} begin{pmatrix}p q end{pmatrix} = frac{1}{ alpha beta- gamma^2} begin{pmatrix} beta p - gamma q alpha q - gamma p end{pmatrix} end{align*}$$Finally, the covariance matrix $ Sigma_{ Theta} = 2 ( nabla nabla chi^2)^{-1}$ is just . $$ begin{align*} nabla nabla chi^2 &amp;= begin{pmatrix} alpha &amp; gamma gamma &amp; beta end{pmatrix} Sigma_{ Theta} &amp;= 2 frac{1}{ alpha beta - gamma^2} begin{pmatrix} beta &amp;- gamma - gamma &amp; alpha end{pmatrix} end{align*}$$ @jit def chi2(θ): return np.sum((y_avg-func(θ,x_sample))**2/y_err**2) grad_chi2 = jit(grad(chi2)) # nabla χ^2 hess_chi2 = hessian(chi2) # nabla nabla χ^2 def exact_sol(xk,yk,dyk): wk = 2/dyk**2 α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_avg, y_err) . opt = minimize(chi2, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_chi2, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_chi2(opt[&#39;x&#39;])) # inverse hessian evaluated at the optimum value Θ_0 . Optimization terminated successfully. Current function value: 1.118355 Iterations: 5 Function evaluations: 7 Gradient evaluations: 7 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.2f}]&#39;) . |θ - Θ_exact|_inf = 2.442491e-15 |H - H_exact|_inf = 3.469447e-18 |H_min - H_exact|_inf = 1.040834e-17 1.69 &lt; θ^1 (b) &lt; 2.24 [μ = 1.961, σ = 0.14] 0.79 &lt; θ^2 (m) &lt; 1.24 [μ = 1.018, σ = 0.11] . The exact values used to generate the data were $ Theta^1 = b = 2$ and $ Theta^2 = m = 1$, which is very close to the optimal value obtained here. Furthermore, we see that the approximate of the Hessian returned by the minimization routine coincides with the exact result (given by jax). We use $ pm 2 sigma$ interval as a measure of the error bounds. . The full Bayesian route . Now let&#39;s see how to get the &quot;full&quot; solution, not just a point-estimate. For this we use PYMC3 to generate samples from the posterior. To compare with our previous analysis, we will assume a uniform prior for both $m$ and $b$. . with pm.Model() as model: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) # true function y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) # measured data (accounting for noise) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=y_err, observed=y_avg) # generate samples from the prior (before looking at the data) prior = pm.sample_prior_predictive(samples=1000, random_seed = 123456) # generate samples from the posterior trace = pm.sample(5000, tune = 20000, progressbar=True) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [b, m] INFO:pymc3:NUTS: [b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 50000/50000 [00:20&lt;00:00, 2415.40draws/s] The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. WARNING:pymc3:The acceptance probability does not match the target. It is 0.9123745676059328, but should be close to 0.8. Try to increase the number of tuning steps. . First, let us try to get an idea for how good/bad our priors are. We can do this by looking at the prior predictive distribution. This simply takes parameters values sampled from the prior, and pushes them through the model, generating corresponding samples for $y$. If we plot $y-b = mx$ we can clearly see that this prior is heavily skewed towards lines with high slopes. Is this what we want? Probably not, but this will depend on our prior information. . fig, ax = plt.subplots() for y_i,b_i in zip(prior[&#39;y&#39;], prior[&#39;b&#39;]): ax.plot(x_sample, y_i-b_i, alpha=0.2, color=&#39;k&#39;) ax.set_xlabel(r&#39;x&#39;) ax.set_ylabel(r&#39;y&#39;) ax.set_ylim(0,2) ax.set_xlim(0,2) plt.show() . PYMC3 comes with many useful post-processing functions to visualize our data. Let&#39;s look at a few of them. . pm.traceplot(trace) plt.show(); . Above, we see the distribution of the model parameters, with different different lines corresponding to different chains or independent runs of the MC simulation. At first glance, there doesn&#39;t seem to be any obvious divergence and we get nice Gaussian-like distributions for all model parameters (this includes the values of the function at the measured points). . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace, var_names=[&#39;m&#39;, &#39;b&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.show(); . &lt;Figure size 504x504 with 0 Axes&gt; . Here we have the $94 %$ credible intervals, corresponding to the $ mu pm 2 sigma$ interval in the case of a Gaussian distribution. The vertical lines indicate the real values that were used to generate the data. . pm.plot_posterior(trace, var_names=[&#39;m&#39;, &#39;b&#39;]) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fff4d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe550fc8a10&gt;], dtype=object) . Here we have the posterior distribution for our two model parameters. Notice that the $94 %$ HPD (Highest Posterior Density Interval) corresponds exactly to that obtained using the Laplace approximation. This should not be surprising, when we see the posterior. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace[&#39;y&#39;], quantiles_sig) ax.errorbar(x_sample, y_avg, yerr=y_err, ls=&#39;None&#39;, marker=&#39;o&#39;, color=color[&#39;superfine&#39;], alpha=0.5, label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], alpha=1, label=&#39;exact&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.legend() plt.show() . The shaded red regions show the $ sigma$, $2 sigma$ and $3 sigma $ regions $y$ obtained from the posterior samples of y. . fig, ax = plt.subplots() plot_MCMC_trace(ax, trace[&#39;b&#39;], trace[&#39;m&#39;], scatter=True, colors=[color[&#39;mid&#39;], color[&#39;light_highlight&#39;], color[&#39;light&#39;]]) ax.set_xlabel(&#39;b&#39;) ax.set_ylabel(&#39;m&#39;) plt.show() . Here we see the trace generated by the MC simulation, where the distribution of points approximates the the posterior distribution. Also drawn are the $ pm sigma$ and $ pm 2 sigma$ contour levels, which should contain $68 %$ and $95 %$ of the points, respectively. . We can redo the analysis using a &quot;better&quot; prior, e.g., assigning a uniform prior on the angles of the lines instead of their slope, but the results will not vary significantly (unless the number of data points is reduced). . Example : Fitting a straight line (with unknown measurement error) . The &quot;Least-Squares&quot; route . Now let&#39;s analyze the case where the measurement error is not known. As shown above, instead of minimizing $ chi^2$, we should maximize $L=- frac{N}{2} ln{S}$, with $S= sum_k (y_k -f_k)^2$, which coincides with minimizing $S$. Thus, the form of the solution is the same as before, we simply set $w_k=2 , ( sigma_k=1)$, as all the points are given equal weight. Recall that the covariance matrix is $ Sigma_ Theta^{-1} = frac{1}{2} nabla nabla frac{S}{S_0/N} lvert_{ Theta_0}$. . @jit def loss(θ): return np.sum((y_sample[0]-func(θ,x_sample))**2) grad_loss = jit(grad(loss)) hess_loss = hessian(loss) def exact_sol(xk,yk): wk = 2.0*np.ones_like(yk) α,β,γ = np.sum(wk*xk**2), np.sum(wk), np.sum(wk*xk) p,q = np.sum(wk*xk*yk), np.sum(wk*yk) idet = 1.0 / (α*β - γ**2) m,b = idet*(β*p - γ*q), idet*(α*q - γ*p) return np.array([b,m]), idet*np.array([[α, -γ], [-γ, β]]) # reorder solution (m,b) -&gt; (b,m) θgold, iHgold = exact_sol(x_sample, y_sample[0]) opt = minimize(loss, np.ones_like(θ0), method=&#39;BFGS&#39;, jac=grad_loss, options={&#39;maxiter&#39;:100, &#39;disp&#39;:1}) opt_ihess = np.linalg.inv(hess_loss(opt[&#39;x&#39;])) . Optimization terminated successfully. Current function value: 0.323150 Iterations: 4 Function evaluations: 6 Gradient evaluations: 6 . labels = [&#39;b&#39;, &#39;m&#39;] print(f&quot;|θ - Θ_exact|_inf = {np.max(np.abs(opt[&#39;x&#39;]-θgold)):.6e}&quot;) print(f&quot;|H - H_exact|_inf = {np.max(np.abs(opt_ihess-iHgold)):.6e}&quot;) print(f&quot;|H_min - H_exact|_inf = {np.max(np.abs(opt[&#39;hess_inv&#39;] - iHgold)):.6e}&quot;) print(&quot;&quot;) for i in range(nparams): avg, sigma = opt[&#39;x&#39;][i], np.sqrt(opt_ihess[i,i]*2*loss(opt[&#39;x&#39;])/npoints) print(f&#39;{avg-2*sigma:.2f} &lt; θ^{i+1} ({labels[i]}) &lt; {avg+2*sigma:.2f} [μ = {avg:.3f}, σ = {sigma:.3f}]&#39;) . |θ - Θ_exact|_inf = 2.220446e-16 |H - H_exact|_inf = 2.775558e-17 |H_min - H_exact|_inf = 4.163336e-17 1.77 &lt; θ^1 (b) &lt; 2.20 [μ = 1.984, σ = 0.106] 0.96 &lt; θ^2 (m) &lt; 1.32 [μ = 1.140, σ = 0.089] . Within the Gaussian approximation, we can also use this optimum solution to give an estimate of the unknown measurement error . $$ begin{align*} sigma_0^2 &amp;= frac{S_0}{N+1-p} sigma &amp;= sigma_0 pm frac{ sigma_0}{ sqrt{2(N+1-p)}} end{align*}$$ σa = np.sqrt(loss(opt[&#39;x&#39;])/(npoints + 1 - nparams)) dσa= σa / np.sqrt(2*(npoints + 1 - nparams)) print(f&#39;{σa - 2*dσa:.2f} &lt; σ &lt; {σa + 2*dσa:.2f} [μ = {σa : .3f}, σ = {dσa : .3f}]&#39;) . 0.10 &lt; σ &lt; 0.28 [μ = 0.189, σ = 0.045] . Which is in good agreement with the exact value used to generate the noisy data ($ sigma = 0.25$). . fig, ax = plt.subplots() ax.plot(x_sample, y_sample[0], marker=&#39;o&#39;, ls=&#39;None&#39;, color=color[&#39;mid&#39;], mfc=&#39;None&#39;, mew=2, alpha=0.8) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, label=&#39;Exact&#39;, color=&#39;Grey&#39;) ax.plot(x_sample, func(opt[&#39;x&#39;],x_sample), ls=&#39;-&#39;, color=color[&#39;dark&#39;], alpha=1, label=&#39;Estimate&#39;) ax.legend() ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$f(x)$&#39;) plt.show() . with pm.Model() as model_sig: #priors for unknown model parameters m = pm.Uniform(&#39;m&#39;, lower=-10, upper=10) b = pm.Uniform(&#39;b&#39;, lower=-10, upper=10) logσ = pm.Uniform(&#39;logσ&#39;, lower=-6, upper=6) σ = pm.Deterministic(&#39;σ&#39;, tt.exp(logσ)) y = pm.Deterministic(&#39;y&#39;, func([b,m], x_sample)) yobs = pm.Normal(&#39;yobs&#39;, mu=y, sd=σ, observed=y_sample[0]) prior_sig = pm.sample_prior_predictive(samples=1000, random_seed = 123456) trace_sig = pm.sample(5000, tune = 50000, progressbar=True) #post_sig = pm.sample_posterior_predictive(trace_uni, model=model_sig, random_seed=4938483) . Auto-assigning NUTS sampler... INFO:pymc3:Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... INFO:pymc3:Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) INFO:pymc3:Multiprocess sampling (2 chains in 2 jobs) NUTS: [logσ, b, m] INFO:pymc3:NUTS: [logσ, b, m] Sampling 2 chains, 0 divergences: 100%|██████████| 110000/110000 [01:08&lt;00:00, 1606.14draws/s] . plt.figure(figsize=(12,12)) pm.traceplot(trace_sig) plt.show(); . &lt;Figure size 864x864 with 0 Axes&gt; . fig = plt.figure(figsize=(7,7)) pm.forestplot(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); plt.axvline(1, 0, 1, c=&#39;C1&#39;) plt.axvline(2, 0, 1, c=&#39;C1&#39;) plt.axvline(σ0, 0, 1, c=&#39;C1&#39;) plt.show() . &lt;Figure size 504x504 with 0 Axes&gt; . pm.plot_posterior(trace_sig, var_names=[&#39;m&#39;, &#39;b&#39;, &#39;σ&#39;]); . Notice how the resulting distribution for $ sigma$ is no longer symmetric. In this case we expect the Gaussian approximation will start to break down. However, the error bounds derived above, $0.1&lt; sigma &lt; 0.28$ are still close to the &quot;real&quot; values provided by the MC trace. . fig, ax = plt.subplots() plot_quantiles(ax, x_sample, trace_sig[&#39;y&#39;], quantiles_sig, label=&#39;prediction&#39;) ax.plot(x_sample, y_sample[0], ls=&#39;None&#39;, marker=&#39;o&#39;, mfc=&#39;None&#39;, mew=2, color=color[&#39;dark&#39;], label=&#39;measurements&#39;) ax.plot(x_sample, func(θ0,x_sample), ls=&#39;--&#39;, color=color[&#39;superfine&#39;], label=&#39;exact&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7fe5629e4c90&gt; . Finally, here is the list of packages in my environment. . from pip._internal.operations.freeze import freeze for requirement in freeze(local_only=True): print(requirement) . absl-py==0.9.0 appnope==0.1.0 arviz==0.7.0 attrs==19.3.0 backcall==0.1.0 bleach==3.1.4 certifi==2019.11.28 cffi==1.14.0 cftime==1.1.1.2 chardet==3.0.4 cryptography==2.8 cycler==0.10.0 decorator==4.4.2 defusedxml==0.6.0 entrypoints==0.3 fastcache==1.1.0 h5py==2.10.0 idna==2.9 importlib-metadata==1.6.0 ipykernel==5.2.0 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jax==0.1.62 jaxlib==0.1.42 jedi==0.16.0 Jinja2==2.11.1 json5==0.9.0 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-console==6.1.0 jupyter-core==4.6.3 jupyterlab==2.0.1 jupyterlab-server==1.1.0 kiwisolver==1.1.0 Mako==1.1.0 MarkupSafe==1.1.1 matplotlib==3.2.1 mistune==0.8.4 mkl-service==2.3.0 nbconvert==5.6.1 nbformat==5.0.4 netCDF4==1.5.3 notebook==6.0.3 numpy==1.18.1 opt-einsum==0+untagged.53.g6ab433b.dirty packaging==20.1 pandas==1.0.3 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 pip==20.0.2 prometheus-client==0.7.1 prompt-toolkit==3.0.5 protobuf==3.11.4 ptyprocess==0.6.0 pycparser==2.20 Pygments==2.6.1 pygpu==0.7.6 pymc3==3.8 pyOpenSSL==19.1.0 pyparsing==2.4.6 PyQt5==5.12.3 PyQt5-sip==4.19.18 PyQtWebEngine==5.12.1 pyreadr==0.2.6 pyrsistent==0.16.0 PySocks==1.7.1 python-dateutil==2.8.1 pytz==2019.3 pyzmq==19.0.0 qtconsole==4.7.2 QtPy==1.9.0 requests==2.23.0 rpy2==3.1.0 scipy==1.4.1 seaborn==0.10.0 Send2Trash==1.5.0 setuptools==46.1.3.post20200325 simplegeneric==0.8.1 six==1.14.0 statsmodels==0.11.1 terminado==0.8.3 testpath==0.4.4 Theano==1.0.4 tornado==6.0.4 tqdm==4.45.0 traitlets==4.3.3 tzlocal==2.0.0 urllib3==1.25.7 wcwidth==0.1.9 webencodings==0.5.1 wheel==0.34.2 widgetsnbextension==3.5.1 xarray==0.15.1 zipp==3.1.0 .",
            "url": "https://johnjmolina.github.io/MLKyoto/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "relUrl": "/data%20analysis/parameter%20estimation/2020/05/13/Parameter-Estimation.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://johnjmolina.github.io/MLKyoto/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://johnjmolina.github.io/MLKyoto/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://johnjmolina.github.io/MLKyoto/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://johnjmolina.github.io/MLKyoto/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}