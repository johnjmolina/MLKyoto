{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Gaussian Processes 1 - Theory\"\n",
    "> \"We introduce the basic theory behind Gaussian Processes\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: John J. Molina\n",
    "- categories: [Gaussian Processes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard textbook on Gaussian Processes (GP) is that of Rasmussen and Williams. The book by Murphy on Machine Learning also has a nice intro to GP and how they connect with other ML methods. \n",
    "Finally, the \"Matrix Cookbook\" has an extensive list of identities that are helpful for the GP derivations.\n",
    "\n",
    "- [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/). C. E. Rasmussen and C. K. I. Williams, Cambridge, the MIT Press (2006)\n",
    "- Machine Learning : A Probabilistic Perspective. K. P. Murphy, Cambridge, the MIT Press (2012)\n",
    "- [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html). K. B. Petersen and M. S. Pedersen (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating GP will require a bit of matrix algebra and the use of some not very well know identities (at least to the author).\n",
    "Thus, we will start by giving (without proof) the main results needed to derive the basic GP equations.\n",
    "\n",
    "Let $\\Sigma$ be a block matrix, defined as\n",
    "\\begin{align}\n",
    "\\Sigma &= \\begin{pmatrix}\n",
    "A & C \\\\\n",
    "D & B\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "From the sub-matrices, define $E$ and $F$ as\n",
    "\\begin{align}\n",
    "E &= A - C B^{-1} D \\\\\n",
    "F &= B - D A^{-1} C\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of $\\Sigma$ can be written in terms of that of $A$ and $F$, or $B$ and $E$, as\n",
    "\n",
    "\\begin{align}\n",
    "\\det{\\Sigma} &= \\det{A}\\cdot \\det{F} = \\det{B}\\cdot \\det{E}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix inverse of $\\Sigma$ can also be expressed in block form as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Sigma^{-1} &= \\begin{pmatrix}\n",
    "\\widetilde{A} & \\widetilde{C}\\\\\n",
    "\\widetilde{D} & \\widetilde{B}\n",
    "\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\n",
    "E^{-1} & - A^{-1} C F^{-1} \\\\\n",
    "-F^{-1} D A^{-1}& F^{-1}\n",
    "\\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix}\n",
    "A^{-1} + A^{-1} C F^{-1} D A^{-1} & -E^{-1} C B^{-1} \\\\\n",
    "-B^{-1} D E^{-1} & B^{-1} + B^{-1} D E^{-1} C B^{-1}\n",
    "\\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When \"training\" our GP models, it will be useful to be able to compute derivatives of these block matrices with respect to the hyper-parameters $\\Theta$.\n",
    "In particular, we will need the derivatives of the matrix inverse and the log of the determinant. These are given by\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\theta} A^{-1} &= -A^{-1} \\frac{\\partial A}{\\partial\\theta} A^{-1}\\\\\n",
    "\\frac{\\partial}{\\partial\\theta} \\log{\\left(\\det{A}\\right)} &= \\text{tr}{\\left(A^{-1}\\frac{\\partial A}{\\partial\\theta}\\right)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that $\\Sigma$ is a symmetric matrix, which is the only case we will be interested in here, $\\Sigma = \\Sigma^{t}$, which in turn implies that $A=A^t$, $B = B^{t}$, $D = C^{t}$, the block form of the matrix inverse (also symmetric) can be written as\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Sigma^{-1} &= \\begin{pmatrix}\n",
    "\\widetilde{A} & \\widetilde{C}\\\\\n",
    "\\widetilde{C}^t & \\widetilde{B}\n",
    "\\end{pmatrix} \\\\\n",
    "&=\\begin{pmatrix}\n",
    "E^{-1} & - A^{-1} C F^{-1} \\\\\n",
    "- F^{-1} C^{t} A^{-1} & F^{-1}\n",
    "\\end{pmatrix} \\\\\n",
    "&=\\begin{pmatrix}\n",
    " A^{-1} + A^{-1} C F^{-1} C^{t} A^{-1}& - E^{-1} C B^{-1} \\\\\n",
    " -B^{-1} C^{t} E^{-1} & B^{-1} + B^{-1} C^{t} E^{-1} C B^{-1}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align}\n",
    "\\widetilde{A}^{-1} = E &= A - C B^{-1} C^{t} \\\\\n",
    "\\widetilde{B}^{-1} = F &= B - C^{t} A^{-1} C\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following form of the relations will be particularly useful for the derivations below\n",
    "\\begin{align}\n",
    "\\widetilde{C} &= -E^{-1} C B^{-1} = - \\widetilde{A} C B^{-1} \\\\\n",
    "\\widetilde{B} &= B^{-1} + B^{-1} C^t E^{-1} C B^{-1} = B^{-1} + B^{-1} C^t \\widetilde{A} C B^{-1} \\Longrightarrow\n",
    "\\widetilde{B} - B^{-1} = B^{-1} C^t \\widetilde{A}C B^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Multi-variate) Gaussians / GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn our attention to GP. As defined in Rasmussen and Williams, a GP is \"a collection of random variables, any finite number of which have a joint Gaussian distribution\". What does this mean?\n",
    "\n",
    "Previously, for the Bayesian Parameter Estimation problem, we were given some data and a model (which on prior information was assumed to explain the data), and tasked with finding the distribution of the parameters that could explain the data. That is, we wanted to infer or learn the parameters from the data. However, this only works if we know the model. What happens when we don't posses this information? \n",
    "\n",
    "This leads us to the much trickier problem of \"Non-parametric Bayesian Inference\". Since we don't have a model to parametrize, we take the function values themselves to be the parameters! So it's not so much that there are no parameters, it's just that there is an infinite number of them. Instead of learning the parameters in some model, we will try to learn the function itself from the data. \n",
    "\n",
    "In the specific case of GP, we assume that the value of the function at each point (e.g., $x(t)$) is a random variable, and that they are all correlated, with a joint Gaussian distribution. Thus, the joint probability distribution for the $x = (x(t_1), x(t_2), \\ldots x(t_n)) = (x_1, x_2, \\ldots x_n)$ is given by a multi-variate Gaussian, specified by some mean $\\mu$ and (symmetric) covariance matrix $\\Sigma$. We express this as \n",
    "\n",
    "\\begin{align}x\\sim \\mathcal{N}(\\mu, \\Sigma)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is to be interpreted according to\n",
    "\\begin{align}\n",
    "p(x\\lvert \\mu, \\Sigma) &= \\frac{1}{\\sqrt{\\det{\\left(2\\pi\\Sigma\\right)}}} \\exp{\\left[-\\frac{1}{2}\\delta x^t \\Sigma^{-1}\\delta x\\right]}\\qquad \\left(\\delta x= x - \\mu\\right)\\\\\n",
    "\\int p(x\\lvert \\mu, \\Sigma) \\,\\mathrm{d}x &= 1 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition the first and second moments are given by the average and covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\langle x\\rangle \\equiv \\int x p(x\\lvert \\mu, \\Sigma) \\,\\mathrm{d}x &= \\mu \\\\\n",
    "\\left\\langle\\delta x_i \\delta x_j\\right\\rangle \\equiv \\int \\delta x_i \\delta x_j p(x\\lvert \\mu,\\Sigma) \\,\\mathrm{d}x &= \\Sigma_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digression on Gaussian Integrals\n",
    "\n",
    "To compute the marginal and conditional distributions we will need to manipulate the quadratic expressions appearing in the exponential. \n",
    "\n",
    "In particular, we will need to compute integrals of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "I(A, b, c) &= \\int \\exp{\\left[-\\frac{1}{2} x^t A x + x^t b + c\\right]} \\,\\mathrm{d}x\n",
    "\\end{align}\n",
    "where $A$ is a symmetric symmetric. This integral can be performed easily by completing the square, as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "-\\frac{1}{2} x^t A x + x^t b &= -\\frac{1}{2} x^t A x + b^t x \\\\\n",
    "&= -\\frac{1}{2}\\left[x^t A x - b^t x - x^t b\\right] \\\\\n",
    "&= -\\frac{1}{2}\\left[x^t A^t x - b^t A^{-t} A^{t} x - x^t b\\right]\\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x^t - b^t A^{-t}\\right) A^t x - x^t b\\right]\\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x - A^{-1} b\\right)^t A^t x - x^t b\\right] \\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x - A^{-1} b\\right)^t A^t x - \\left(x - A^{-1} b + A^{-1} b\\right)^t b\\right] \\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x - A^{-1} b\\right)^t A^t x - \\left(x - A^{-1} b\\right)^t b - b^t A^{-t} b\\right]\\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x - A^{-1} b\\right)^t A x - \\left(x - A^{-1} b\\right)^t A A^{-1}b\\right] + \\frac{1}{2} b^t A^{-1} b \\\\\n",
    "&= -\\frac{1}{2}\\left[\\left(x - A^{-1} b\\right)^t A \\left(x - A^{-1} b\\right)\\right] + \\frac{1}{2} b^t A^{-1} b\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have repeatedly used the fact that a scalar is by definition symmetric, so $\\alpha=\\alpha^t$, $x^t y = y^t x$, $x^t A y = y^t A^t x$, and so on ($\\alpha$ a scalar, $x$ and $y$ vectors, and $A$ a square matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "I(A, b, c)&= \\int \\exp{\\left[-\\frac{1}{2}x^t A x + x^t b + c\\right]\\mathrm{d}x} \\\\\n",
    "&= \\exp{\\left[\\frac{1}{2}b^t A^{-1} b + c\\right]} \\underbrace{\\int\\exp{\\left[-\\frac{1}{2} \\left(x - A^{-1}b\\right)^t A \\left(x - A^{-1} b\\right)\\right]} \\mathrm{d}x}_{\\equiv \\sqrt{\\det{\\left(2\\pi A^{-1}\\right)}}}\\\\\n",
    "&= \\sqrt{\\det{\\left(2\\pi A^{-1}\\right)}} \\exp{\\left[\\frac{1}{2} b^t A^{-1} b + c\\right]} \\\\\n",
    "&= \\sqrt{\\frac{(2\\pi)^{n}}{\\det{A}}} \\exp{\\left[\\frac{1}{2} b^t A^{-1} b + c\\right]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets consider partitioning our set of points $x$ in two, $x_A$ and $x_B$, which could represent the (known) training data and (unknown) test data, respectively.\n",
    "The joint distribution, is given exactly by the expression above, but we can rewrite it in block form to highlight the contribution of the $x_A$ and $x_B$ points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(x\\lvert \\mu, \\Sigma) &= \\frac{1}{\\sqrt{(2\\pi)^n \\det{\\Sigma}}} \\exp{\\left[-\\frac{1}{2}\\delta x^t \\Sigma^{-1} \\delta x\\right]}\\, ,\\qquad\n",
    "\\Sigma = \\begin{pmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\ \\Sigma_{AB}^t & \\Sigma_{BB}\\end{pmatrix}\\\\\n",
    "&= \\frac{1}{\\sqrt{(2\\pi)^n\\det{\\Sigma}}} \\exp{\\left[-\\frac{1}{2}\\begin{pmatrix}\\delta x_A\\\\\\delta x_B\\end{pmatrix}^t \\begin{pmatrix}\\widetilde{\\Sigma}_{AA} & \\widetilde{\\Sigma}_{AB}\\\\\n",
    "\\widetilde{\\Sigma}_{AB}^t & \\widetilde{\\Sigma}_{BB}\\end{pmatrix}\\begin{pmatrix}\\delta x_A \\\\ \\delta x_B\\end{pmatrix}\\right]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we used the properties of block matrices to rewrite the inverse of $\\Sigma$ into block form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this joint distribution for $x_A$ and $x_B$, what can we say about the distribution for $x_B$, regardless of $x_A$?\n",
    "\n",
    "By definition, we simply marginalize over $x_A$\n",
    "\\begin{align}\n",
    "p(x_B\\lvert\\mu,\\Sigma) &= \\int p(x_A,x_B\\lvert\\mu,\\Sigma)\\mathrm{d}x_A\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this integral, lets rewrite the terms appearing in the exponent, trying to separate out the $x_A$ and $x_B$ contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\delta x^t \\Sigma^{-1}\\delta x &= \\delta x_A^t\\left(\\widetilde{\\Sigma}_{AA}\\delta x_A + \\widetilde{\\Sigma}_{AB}\\delta x_B\\right) + \\delta x_B^t \\left(\\widetilde{\\Sigma}^t_{AB}\\delta x_A + \\widetilde{\\Sigma}_{BB}\\delta x_B\\right) \\\\\n",
    "&= \\bigg[\\delta x_A^t \\widetilde{\\Sigma}_{AA}\\delta x_A + 2\\delta x_A^t \\widetilde{\\Sigma}_{AB}\\delta x_B\\bigg] + \\delta x_B^t \\widetilde{\\Sigma}_{BB}\\delta x_B\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using the properties of these block matrices, the determinant in the normalization factor can be conveniently written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\det{\\Sigma} = \\det{\\Sigma_{AA}}\\cdot \\det{\\widetilde{\\Sigma}_{BB}^{-1}} = \\det{\\Sigma_{BB}}\\cdot \\det{\\widetilde{\\Sigma}_{AA}^{-1}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all this together, the marginal distribution for $x_B$ takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(x_B\\lvert\\mu,\\Sigma) &= \\left(\\frac{1}{\\sqrt{(2\\pi)^{n_A}\\det{\\widetilde{\\Sigma}_{AA}^{-1}}}}\\underbrace{\\int \\exp{\\left[-\\frac{1}{2}\\delta x_A^t \\widetilde{\\Sigma}_{AA}\\delta x_A - \\delta x_A^t \\widetilde{\\Sigma}_{AB}\\delta x_B\\right]}\\mathrm{d}x_A}_{I(\\widetilde{\\Sigma}_{AA}, -\\widetilde{\\Sigma}_{AB}\\delta x_B, 0)}\\right)\\times\n",
    "\\left(\\frac{1}{\\sqrt{(2\\pi)^{n_B}\\det{\\Sigma_{BB}}}} \\exp{\\left[-\\frac{1}{2}\\delta x^{t}_B \\widetilde{\\Sigma}_{BB}\\delta x_B\\right]}\\right) \\\\\n",
    "&= \\left(\\frac{1}{\\sqrt{(2\\pi)^{n_A}\\det{\\widetilde{\\Sigma}_{AA}^{-1}}}} \\times \\sqrt{\\frac{(2\\pi)^{n_A}}{\\det\\widetilde{\\Sigma}_{AA}}} \\exp{\\left[\\frac{1}{2}\\left(\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)^t\\widetilde{\\Sigma}_{AA}^{-1}\\left(\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)\\right]}\\right)\\times\n",
    "\\left(\\frac{1}{\\sqrt{(2\\pi)^{n_B}\\det{\\Sigma_{BB}}}} \\exp{\\left[-\\frac{1}{2}\\delta x^{t}_B \\widetilde{\\Sigma}_{BB}\\delta x_B\\right]}\\right) \\\\\n",
    "&=\\frac{1}{\\sqrt{(2\\pi)^{n_B}\\det{\\Sigma_{BB}}}} \\exp{\\left[-\\frac{1}{2}\\delta x^t_B\\left(\\widetilde{\\Sigma}_{BB}- \\widetilde{\\Sigma}_{AB}^t \\widetilde{\\Sigma}_{AA}^{-1} \\widetilde{\\Sigma}_{AB}\\right)\\delta x_B\\right]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost in the form of a multi-variate Gaussian. We can further simplify it by using the properties of block matrices listed above, since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\widetilde{\\Sigma}_{AB}^t \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB} &= \\widetilde{C}^t \\widetilde{A}^{-1} \\widetilde{C} \\\\\n",
    "&=\\left(-B^{-1} C^{t} \\widetilde{A}\\right) \\widetilde{A}^{-1}\\left(-\\widetilde{A} C B^{-1}\\right) \\\\\n",
    "&= B^{-1}C^t \\widetilde{A}C B^{-1}\\\\\n",
    "&\\equiv \\widetilde{B} - B^{-1} \\\\\n",
    "&= \\widetilde{\\Sigma}_{BB} - \\Sigma_{BB}^{-1}\n",
    "\\end{align}\n",
    "\n",
    "where we have used the fact that $\\widetilde{C}= -\\widetilde{A}C B^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus arrive at the result that the marginal distribution for $x_B$ is also Gaussian, with average $\\mu_B$ and covariance matrix $\\Sigma_{BB}$. \n",
    "\n",
    "We can simply read off the distribution for $x_B$ from the original joint distribution!\n",
    "\\begin{align}\n",
    "p(x_B\\lvert\\mu,\\Sigma) = p(x_B\\lvert \\mu_B, \\Sigma_{BB}) &= \\frac{1}{\\sqrt{(2\\pi)^{n_B}\\det{\\Sigma_{BB}}}} \\exp{\\left[-\\frac{1}{2}\\delta x^{t}_B \\Sigma^{-1}_{BB}\\delta x_B\\right]} \\\\\n",
    "x_B &\\sim \\mathcal{N}(\\mu_B, \\Sigma_{BB})\n",
    "\\end{align}\n",
    "\n",
    "This is the meaning of the quoted text which says that \"any finite number of which have a joint Gaussian distribution\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more useful result comes from considering the conditional distribution. Say we have already measured $x_B$, this would be our training data set, what can we say about the function values $x_A$ at other points? \n",
    "\n",
    "From Bayes' theorem, this conditional distribution is simply given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(x_A\\lvert x_B, \\mu, \\Sigma) &= \\frac{p(x_A, x_B\\lvert \\mu, \\Sigma)}{p(x_B\\lvert \\mu, \\Sigma)}\n",
    "\\end{align}\n",
    "\n",
    "After some simple manipulations, we will see that this distribution again has the form of a multi-variate Gaussian, although the average and covariance matrices will be a bit more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let us rewrite the exponent appearing in the numerator, in order to cancel out the exponent in the denominator.\n",
    "\\begin{align}\n",
    "\\delta x^t \\Sigma^{-1}\\delta x &= \\delta x_A^t\\left(\\widetilde{\\Sigma}_{AA}\\delta x_A+ \\widetilde{\\Sigma}_{AB}\\delta x_B\\right) + \\delta x_B^t \\left(\\widetilde{\\Sigma}^t_{AB}\\delta x_A + \\widetilde{\\Sigma}_{BB}\\delta x_B\\right) \\\\\n",
    "&= \\bigg[\\delta x_A^t \\widetilde{\\Sigma}_{AA}\\delta x_A + 2\\delta x_A^t \\widetilde{\\Sigma}_{AB}\\delta x_B\\bigg] + \\delta x_B^t \\widetilde{\\Sigma}_{BB}\\delta x_B\\\\\n",
    "&= \\bigg[\\delta x_A^t \\widetilde{\\Sigma}_{AA}\\left(\\delta x_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right) + \\delta x_B^t \\widetilde{\\Sigma}_{AB}^t \\delta x_A\\bigg] + \\delta x_B^t \\widetilde{\\Sigma}_{BB}\\delta x_B\\\\\n",
    "&= \\bigg[\\delta x_A^t \\widetilde{\\Sigma}_{AA}\\left(\\delta x_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right) + \\delta x_B^t \\widetilde{\\Sigma}_{AB}^t\\widetilde{\\Sigma}_{AA}^{-t}\\widetilde{\\Sigma}_{AA}^t \\left(\\delta x_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B - \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)\\bigg] + \\delta x_B^t \\widetilde{\\Sigma}_{BB}\\delta x_B \\\\\n",
    "&= \\bigg[\\left(\\delta x_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)^t \\widetilde{\\Sigma}_{AA}\\left(\\delta x_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)\\bigg] -\n",
    "\\delta x_B^t \\underbrace{\\bigg[\\widetilde{\\Sigma}_{AB}^t\\widetilde{\\Sigma}_{AA}^{-t}\\widetilde{\\Sigma}_{AB} - \\widetilde{\\Sigma}_{BB}\\bigg]}_{-\\Sigma_{BB}^{-1}}\\delta x_B\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we see that the second term on the right hand side will exactly cancel the exponential in the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets consider the ratio of normalization constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sqrt{\\frac{(2\\pi)^{n_B}\\det{\\Sigma_{BB}}}{(2\\pi)^{n_A + n_B}\\det{\\Sigma}}} &= \\sqrt{\\frac{\\det{\\Sigma_{BB}}}{(2\\pi)^{n_A}\\det{\\Sigma_{BB}}\\times\\det{\\widetilde{\\Sigma}_{AA}^{-1}}}} \\\\\n",
    "&= \\frac{1}{\\sqrt{(2\\pi)^{n_A} \\det{\\widetilde{\\Sigma}_{AA}^{-1}}}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, the conditional distribution for $x_A$ (conditioned on $x_B$) is another Gaussian!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(x_A\\lvert x_B, \\mu, \\Sigma) &= \\frac{1}{\\sqrt{(2\\pi)^{n_A}\\det{\\widetilde{\\Sigma}_{AA}^{-1}}}} \\exp{\\left[-\\frac{1}{2}\\left(x_A  - \\mu_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)^t \\widetilde{\\Sigma}_{AA}\\left(x_A - \\mu_A + \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B\\right)\\right]}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "x_A\\lvert x_B &\\sim \\mathcal{N}\\left(\\mu_A - \\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB}\\delta x_B, \\widetilde{\\Sigma}_{AA}^{-1}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's more convenient to express all quantities in terms of the block matrices of $\\Sigma$, we can rewrite the average and covariance using the following relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\widetilde{\\Sigma}_{AA}^{-1} &= \\Sigma_{AA} - \\Sigma_{AB} \\Sigma_{BB}^{-1} \\Sigma_{AB}^t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\widetilde{\\Sigma}_{AA}^{-1}\\widetilde{\\Sigma}_{AB} &\\equiv \\widetilde{A}^{-1} \\widetilde{C}\\\\\n",
    "&= -\\widetilde{A}^{-1}\\widetilde{A} C B^{-1} \\\\\n",
    "&= - C B^{-1} \\\\\n",
    "&= -\\Sigma_{AB}\\Sigma_{BB}^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we obtain the equivalent expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "x_A\\lvert x_B \\sim \\mathcal{N}\\left(\\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}\\delta x_B, \\Sigma_{AA}-\\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma^{t}_{AB}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! This is (almost) everything we need to do some Machine Learning with GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of using GP lies in their linearity.\n",
    "If $x$ and $y$ are two GP, then any linear combination of them is also a GP.\n",
    "\n",
    "In particular, \n",
    "\\begin{align}\n",
    "x&\\sim \\mathcal{N}(\\mu_x, \\Sigma_x) \\\\\n",
    "y&\\sim \\mathcal{N}(\\mu_y, \\Sigma_y) \\\\\n",
    "A x + B y + c &\\sim \\mathcal{N}(A\\mu_x + B\\mu_y + c, A\\Sigma_x A^t + B\\Sigma_y B^t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, products of GP do not result in GP..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (simple) implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For improved numerical stability and computational cost, it is recommended not to compute the matrix inverses appearing in the expressions for the averages and covariances of the conditional distribution. A better approach, which is still quite expensive, is to use the Cholesky decomposition.\n",
    "\n",
    "If $A$ be a positive deffinite matrix (i.e., a covariance matrix $\\Sigma$), $A$ can be written as the product of a lower-triangular matrix $L$ and its transpose\n",
    "\\begin{align}\n",
    "A&= L L^t\n",
    "\\end{align}\n",
    "such that expression of the form $A^{-1} b = x$, for known $A$ and $b$ can be computed as\n",
    "\\begin{align}\n",
    "(L L^t)^{-1} b &= x\\\\\n",
    "L^{-t} L^{-1} b &= x\\\\\n",
    "L^t \\backslash \\left(L \\backslash b\\right) &= x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we have adopted the backslash notation used by Rasmussen and Williams\n",
    "\\begin{align}\n",
    "A x&= b\\\\\n",
    "x &= A^{-1} b\\\\\n",
    "x &\\equiv A\\backslash b\n",
    "\\end{align}\n",
    "where it is assumed that we know $A$, but not necessarily $A^{-1}$, and $b$. This notation is useful to emphasis the fact that we don't want to calculate $A^{-1}$ explicitly, we just need its product with some vector $b$ (i.e., to solve for $x$). \n",
    "\n",
    "Using this Cholesky decomposition, a sandwich product of the form $b^{t} A^{-1} c$ would be expressed as\n",
    "\\begin{align}\n",
    "b^t A^{-1}c &= b^{t}L^{-t} L^{-1} c \\\\\n",
    "&= (L^{-1} b)^t (L^{-1} c)\\\\\n",
    "&= w^t v\n",
    "\\end{align}\n",
    "with $v = L\\backslash c$ and $w = L\\backslash b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can evaluate more complicated expression, such as $A = C^t B^{-1} C$, without directly computing $B^{-1}$.\n",
    "Let $C = (c_1, c_2, \\ldots, c_n)$, where $c_i$ are the column-vector components of $C$. We have\n",
    "\n",
    "\\begin{align}\n",
    "A &= (c_1, c_2, \\ldots, c_n)^t B^{-1} (c_1, c_2, \\ldots c_n) \\\\\n",
    "&= \\begin{pmatrix}c_1\\\\ c_2\\\\ \\vdots\\\\ c_n\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "B^{-1} c_1 & B^{-1} c_2 &\\ldots & B^{-1} c_n\n",
    "\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\n",
    "c_1^t B^{-1}c_1 & \\ldots & c_1^t B^{-1} c_n \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "c_n^t B^{-1}c_1 & \\ldots & c_n^t B^{-1} c_n\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where each term is computed using the expression derived above\n",
    "\\begin{align}\n",
    "(A)_{ij} &= c_i^t B^{-1} c_j \\equiv (L\\backslash c_i)^t (L\\backslash c_j)\n",
    "\\end{align}\n",
    "with $L$ now the Cholesky decomposition of $B=LL^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating terms like $\\log{\\det{A}}$ is also considerably simplified \n",
    "\n",
    "\\begin{align}\n",
    "\\log\\det{A} &= \\log\\det{LL^t} \\\\\n",
    "&= \\log\\left(\\det{L}\\cdot \\det{L^t}\\right) \\\\\n",
    "&= 2\\log\\det{L} = 2\\sum_i\\log L_{ii}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as hinted at above, this is still not the ideal way to evaluate the GP. The reason for this is the $\\mathcal{O}(n^3)$ scaling of the Cholesky decomposition.\n",
    "Fortunately, in recent years advanced matrix-matrix algorithms have been developed that allow for exact calculations even on millions of points!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
